2024-05-02 04:02:19,167 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-02 04:02:19,167 - INFO - joeynmt.helpers -                           cfg.name : transformer_sample_config
2024-05-02 04:02:19,167 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2024-05-02 04:02:19,167 - INFO - joeynmt.helpers -                     cfg.data.train : data/train
2024-05-02 04:02:19,167 - INFO - joeynmt.helpers -                       cfg.data.dev : data/dev
2024-05-02 04:02:19,167 - INFO - joeynmt.helpers -                      cfg.data.test : data/test
2024-05-02 04:02:19,167 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2024-05-02 04:02:19,167 - INFO - joeynmt.helpers -                  cfg.data.src.lang : en
2024-05-02 04:02:19,167 - INFO - joeynmt.helpers -                 cfg.data.src.level : word
2024-05-02 04:02:19,167 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2024-05-02 04:02:19,167 - INFO - joeynmt.helpers -       cfg.data.src.max_sent_length : 100
2024-05-02 04:02:19,167 - INFO - joeynmt.helpers -             cfg.data.src.voc_limit : 2000
2024-05-02 04:02:19,167 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : moses
2024-05-02 04:02:19,168 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : de
2024-05-02 04:02:19,168 - INFO - joeynmt.helpers -                 cfg.data.trg.level : word
2024-05-02 04:02:19,168 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2024-05-02 04:02:19,168 - INFO - joeynmt.helpers -       cfg.data.trg.max_sent_length : 100
2024-05-02 04:02:19,168 - INFO - joeynmt.helpers -             cfg.data.trg.voc_limit : 2000
2024-05-02 04:02:19,168 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : moses
2024-05-02 04:02:19,168 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2024-05-02 04:02:19,168 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0
2024-05-02 04:02:19,168 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2024-05-02 04:02:19,168 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2024-05-02 04:02:19,168 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2024-05-02 04:02:19,168 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2024-05-02 04:02:19,168 - INFO - joeynmt.helpers -            cfg.training.batch_size : 2048
2024-05-02 04:02:19,168 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2024-05-02 04:02:19,168 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2024-05-02 04:02:19,168 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2024-05-02 04:02:19,168 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2024-05-02 04:02:19,168 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2024-05-02 04:02:19,169 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2024-05-02 04:02:19,169 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2024-05-02 04:02:19,169 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2024-05-02 04:02:19,169 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2024-05-02 04:02:19,169 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2024-05-02 04:02:19,169 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2024-05-02 04:02:19,169 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu
2024-05-02 04:02:19,169 - INFO - joeynmt.helpers -             cfg.training.model_dir : models/words
2024-05-02 04:02:19,169 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2024-05-02 04:02:19,169 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2024-05-02 04:02:19,169 - INFO - joeynmt.helpers -              cfg.training.use_cuda : False
2024-05-02 04:02:19,169 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2024-05-02 04:02:19,169 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2024-05-02 04:02:19,169 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2024-05-02 04:02:19,169 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2024-05-02 04:02:19,169 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2024-05-02 04:02:19,169 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2024-05-02 04:02:19,169 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2024-05-02 04:02:19,169 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2024-05-02 04:02:19,170 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : False
2024-05-02 04:02:19,170 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2024-05-02 04:02:19,170 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2024-05-02 04:02:19,170 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2024-05-02 04:02:19,170 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2024-05-02 04:02:19,170 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2024-05-02 04:02:19,170 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2024-05-02 04:02:19,170 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0
2024-05-02 04:02:19,170 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2024-05-02 04:02:19,170 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2024-05-02 04:02:19,170 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0
2024-05-02 04:02:19,170 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2024-05-02 04:02:19,170 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2024-05-02 04:02:19,170 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2024-05-02 04:02:19,170 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2024-05-02 04:02:19,170 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2024-05-02 04:02:19,170 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0
2024-05-02 04:02:19,170 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2024-05-02 04:02:19,170 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2024-05-02 04:02:19,170 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0
2024-05-02 04:02:19,176 - INFO - joeynmt.data - Building tokenizer...
2024-05-02 04:02:19,565 - INFO - joeynmt.tokenizers - en tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=moses)
2024-05-02 04:02:19,565 - INFO - joeynmt.tokenizers - de tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=moses)
2024-05-02 04:02:19,565 - INFO - joeynmt.data - Loading train set...
2024-05-02 04:02:42,619 - INFO - joeynmt.data - Building vocabulary...
2024-05-02 04:02:44,190 - INFO - joeynmt.data - Loading dev set...
2024-05-02 04:02:44,427 - INFO - joeynmt.data - Loading test set...
2024-05-02 04:02:44,808 - INFO - joeynmt.data - Data loaded.
2024-05-02 04:02:44,808 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=100000, src_lang=en, trg_lang=de, has_trg=True, random_subset=-1)
2024-05-02 04:02:44,808 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=888, src_lang=en, trg_lang=de, has_trg=True, random_subset=-1)
2024-05-02 04:02:44,808 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=1568, src_lang=en, trg_lang=de, has_trg=True, random_subset=-1)
2024-05-02 04:02:44,808 - INFO - joeynmt.data - First training example:
	[SRC] Al Gore : Averting the climate crisis
	[TRG] Al Gore : Die Abwendung der Klimakatastrophe
2024-05-02 04:02:44,808 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) a
2024-05-02 04:02:44,808 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) die (7) und (8) der (9) ist
2024-05-02 04:02:44,808 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 2004
2024-05-02 04:02:44,809 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 2004
2024-05-02 04:02:44,810 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-02 04:02:44,915 - INFO - joeynmt.model - Enc-dec model built.
2024-05-02 04:02:44,918 - INFO - joeynmt.model - Total params: 3925248
2024-05-02 04:02:44,919 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2024-05-02 04:02:44,919 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2024-05-02 04:02:44,920 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2024-05-02 04:02:44,920 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=8)
2024-05-02 04:02:44,920 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2048
	effective batch size (w. parallel & accumulation): 2048
2024-05-02 04:02:44,920 - INFO - joeynmt.training - EPOCH 1
2024-05-02 04:03:18,517 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     3.126262, Batch Acc: 0.200719, Tokens per Sec:     2021, Lr: 0.000300
2024-05-02 04:03:50,506 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     2.807392, Batch Acc: 0.251282, Tokens per Sec:     2103, Lr: 0.000300
2024-05-02 04:04:21,286 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     2.688276, Batch Acc: 0.285439, Tokens per Sec:     2206, Lr: 0.000300
2024-05-02 04:04:50,882 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     2.531602, Batch Acc: 0.311844, Tokens per Sec:     2289, Lr: 0.000300
2024-05-02 04:05:21,105 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     2.486549, Batch Acc: 0.322801, Tokens per Sec:     2247, Lr: 0.000300
2024-05-02 04:05:21,106 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 04:05:21,106 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 04:06:24,940 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.48, ppl:  11.88, acc:   0.33, generation: 63.7790[sec], evaluation: 0.0000[sec]
2024-05-02 04:06:24,943 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 04:06:25,163 - INFO - joeynmt.training - Example #0
2024-05-02 04:06:25,163 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 04:06:25,163 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 04:06:25,163 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', ',', 'dass', 'ich', '<unk>', ',', 'dass', 'die', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'dass', 'die', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:06:25,164 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 04:06:25,164 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 04:06:25,164 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk>, dass ich <unk>, dass die <unk> <unk> <unk> <unk>, dass die <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>.
2024-05-02 04:06:25,164 - INFO - joeynmt.training - Example #1
2024-05-02 04:06:25,164 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 04:06:25,164 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 04:06:25,164 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', '<unk>', 'der', '<unk>', 'der', '<unk>', ',', 'dass', 'es', 'die', '<unk>', '<unk>', 'der', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:06:25,164 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 04:06:25,164 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 04:06:25,164 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> <unk> der <unk> der <unk>, dass es die <unk> <unk> der <unk> <unk>.
2024-05-02 04:06:25,165 - INFO - joeynmt.training - Example #2
2024-05-02 04:06:25,165 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 04:06:25,165 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 04:06:25,165 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', '<unk>', '<unk>', 'ist', ',', '<unk>', '<unk>', ',', 'die', '<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:06:25,165 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 04:06:25,165 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 04:06:25,165 - INFO - joeynmt.training - 	Hypothesis: Die <unk> <unk> ist, <unk> <unk>, die <unk> <unk> <unk> <unk>.
2024-05-02 04:06:25,165 - INFO - joeynmt.training - Example #3
2024-05-02 04:06:25,165 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 04:06:25,165 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 04:06:25,165 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', '<unk>', 'und', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:06:25,166 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 04:06:25,166 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 04:06:25,166 - INFO - joeynmt.training - 	Hypothesis: Es <unk> <unk> und <unk> <unk> <unk>.
2024-05-02 04:06:25,166 - INFO - joeynmt.training - Example #4
2024-05-02 04:06:25,166 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 04:06:25,166 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 04:06:25,166 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', '<unk>', '<unk>', 'ich', 'ich', 'ich', 'ich', '<unk>', '<unk>', ',', 'was', 'das', 'das', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:06:25,166 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 04:06:25,167 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 04:06:25,167 - INFO - joeynmt.training - 	Hypothesis: Die <unk> <unk> ich ich ich ich <unk> <unk>, was das das <unk> <unk>.
2024-05-02 04:06:56,315 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     2.362620, Batch Acc: 0.340482, Tokens per Sec:     2169, Lr: 0.000300
2024-05-02 04:07:27,031 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     2.325246, Batch Acc: 0.350348, Tokens per Sec:     2191, Lr: 0.000300
2024-05-02 04:07:57,629 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     2.243126, Batch Acc: 0.362521, Tokens per Sec:     2225, Lr: 0.000300
2024-05-02 04:08:28,974 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     2.310081, Batch Acc: 0.366851, Tokens per Sec:     2139, Lr: 0.000300
2024-05-02 04:08:59,626 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     2.112565, Batch Acc: 0.374656, Tokens per Sec:     2254, Lr: 0.000300
2024-05-02 04:08:59,627 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 04:08:59,627 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 04:09:43,660 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.20, ppl:   9.02, acc:   0.38, generation: 43.9839[sec], evaluation: 0.0000[sec]
2024-05-02 04:09:43,662 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 04:09:43,888 - INFO - joeynmt.training - Example #0
2024-05-02 04:09:43,889 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 04:09:43,889 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 04:09:43,889 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'ich', 'zwei', '<unk>', ',', 'dass', 'die', '<unk>', '<unk>', '<unk>', ',', 'die', '<unk>', '<unk>', ',', 'die', '<unk>', '<unk>', '<unk>', ',', 'die', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'die', '<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:09:43,889 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 04:09:43,889 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 04:09:43,889 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> ich zwei <unk>, dass die <unk> <unk> <unk>, die <unk> <unk>, die <unk> <unk> <unk>, die <unk> <unk> <unk> <unk> <unk>, die <unk> <unk> <unk> <unk>.
2024-05-02 04:09:43,889 - INFO - joeynmt.training - Example #1
2024-05-02 04:09:43,889 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 04:09:43,889 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 04:09:43,890 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', '<unk>', ',', 'weil', 'es', 'nicht', '<unk>', ',', 'weil', 'es', 'nicht', 'die', '<unk>', '.', '</s>']
2024-05-02 04:09:43,890 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 04:09:43,890 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 04:09:43,890 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> <unk>, weil es nicht <unk>, weil es nicht die <unk>.
2024-05-02 04:09:43,890 - INFO - joeynmt.training - Example #2
2024-05-02 04:09:43,890 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 04:09:43,890 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 04:09:43,890 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', '<unk>', 'ist', '<unk>', 'ist', ',', '<unk>', '<unk>', ',', 'die', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:09:43,891 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 04:09:43,891 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 04:09:43,891 - INFO - joeynmt.training - 	Hypothesis: Die <unk> ist <unk> ist, <unk> <unk>, die <unk> <unk>.
2024-05-02 04:09:43,891 - INFO - joeynmt.training - Example #3
2024-05-02 04:09:43,891 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 04:09:43,891 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 04:09:43,891 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '.', '</s>']
2024-05-02 04:09:43,891 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 04:09:43,891 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 04:09:43,891 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk>.
2024-05-02 04:09:43,891 - INFO - joeynmt.training - Example #4
2024-05-02 04:09:43,892 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 04:09:43,892 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 04:09:43,892 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', '<unk>', '<unk>', 'ich', 'Ihnen', 'eine', '<unk>', '<unk>', ',', 'was', 'das', 'ist', 'das', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:09:43,892 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 04:09:43,892 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 04:09:43,892 - INFO - joeynmt.training - 	Hypothesis: Die <unk> <unk> ich Ihnen eine <unk> <unk>, was das ist das <unk> <unk>.
2024-05-02 04:10:14,661 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     2.320046, Batch Acc: 0.387629, Tokens per Sec:     2181, Lr: 0.000300
2024-05-02 04:10:45,463 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     2.306488, Batch Acc: 0.390545, Tokens per Sec:     2202, Lr: 0.000300
2024-05-02 04:11:16,294 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     2.097760, Batch Acc: 0.399956, Tokens per Sec:     2224, Lr: 0.000300
2024-05-02 04:11:47,879 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     2.095229, Batch Acc: 0.400377, Tokens per Sec:     2117, Lr: 0.000300
2024-05-02 04:12:19,290 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     1.928549, Batch Acc: 0.413755, Tokens per Sec:     2249, Lr: 0.000300
2024-05-02 04:12:19,290 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 04:12:19,290 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 04:13:06,312 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.03, ppl:   7.59, acc:   0.41, generation: 46.9684[sec], evaluation: 0.0000[sec]
2024-05-02 04:13:06,314 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 04:13:06,542 - INFO - joeynmt.training - Example #0
2024-05-02 04:13:06,542 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 04:13:06,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 04:13:06,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'ich', 'diese', 'zwei', '<unk>', ',', 'die', '<unk>', ',', 'die', '<unk>', '<unk>', ',', 'die', 'die', '<unk>', '<unk>', ',', 'die', 'die', 'letzten', '<unk>', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', 'hat', '.', '</s>']
2024-05-02 04:13:06,542 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 04:13:06,543 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 04:13:06,543 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> ich diese zwei <unk>, die <unk>, die <unk> <unk>, die die <unk> <unk>, die die letzten <unk> <unk> <unk> <unk>, <unk> <unk> <unk> <unk>, <unk> <unk> <unk> hat.
2024-05-02 04:13:06,543 - INFO - joeynmt.training - Example #1
2024-05-02 04:13:06,543 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 04:13:06,543 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 04:13:06,543 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'dieser', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'nicht', 'die', '<unk>', 'der', '<unk>', '.', '</s>']
2024-05-02 04:13:06,543 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 04:13:06,543 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 04:13:06,543 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> dieser Problem, weil es nicht die <unk> nicht die <unk> der <unk>.
2024-05-02 04:13:06,543 - INFO - joeynmt.training - Example #2
2024-05-02 04:13:06,544 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 04:13:06,544 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 04:13:06,544 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', '<unk>', 'ist', ',', 'ist', ',', 'in', 'einem', '<unk>', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 04:13:06,544 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 04:13:06,544 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 04:13:06,544 - INFO - joeynmt.training - 	Hypothesis: Der <unk> ist, ist, in einem <unk> <unk> des <unk>.
2024-05-02 04:13:06,544 - INFO - joeynmt.training - Example #3
2024-05-02 04:13:06,544 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 04:13:06,544 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 04:13:06,544 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '.', '</s>']
2024-05-02 04:13:06,545 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 04:13:06,545 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 04:13:06,545 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>.
2024-05-02 04:13:06,545 - INFO - joeynmt.training - Example #4
2024-05-02 04:13:06,545 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 04:13:06,545 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 04:13:06,545 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', ',', 'dass', 'ich', 'Ihnen', 'eine', '<unk>', '<unk>', '<unk>', ',', 'was', 'das', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:13:06,545 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 04:13:06,546 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 04:13:06,546 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk>, dass ich Ihnen eine <unk> <unk> <unk>, was das <unk> <unk> <unk>.
2024-05-02 04:13:37,949 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     1.881182, Batch Acc: 0.423019, Tokens per Sec:     2131, Lr: 0.000300
2024-05-02 04:14:09,136 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     1.943028, Batch Acc: 0.424905, Tokens per Sec:     2188, Lr: 0.000300
2024-05-02 04:14:39,517 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     2.057819, Batch Acc: 0.427660, Tokens per Sec:     2202, Lr: 0.000300
2024-05-02 04:15:11,793 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     2.045493, Batch Acc: 0.429376, Tokens per Sec:     2086, Lr: 0.000300
2024-05-02 04:15:44,991 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     1.856341, Batch Acc: 0.440612, Tokens per Sec:     2035, Lr: 0.000300
2024-05-02 04:15:44,991 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 04:15:44,991 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 04:16:30,802 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.91, ppl:   6.73, acc:   0.43, generation: 45.7609[sec], evaluation: 0.0000[sec]
2024-05-02 04:16:30,805 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 04:16:31,038 - INFO - joeynmt.training - Example #0
2024-05-02 04:16:31,038 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 04:16:31,038 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 04:16:31,038 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'ich', 'diese', '<unk>', '<unk>', ',', 'dass', 'die', '<unk>', '<unk>', '<unk>', ',', 'die', '<unk>', 'für', 'die', '<unk>', '<unk>', ',', 'für', 'die', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:16:31,038 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 04:16:31,039 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 04:16:31,039 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr ich diese <unk> <unk>, dass die <unk> <unk> <unk>, die <unk> für die <unk> <unk>, für die <unk> <unk> <unk>, <unk> <unk>, <unk> <unk>, <unk> <unk>, <unk> <unk> <unk>.
2024-05-02 04:16:31,039 - INFO - joeynmt.training - Example #1
2024-05-02 04:16:31,039 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 04:16:31,039 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 04:16:31,039 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'der', '<unk>', 'des', '<unk>', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 04:16:31,039 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 04:16:31,039 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 04:16:31,039 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> der <unk> des <unk>, weil es nicht die <unk> des <unk>.
2024-05-02 04:16:31,039 - INFO - joeynmt.training - Example #2
2024-05-02 04:16:31,040 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 04:16:31,040 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 04:16:31,040 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', '<unk>', 'ist', '<unk>', ',', 'in', 'einem', '<unk>', 'der', '<unk>', '.', '</s>']
2024-05-02 04:16:31,040 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 04:16:31,040 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 04:16:31,040 - INFO - joeynmt.training - 	Hypothesis: Der <unk> ist <unk>, in einem <unk> der <unk>.
2024-05-02 04:16:31,040 - INFO - joeynmt.training - Example #3
2024-05-02 04:16:31,040 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 04:16:31,040 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 04:16:31,040 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '.', '</s>']
2024-05-02 04:16:31,041 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 04:16:31,041 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 04:16:31,041 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>.
2024-05-02 04:16:31,041 - INFO - joeynmt.training - Example #4
2024-05-02 04:16:31,041 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 04:16:31,041 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 04:16:31,041 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', ',', 'ich', 'Ihnen', 'Ihnen', '<unk>', 'wird', ',', 'was', 'was', '<unk>', '<unk>', 'ist', ',', 'was', '<unk>', '<unk>', 'Jahren', 'ist', '.', '</s>']
2024-05-02 04:16:31,041 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 04:16:31,041 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 04:16:31,042 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk>, ich Ihnen Ihnen <unk> wird, was was <unk> <unk> ist, was <unk> <unk> Jahren ist.
2024-05-02 04:17:04,391 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     1.850894, Batch Acc: 0.446118, Tokens per Sec:     2071, Lr: 0.000300
2024-05-02 04:17:38,113 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     1.829015, Batch Acc: 0.444732, Tokens per Sec:     2026, Lr: 0.000300
2024-05-02 04:18:11,863 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     1.775257, Batch Acc: 0.456428, Tokens per Sec:     2011, Lr: 0.000300
2024-05-02 04:18:44,026 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     1.808231, Batch Acc: 0.456718, Tokens per Sec:     2056, Lr: 0.000300
2024-05-02 04:19:17,183 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     1.813071, Batch Acc: 0.461538, Tokens per Sec:     2046, Lr: 0.000300
2024-05-02 04:19:17,183 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 04:19:17,183 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 04:20:20,782 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.81, ppl:   6.13, acc:   0.45, generation: 63.5425[sec], evaluation: 0.0000[sec]
2024-05-02 04:20:20,784 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 04:20:21,024 - INFO - joeynmt.training - Example #0
2024-05-02 04:20:21,024 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 04:20:21,024 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 04:20:21,025 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'habe', 'ich', 'diese', 'zwei', '<unk>', ',', 'dass', 'die', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'die', 'für', 'die', '<unk>', 'der', 'letzten', 'drei', 'Millionen', 'Jahren', 'hat', 'hat', 'die', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', 'die', 'die', '<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:20:21,025 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 04:20:21,025 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 04:20:21,025 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> habe ich diese zwei <unk>, dass die <unk> <unk> <unk> <unk>, die für die <unk> der letzten drei Millionen Jahren hat hat die <unk> <unk> <unk> <unk> <unk>, <unk> <unk>, <unk> <unk> <unk>, <unk> <unk> <unk>, <unk> <unk>, die die <unk> <unk> <unk> <unk>.
2024-05-02 04:20:21,025 - INFO - joeynmt.training - Example #1
2024-05-02 04:20:21,025 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 04:20:21,025 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 04:20:21,025 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'dieser', '<unk>', '<unk>', 'dieser', '<unk>', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:20:21,026 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 04:20:21,026 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 04:20:21,026 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> dieser <unk> <unk> dieser <unk>, weil es nicht die <unk> des <unk> <unk>.
2024-05-02 04:20:21,026 - INFO - joeynmt.training - Example #2
2024-05-02 04:20:21,026 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 04:20:21,026 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 04:20:21,026 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', '<unk>', '<unk>', 'ist', 'ist', ',', 'in', 'einem', '<unk>', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:20:21,026 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 04:20:21,027 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 04:20:21,027 - INFO - joeynmt.training - 	Hypothesis: Der <unk> <unk> ist ist, in einem <unk> <unk> des <unk> <unk>.
2024-05-02 04:20:21,027 - INFO - joeynmt.training - Example #3
2024-05-02 04:20:21,027 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 04:20:21,027 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 04:20:21,027 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '.', '</s>']
2024-05-02 04:20:21,027 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 04:20:21,027 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 04:20:21,027 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>.
2024-05-02 04:20:21,027 - INFO - joeynmt.training - Example #4
2024-05-02 04:20:21,027 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 04:20:21,028 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 04:20:21,028 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'nächste', '<unk>', '<unk>', ',', 'ich', 'Ihnen', 'Ihnen', '<unk>', 'werden', ',', 'was', 'passiert', 'passiert', ',', 'was', 'passiert', 'ist', ',', 'über', 'den', 'letzten', '25', 'Jahren', '.', '</s>']
2024-05-02 04:20:21,028 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 04:20:21,028 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 04:20:21,028 - INFO - joeynmt.training - 	Hypothesis: Der nächste <unk> <unk>, ich Ihnen Ihnen <unk> werden, was passiert passiert, was passiert ist, über den letzten 25 Jahren.
2024-05-02 04:20:53,628 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     1.915084, Batch Acc: 0.460066, Tokens per Sec:     2043, Lr: 0.000300
2024-05-02 04:21:26,548 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     1.751465, Batch Acc: 0.461824, Tokens per Sec:     2057, Lr: 0.000300
2024-05-02 04:21:58,993 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     1.754249, Batch Acc: 0.471122, Tokens per Sec:     2081, Lr: 0.000300
2024-05-02 04:22:31,539 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     1.790296, Batch Acc: 0.473732, Tokens per Sec:     2128, Lr: 0.000300
2024-05-02 04:22:47,996 - INFO - joeynmt.training - Epoch   1: total training loss 6314.84
2024-05-02 04:22:47,996 - INFO - joeynmt.training - EPOCH 2
2024-05-02 04:23:04,315 - INFO - joeynmt.training - Epoch   2, Step:     3000, Batch Loss:     1.604604, Batch Acc: 0.484915, Tokens per Sec:     2031, Lr: 0.000300
2024-05-02 04:23:04,316 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 04:23:04,316 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 04:23:55,954 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.67, acc:   0.47, generation: 51.5855[sec], evaluation: 0.0000[sec]
2024-05-02 04:23:55,956 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 04:23:56,184 - INFO - joeynmt.helpers - delete models/words/500.ckpt
2024-05-02 04:23:56,187 - INFO - joeynmt.training - Example #0
2024-05-02 04:23:56,187 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 04:23:56,187 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 04:23:56,187 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', ',', 'dass', 'die', '<unk>', '<unk>', '<unk>', ',', 'die', 'für', 'die', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', 'von', '40', 'Prozent', 'der', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 04:23:56,188 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 04:23:56,188 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 04:23:56,188 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk>, dass die <unk> <unk> <unk>, die für die <unk> <unk> <unk>, <unk> <unk> <unk>, <unk> von 40 Prozent der <unk> <unk> <unk>, <unk> <unk> <unk>, <unk> <unk> <unk>, <unk> von 40 Prozent.
2024-05-02 04:23:56,188 - INFO - joeynmt.training - Example #1
2024-05-02 04:23:56,188 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 04:23:56,188 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 04:23:56,188 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'dieses', '<unk>', 'Problem', ',', 'weil', 'es', 'das', 'Problem', 'nicht', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 04:23:56,189 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 04:23:56,189 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 04:23:56,189 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> dieses <unk> Problem, weil es das Problem nicht <unk> des <unk>.
2024-05-02 04:23:56,189 - INFO - joeynmt.training - Example #2
2024-05-02 04:23:56,189 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 04:23:56,189 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 04:23:56,189 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', ',', 'in', 'einem', '<unk>', ',', 'das', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:23:56,189 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 04:23:56,189 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 04:23:56,189 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist, in einem <unk>, das <unk> des <unk> <unk>.
2024-05-02 04:23:56,190 - INFO - joeynmt.training - Example #3
2024-05-02 04:23:56,190 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 04:23:56,190 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 04:23:56,190 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '.', '</s>']
2024-05-02 04:23:56,190 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 04:23:56,190 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 04:23:56,190 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>.
2024-05-02 04:23:56,190 - INFO - joeynmt.training - Example #4
2024-05-02 04:23:56,190 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 04:23:56,190 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 04:23:56,190 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', ',', 'ich', 'Ihnen', '<unk>', '<unk>', ',', 'was', 'passiert', 'ist', ',', 'was', 'passiert', 'ist', '.', '</s>']
2024-05-02 04:23:56,191 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 04:23:56,191 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 04:23:56,191 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk>, ich Ihnen <unk> <unk>, was passiert ist, was passiert ist.
2024-05-02 04:24:28,443 - INFO - joeynmt.training - Epoch   2, Step:     3100, Batch Loss:     1.641523, Batch Acc: 0.482818, Tokens per Sec:     2119, Lr: 0.000300
2024-05-02 04:25:00,868 - INFO - joeynmt.training - Epoch   2, Step:     3200, Batch Loss:     1.709494, Batch Acc: 0.496000, Tokens per Sec:     2144, Lr: 0.000300
2024-05-02 04:25:32,962 - INFO - joeynmt.training - Epoch   2, Step:     3300, Batch Loss:     1.831231, Batch Acc: 0.491925, Tokens per Sec:     2124, Lr: 0.000300
2024-05-02 04:26:06,367 - INFO - joeynmt.training - Epoch   2, Step:     3400, Batch Loss:     1.662608, Batch Acc: 0.493050, Tokens per Sec:     2070, Lr: 0.000300
2024-05-02 04:26:40,331 - INFO - joeynmt.training - Epoch   2, Step:     3500, Batch Loss:     1.714244, Batch Acc: 0.493249, Tokens per Sec:     2019, Lr: 0.000300
2024-05-02 04:26:40,332 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 04:26:40,332 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 04:27:39,346 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.69, ppl:   5.41, acc:   0.48, generation: 58.9561[sec], evaluation: 0.0000[sec]
2024-05-02 04:27:39,348 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 04:27:39,597 - INFO - joeynmt.helpers - delete models/words/1000.ckpt
2024-05-02 04:27:39,607 - INFO - joeynmt.training - Example #0
2024-05-02 04:27:39,607 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 04:27:39,607 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 04:27:39,607 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', ',', 'dass', 'ich', 'diese', 'beiden', '<unk>', '<unk>', ',', 'dass', 'die', '<unk>', '<unk>', '<unk>', ',', 'die', 'für', 'die', 'meisten', '<unk>', 'drei', 'Millionen', 'Jahre', 'hat', 'hat', 'die', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'von', '40', 'Prozent', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'die', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'die', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']
2024-05-02 04:27:39,608 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 04:27:39,608 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 04:27:39,608 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr, dass ich diese beiden <unk> <unk>, dass die <unk> <unk> <unk>, die für die meisten <unk> drei Millionen Jahre hat hat die <unk> <unk> <unk> <unk> <unk>, von 40 Prozent <unk> <unk> <unk> <unk>, die <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>, die <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-02 04:27:39,608 - INFO - joeynmt.training - Example #1
2024-05-02 04:27:39,608 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 04:27:39,608 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 04:27:39,608 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', '<unk>', ',', 'weil', 'es', 'das', '<unk>', 'nicht', 'die', '<unk>', '<unk>', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:27:39,609 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 04:27:39,609 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 04:27:39,609 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> <unk>, weil es das <unk> nicht die <unk> <unk> nicht die <unk> des <unk> <unk>.
2024-05-02 04:27:39,609 - INFO - joeynmt.training - Example #2
2024-05-02 04:27:39,609 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 04:27:39,609 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 04:27:39,609 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', '<unk>', '<unk>', 'ist', ',', 'in', 'einem', 'Sinn', ',', 'der', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:27:39,609 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 04:27:39,609 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 04:27:39,609 - INFO - joeynmt.training - 	Hypothesis: Der <unk> <unk> ist, in einem Sinn, der <unk> des <unk> <unk>.
2024-05-02 04:27:39,610 - INFO - joeynmt.training - Example #3
2024-05-02 04:27:39,610 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 04:27:39,610 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 04:27:39,610 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '.', '</s>']
2024-05-02 04:27:39,610 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 04:27:39,610 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 04:27:39,610 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>.
2024-05-02 04:27:39,610 - INFO - joeynmt.training - Example #4
2024-05-02 04:27:39,610 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 04:27:39,610 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 04:27:39,610 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'dass', 'Sie', 'einen', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'ist', '.', '</s>']
2024-05-02 04:27:39,611 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 04:27:39,611 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 04:27:39,611 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, dass Sie einen <unk> <unk> <unk> <unk>, was über die letzten 25 Jahren ist.
2024-05-02 04:28:12,533 - INFO - joeynmt.training - Epoch   2, Step:     3600, Batch Loss:     1.868198, Batch Acc: 0.499678, Tokens per Sec:     2014, Lr: 0.000300
2024-05-02 04:28:45,822 - INFO - joeynmt.training - Epoch   2, Step:     3700, Batch Loss:     1.628660, Batch Acc: 0.500221, Tokens per Sec:     2035, Lr: 0.000300
2024-05-02 04:29:19,759 - INFO - joeynmt.training - Epoch   2, Step:     3800, Batch Loss:     1.759064, Batch Acc: 0.500314, Tokens per Sec:     1971, Lr: 0.000300
2024-05-02 04:29:53,151 - INFO - joeynmt.training - Epoch   2, Step:     3900, Batch Loss:     1.671862, Batch Acc: 0.496601, Tokens per Sec:     2062, Lr: 0.000300
2024-05-02 04:30:25,879 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     1.580673, Batch Acc: 0.501271, Tokens per Sec:     2091, Lr: 0.000300
2024-05-02 04:30:25,880 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 04:30:25,880 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 04:31:16,155 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.65, ppl:   5.22, acc:   0.49, generation: 50.2100[sec], evaluation: 0.0000[sec]
2024-05-02 04:31:16,158 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 04:31:16,458 - INFO - joeynmt.helpers - delete models/words/1500.ckpt
2024-05-02 04:31:16,459 - INFO - joeynmt.training - Example #0
2024-05-02 04:31:16,460 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 04:31:16,460 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 04:31:16,460 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'ich', 'diese', 'zwei', '<unk>', '<unk>', ',', 'dass', 'die', '<unk>', '<unk>', '<unk>', ',', 'die', 'für', 'die', 'meisten', '<unk>', '<unk>', ',', 'die', 'für', 'die', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 04:31:16,460 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 04:31:16,460 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 04:31:16,460 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr ich diese zwei <unk> <unk>, dass die <unk> <unk> <unk>, die für die meisten <unk> <unk>, die für die <unk> <unk>, <unk> <unk>, <unk> <unk> <unk>, <unk> von 40 Prozent.
2024-05-02 04:31:16,461 - INFO - joeynmt.training - Example #1
2024-05-02 04:31:16,461 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 04:31:16,461 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 04:31:16,461 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'dieses', '<unk>', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 04:31:16,461 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 04:31:16,461 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 04:31:16,461 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> dieses <unk> dieses Problem, weil es nicht die <unk> des <unk>.
2024-05-02 04:31:16,461 - INFO - joeynmt.training - Example #2
2024-05-02 04:31:16,462 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 04:31:16,462 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 04:31:16,462 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', ',', 'das', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:31:16,462 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 04:31:16,462 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 04:31:16,462 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist, das <unk> des <unk> <unk>.
2024-05-02 04:31:16,463 - INFO - joeynmt.training - Example #3
2024-05-02 04:31:16,463 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 04:31:16,463 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 04:31:16,463 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'im', 'Sommer', '.', '</s>']
2024-05-02 04:31:16,463 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 04:31:16,463 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 04:31:16,463 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> im Sommer.
2024-05-02 04:31:16,463 - INFO - joeynmt.training - Example #4
2024-05-02 04:31:16,464 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 04:31:16,464 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 04:31:16,464 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', '<unk>', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'ist', '.', '</s>']
2024-05-02 04:31:16,464 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 04:31:16,464 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 04:31:16,464 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen <unk>, was in den letzten 25 Jahren ist.
2024-05-02 04:31:57,728 - INFO - joeynmt.training - Epoch   2, Step:     4100, Batch Loss:     1.773405, Batch Acc: 0.503209, Tokens per Sec:     1615, Lr: 0.000300
2024-05-02 04:32:35,646 - INFO - joeynmt.training - Epoch   2, Step:     4200, Batch Loss:     1.516855, Batch Acc: 0.507495, Tokens per Sec:     1818, Lr: 0.000300
2024-05-02 04:33:15,643 - INFO - joeynmt.training - Epoch   2, Step:     4300, Batch Loss:     1.623428, Batch Acc: 0.501264, Tokens per Sec:     1681, Lr: 0.000300
2024-05-02 04:33:54,810 - INFO - joeynmt.training - Epoch   2, Step:     4400, Batch Loss:     1.611235, Batch Acc: 0.509310, Tokens per Sec:     1759, Lr: 0.000300
2024-05-02 04:34:34,101 - INFO - joeynmt.training - Epoch   2, Step:     4500, Batch Loss:     1.606679, Batch Acc: 0.501820, Tokens per Sec:     1727, Lr: 0.000300
2024-05-02 04:34:34,101 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 04:34:34,101 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 04:35:36,111 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   4.99, acc:   0.50, generation: 61.9417[sec], evaluation: 0.0000[sec]
2024-05-02 04:35:36,114 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 04:35:36,397 - INFO - joeynmt.helpers - delete models/words/2000.ckpt
2024-05-02 04:35:36,401 - INFO - joeynmt.training - Example #0
2024-05-02 04:35:36,401 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 04:35:36,401 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 04:35:36,401 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'ich', 'diese', 'zwei', '<unk>', '<unk>', ',', 'dass', 'die', '<unk>', '<unk>', '<unk>', ',', 'die', 'für', 'die', 'meisten', '<unk>', '<unk>', '<unk>', ',', 'die', '<unk>', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', 'hat', ',', 'von', '40', 'Prozent', '<unk>', '<unk>', '<unk>', '<unk>', ',', '<unk>', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 04:35:36,402 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 04:35:36,402 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 04:35:36,402 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr ich diese zwei <unk> <unk>, dass die <unk> <unk> <unk>, die für die meisten <unk> <unk> <unk>, die <unk> <unk> <unk> <unk>, <unk> <unk> <unk> hat, von 40 Prozent <unk> <unk> <unk> <unk>, <unk> von 40 Prozent.
2024-05-02 04:35:36,402 - INFO - joeynmt.training - Example #1
2024-05-02 04:35:36,402 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 04:35:36,402 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 04:35:36,403 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', '<unk>', 'das', 'Problem', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:35:36,403 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 04:35:36,403 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 04:35:36,403 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> <unk> das Problem dieses Problem, weil es nicht die <unk> des <unk> <unk>.
2024-05-02 04:35:36,403 - INFO - joeynmt.training - Example #2
2024-05-02 04:35:36,403 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 04:35:36,403 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 04:35:36,404 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', ',', 'in', 'einem', 'Sinn', ',', 'der', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 04:35:36,404 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 04:35:36,404 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 04:35:36,404 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist, in einem Sinn, der <unk> des <unk>.
2024-05-02 04:35:36,404 - INFO - joeynmt.training - Example #3
2024-05-02 04:35:36,405 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 04:35:36,405 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 04:35:36,405 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'im', 'Sommer', '.', '</s>']
2024-05-02 04:35:36,405 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 04:35:36,405 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 04:35:36,405 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> im Sommer.
2024-05-02 04:35:36,405 - INFO - joeynmt.training - Example #4
2024-05-02 04:35:36,406 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 04:35:36,406 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 04:35:36,406 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'ich', 'Ihnen', 'zeigen', ',', 'dass', 'Sie', 'ein', '<unk>', '<unk>', '<unk>', 'werden', ',', 'was', 'über', 'den', 'letzten', '25', 'Jahren', 'ist', '.', '</s>']
2024-05-02 04:35:36,406 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 04:35:36,406 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 04:35:36,406 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, ich Ihnen zeigen, dass Sie ein <unk> <unk> <unk> werden, was über den letzten 25 Jahren ist.
2024-05-02 04:36:18,060 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     1.594979, Batch Acc: 0.511173, Tokens per Sec:     1627, Lr: 0.000300
2024-05-02 04:36:55,305 - INFO - joeynmt.training - Epoch   2, Step:     4700, Batch Loss:     1.471437, Batch Acc: 0.507133, Tokens per Sec:     1803, Lr: 0.000300
2024-05-02 04:37:32,209 - INFO - joeynmt.training - Epoch   2, Step:     4800, Batch Loss:     1.638374, Batch Acc: 0.513496, Tokens per Sec:     1885, Lr: 0.000300
2024-05-02 04:38:15,685 - INFO - joeynmt.training - Epoch   2, Step:     4900, Batch Loss:     1.627684, Batch Acc: 0.512216, Tokens per Sec:     1562, Lr: 0.000300
2024-05-02 04:39:01,967 - INFO - joeynmt.training - Epoch   2, Step:     5000, Batch Loss:     1.538758, Batch Acc: 0.511480, Tokens per Sec:     1479, Lr: 0.000300
2024-05-02 04:39:01,968 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 04:39:01,968 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 04:40:19,429 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.87, acc:   0.51, generation: 77.3805[sec], evaluation: 0.0000[sec]
2024-05-02 04:40:19,433 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 04:40:19,761 - INFO - joeynmt.helpers - delete models/words/2500.ckpt
2024-05-02 04:40:19,765 - INFO - joeynmt.training - Example #0
2024-05-02 04:40:19,765 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 04:40:19,765 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 04:40:19,765 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'beiden', '<unk>', ',', 'so', 'dass', 'die', '<unk>', '<unk>', '<unk>', ',', 'die', 'für', 'die', 'meisten', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'die', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:40:19,766 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 04:40:19,766 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 04:40:19,766 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese beiden <unk>, so dass die <unk> <unk> <unk>, die für die meisten <unk> <unk> <unk> <unk>, die die Größe der <unk> <unk> <unk> <unk> <unk> <unk>.
2024-05-02 04:40:19,766 - INFO - joeynmt.training - Example #1
2024-05-02 04:40:19,766 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 04:40:19,766 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 04:40:19,766 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'das', '<unk>', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 04:40:19,767 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 04:40:19,767 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 04:40:19,767 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> das <unk> dieses Problem, weil es nicht die <unk> des <unk>.
2024-05-02 04:40:19,767 - INFO - joeynmt.training - Example #2
2024-05-02 04:40:19,767 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 04:40:19,767 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 04:40:19,767 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', ',', 'in', 'einem', '<unk>', ',', 'der', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 04:40:19,768 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 04:40:19,768 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 04:40:19,768 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist, in einem <unk>, der <unk> des <unk>.
2024-05-02 04:40:19,768 - INFO - joeynmt.training - Example #3
2024-05-02 04:40:19,768 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 04:40:19,768 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 04:40:19,768 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '.', '</s>']
2024-05-02 04:40:19,769 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 04:40:19,769 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 04:40:19,769 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>.
2024-05-02 04:40:19,769 - INFO - joeynmt.training - Example #4
2024-05-02 04:40:19,769 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 04:40:19,769 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 04:40:19,770 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'nächste', 'Folie', ',', 'ich', 'Ihnen', 'zeigen', ',', 'dass', 'ein', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'was', 'über', 'den', 'letzten', '25', 'Jahren', 'ist', '.', '</s>']
2024-05-02 04:40:19,770 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 04:40:19,770 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 04:40:19,770 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie, ich Ihnen zeigen, dass ein <unk> <unk> <unk> <unk>, was über den letzten 25 Jahren ist.
2024-05-02 04:41:03,994 - INFO - joeynmt.training - Epoch   2, Step:     5100, Batch Loss:     1.602520, Batch Acc: 0.514108, Tokens per Sec:     1513, Lr: 0.000300
2024-05-02 04:41:50,388 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     1.563339, Batch Acc: 0.511746, Tokens per Sec:     1437, Lr: 0.000300
2024-05-02 04:42:38,718 - INFO - joeynmt.training - Epoch   2, Step:     5300, Batch Loss:     1.557972, Batch Acc: 0.518760, Tokens per Sec:     1429, Lr: 0.000300
2024-05-02 04:43:26,553 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     1.569186, Batch Acc: 0.517867, Tokens per Sec:     1432, Lr: 0.000300
2024-05-02 04:44:15,437 - INFO - joeynmt.training - Epoch   2, Step:     5500, Batch Loss:     1.674067, Batch Acc: 0.520063, Tokens per Sec:     1413, Lr: 0.000300
2024-05-02 04:44:15,438 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 04:44:15,438 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 04:45:42,502 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.73, acc:   0.51, generation: 86.9692[sec], evaluation: 0.0000[sec]
2024-05-02 04:45:42,506 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 04:45:42,849 - INFO - joeynmt.helpers - delete models/words/3000.ckpt
2024-05-02 04:45:42,853 - INFO - joeynmt.training - Example #0
2024-05-02 04:45:42,853 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 04:45:42,853 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 04:45:42,853 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', ',', 'dass', 'ich', 'diese', 'zwei', '<unk>', '<unk>', ',', 'dass', 'das', '<unk>', '<unk>', '<unk>', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'hat', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', ',', 'hat', 'von', '40', 'Prozent', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:45:42,854 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 04:45:42,854 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 04:45:42,854 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr, dass ich diese zwei <unk> <unk>, dass das <unk> <unk> <unk>, die für die meisten drei Millionen Jahre hat die Größe der <unk> <unk> <unk>, hat von 40 Prozent <unk> <unk> <unk>.
2024-05-02 04:45:42,854 - INFO - joeynmt.training - Example #1
2024-05-02 04:45:42,855 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 04:45:42,855 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 04:45:42,855 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', '<unk>', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', 'zeigen', '.', '</s>']
2024-05-02 04:45:42,855 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 04:45:42,855 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 04:45:42,856 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses <unk> Problem, weil es nicht die <unk> des <unk> zeigen.
2024-05-02 04:45:42,856 - INFO - joeynmt.training - Example #2
2024-05-02 04:45:42,856 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 04:45:42,856 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 04:45:42,856 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', ',', 'im', '<unk>', '<unk>', ',', 'das', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:45:42,856 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 04:45:42,857 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 04:45:42,857 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist, im <unk> <unk>, das <unk> des <unk> <unk>.
2024-05-02 04:45:42,857 - INFO - joeynmt.training - Example #3
2024-05-02 04:45:42,857 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 04:45:42,857 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 04:45:42,857 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'im', 'Sommer', '.', '</s>']
2024-05-02 04:45:42,858 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 04:45:42,858 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 04:45:42,858 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> im Sommer.
2024-05-02 04:45:42,858 - INFO - joeynmt.training - Example #4
2024-05-02 04:45:42,858 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 04:45:42,858 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 04:45:42,858 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'dass', 'das', 'das', '<unk>', 'über', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 04:45:42,859 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 04:45:42,859 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 04:45:42,859 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen, dass das das <unk> über den letzten 25 Jahren passiert ist.
2024-05-02 04:46:36,964 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     1.470227, Batch Acc: 0.520246, Tokens per Sec:     1241, Lr: 0.000300
2024-05-02 04:47:21,917 - INFO - joeynmt.training - Epoch   2, Step:     5700, Batch Loss:     1.497059, Batch Acc: 0.517766, Tokens per Sec:     1489, Lr: 0.000300
2024-05-02 04:48:15,478 - INFO - joeynmt.training - Epoch   2, Step:     5800, Batch Loss:     1.564449, Batch Acc: 0.521556, Tokens per Sec:     1280, Lr: 0.000300
2024-05-02 04:49:05,477 - INFO - joeynmt.training - Epoch   2: total training loss 4738.05
2024-05-02 04:49:05,478 - INFO - joeynmt.training - EPOCH 3
2024-05-02 04:49:09,014 - INFO - joeynmt.training - Epoch   3, Step:     5900, Batch Loss:     1.543029, Batch Acc: 0.515098, Tokens per Sec:     1293, Lr: 0.000300
2024-05-02 04:50:02,499 - INFO - joeynmt.training - Epoch   3, Step:     6000, Batch Loss:     1.397591, Batch Acc: 0.543004, Tokens per Sec:     1269, Lr: 0.000300
2024-05-02 04:50:02,499 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 04:50:02,500 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 04:51:22,012 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.53, ppl:   4.64, acc:   0.52, generation: 79.4258[sec], evaluation: 0.0000[sec]
2024-05-02 04:51:22,015 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 04:51:22,333 - INFO - joeynmt.helpers - delete models/words/3500.ckpt
2024-05-02 04:51:22,335 - INFO - joeynmt.training - Example #0
2024-05-02 04:51:22,335 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 04:51:22,335 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 04:51:22,336 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', ',', 'dass', 'ich', 'diese', 'zwei', '<unk>', '<unk>', ',', 'dass', 'das', '<unk>', '<unk>', '<unk>', ',', 'die', 'für', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'hat', 'die', '<unk>', '<unk>', '<unk>', '<unk>', ',', '<unk>', 'von', '40', '%', '<unk>', '.', '</s>']
2024-05-02 04:51:22,336 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 04:51:22,336 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 04:51:22,336 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr, dass ich diese zwei <unk> <unk>, dass das <unk> <unk> <unk>, die für die meisten der letzten drei Millionen Jahre hat die <unk> <unk> <unk> <unk>, <unk> von 40% <unk>.
2024-05-02 04:51:22,336 - INFO - joeynmt.training - Example #1
2024-05-02 04:51:22,337 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 04:51:22,337 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 04:51:22,337 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:51:22,337 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 04:51:22,337 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 04:51:22,337 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses Problem, weil es nicht die <unk> des <unk> <unk>.
2024-05-02 04:51:22,338 - INFO - joeynmt.training - Example #2
2024-05-02 04:51:22,338 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 04:51:22,338 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 04:51:22,338 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', '<unk>', 'ist', ',', 'in', 'einem', 'Sinn', ',', 'der', '<unk>', '<unk>', 'des', 'globalen', '<unk>', '.', '</s>']
2024-05-02 04:51:22,338 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 04:51:22,338 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 04:51:22,338 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis <unk> ist, in einem Sinn, der <unk> <unk> des globalen <unk>.
2024-05-02 04:51:22,339 - INFO - joeynmt.training - Example #3
2024-05-02 04:51:22,339 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 04:51:22,339 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 04:51:22,339 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', 'Sommer', '.', '</s>']
2024-05-02 04:51:22,339 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 04:51:22,339 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 04:51:22,339 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in Sommer.
2024-05-02 04:51:22,340 - INFO - joeynmt.training - Example #4
2024-05-02 04:51:22,340 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 04:51:22,340 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 04:51:22,340 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'dass', 'das', 'das', '<unk>', '<unk>', 'ist', ',', 'was', 'über', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 04:51:22,340 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 04:51:22,341 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 04:51:22,341 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen, dass das das <unk> <unk> ist, was über den letzten 25 Jahren passiert ist.
2024-05-02 04:52:16,676 - INFO - joeynmt.training - Epoch   3, Step:     6100, Batch Loss:     1.493077, Batch Acc: 0.537877, Tokens per Sec:     1238, Lr: 0.000300
2024-05-02 04:53:10,487 - INFO - joeynmt.training - Epoch   3, Step:     6200, Batch Loss:     1.321272, Batch Acc: 0.540916, Tokens per Sec:     1298, Lr: 0.000300
2024-05-02 04:53:53,822 - INFO - joeynmt.training - Epoch   3, Step:     6300, Batch Loss:     1.430525, Batch Acc: 0.540244, Tokens per Sec:     1549, Lr: 0.000300
2024-05-02 04:54:41,893 - INFO - joeynmt.training - Epoch   3, Step:     6400, Batch Loss:     1.409332, Batch Acc: 0.536531, Tokens per Sec:     1368, Lr: 0.000300
2024-05-02 04:55:32,937 - INFO - joeynmt.training - Epoch   3, Step:     6500, Batch Loss:     1.372527, Batch Acc: 0.544344, Tokens per Sec:     1377, Lr: 0.000300
2024-05-02 04:55:32,937 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 04:55:32,938 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 04:57:04,996 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.56, acc:   0.52, generation: 91.9653[sec], evaluation: 0.0000[sec]
2024-05-02 04:57:04,999 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 04:57:05,372 - INFO - joeynmt.helpers - delete models/words/4000.ckpt
2024-05-02 04:57:05,377 - INFO - joeynmt.training - Example #0
2024-05-02 04:57:05,377 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 04:57:05,377 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 04:57:05,377 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'beiden', '<unk>', '<unk>', ',', 'dass', 'das', '<unk>', 'Eis', '<unk>', '<unk>', ',', 'das', 'für', 'die', 'meisten', '<unk>', 'drei', 'Millionen', 'Jahre', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:57:05,378 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 04:57:05,378 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 04:57:05,378 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese beiden beiden <unk> <unk>, dass das <unk> Eis <unk> <unk>, das für die meisten <unk> drei Millionen Jahre <unk> <unk>.
2024-05-02 04:57:05,378 - INFO - joeynmt.training - Example #1
2024-05-02 04:57:05,378 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 04:57:05,379 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 04:57:05,379 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'dieses', '<unk>', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 04:57:05,379 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 04:57:05,379 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 04:57:05,379 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> dieses <unk>, weil es nicht die <unk> des <unk> <unk>.
2024-05-02 04:57:05,380 - INFO - joeynmt.training - Example #2
2024-05-02 04:57:05,380 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 04:57:05,380 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 04:57:05,380 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', ',', 'in', 'einem', 'Sinne', ',', 'der', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 04:57:05,380 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 04:57:05,381 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 04:57:05,381 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist, in einem Sinne, der <unk> des <unk>.
2024-05-02 04:57:05,381 - INFO - joeynmt.training - Example #3
2024-05-02 04:57:05,381 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 04:57:05,381 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 04:57:05,381 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '.', '</s>']
2024-05-02 04:57:05,382 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 04:57:05,382 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 04:57:05,382 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>.
2024-05-02 04:57:05,382 - INFO - joeynmt.training - Example #4
2024-05-02 04:57:05,382 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 04:57:05,382 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 04:57:05,382 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'dass', 'ein', '<unk>', '<unk>', 'sein', 'wird', '.', '</s>']
2024-05-02 04:57:05,383 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 04:57:05,383 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 04:57:05,383 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen, dass ein <unk> <unk> sein wird.
2024-05-02 04:57:56,104 - INFO - joeynmt.training - Epoch   3, Step:     6600, Batch Loss:     1.426181, Batch Acc: 0.539937, Tokens per Sec:     1341, Lr: 0.000300
2024-05-02 04:58:43,858 - INFO - joeynmt.training - Epoch   3, Step:     6700, Batch Loss:     1.326282, Batch Acc: 0.544175, Tokens per Sec:     1453, Lr: 0.000300
2024-05-02 04:59:31,223 - INFO - joeynmt.training - Epoch   3, Step:     6800, Batch Loss:     1.400299, Batch Acc: 0.541877, Tokens per Sec:     1477, Lr: 0.000300
2024-05-02 05:00:19,701 - INFO - joeynmt.training - Epoch   3, Step:     6900, Batch Loss:     1.456096, Batch Acc: 0.536180, Tokens per Sec:     1379, Lr: 0.000300
2024-05-02 05:01:05,072 - INFO - joeynmt.training - Epoch   3, Step:     7000, Batch Loss:     1.795566, Batch Acc: 0.537126, Tokens per Sec:     1483, Lr: 0.000300
2024-05-02 05:01:05,072 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 05:01:05,072 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 05:02:45,909 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.54, acc:   0.52, generation: 100.7209[sec], evaluation: 0.0000[sec]
2024-05-02 05:02:45,913 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 05:02:46,307 - INFO - joeynmt.helpers - delete models/words/4500.ckpt
2024-05-02 05:02:46,308 - INFO - joeynmt.training - Example #0
2024-05-02 05:02:46,309 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 05:02:46,309 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 05:02:46,309 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', ',', 'dass', 'ich', 'diese', 'zwei', '<unk>', '<unk>', ',', 'dass', 'das', '<unk>', 'Eis', '<unk>', '<unk>', ',', 'das', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'hat', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'die', 'die', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', ',']
2024-05-02 05:02:46,309 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 05:02:46,310 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 05:02:46,310 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr, dass ich diese zwei <unk> <unk>, dass das <unk> Eis <unk> <unk>, das für die meisten drei Millionen Jahre hat die Größe der <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>, die die <unk> <unk> <unk> <unk> <unk>,
2024-05-02 05:02:46,310 - INFO - joeynmt.training - Example #1
2024-05-02 05:02:46,310 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 05:02:46,310 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 05:02:46,310 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'dieses', '<unk>', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 05:02:46,311 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 05:02:46,311 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 05:02:46,311 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> dieses <unk> Problem, weil es nicht die <unk> des <unk>.
2024-05-02 05:02:46,311 - INFO - joeynmt.training - Example #2
2024-05-02 05:02:46,311 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 05:02:46,311 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 05:02:46,312 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', '<unk>', 'ist', ',', 'im', 'Sinne', ',', 'das', '<unk>', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 05:02:46,312 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 05:02:46,312 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 05:02:46,312 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis <unk> ist, im Sinne, das <unk> <unk> des <unk>.
2024-05-02 05:02:46,312 - INFO - joeynmt.training - Example #3
2024-05-02 05:02:46,313 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 05:02:46,313 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 05:02:46,313 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', 'Sommer', '.', '</s>']
2024-05-02 05:02:46,313 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 05:02:46,314 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 05:02:46,314 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in Sommer.
2024-05-02 05:02:46,314 - INFO - joeynmt.training - Example #4
2024-05-02 05:02:46,314 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 05:02:46,314 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 05:02:46,314 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'dass', 'das', 'das', '<unk>', '<unk>', 'ist', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 05:02:46,315 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 05:02:46,315 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 05:02:46,315 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen, dass das das <unk> <unk> ist, was in den letzten 25 Jahren passiert ist.
2024-05-02 05:03:40,350 - INFO - joeynmt.training - Epoch   3, Step:     7100, Batch Loss:     1.463104, Batch Acc: 0.538387, Tokens per Sec:     1272, Lr: 0.000300
2024-05-02 05:04:25,566 - INFO - joeynmt.training - Epoch   3, Step:     7200, Batch Loss:     1.383352, Batch Acc: 0.542538, Tokens per Sec:     1500, Lr: 0.000300
2024-05-02 05:05:20,980 - INFO - joeynmt.training - Epoch   3, Step:     7300, Batch Loss:     1.454988, Batch Acc: 0.542423, Tokens per Sec:     1225, Lr: 0.000300
2024-05-02 05:06:17,685 - INFO - joeynmt.training - Epoch   3, Step:     7400, Batch Loss:     1.587776, Batch Acc: 0.540710, Tokens per Sec:     1188, Lr: 0.000300
2024-05-02 05:07:09,869 - INFO - joeynmt.training - Epoch   3, Step:     7500, Batch Loss:     1.510023, Batch Acc: 0.541575, Tokens per Sec:     1297, Lr: 0.000300
2024-05-02 05:07:09,870 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 05:07:09,870 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 05:08:27,100 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.50, ppl:   4.48, acc:   0.53, generation: 77.1495[sec], evaluation: 0.0000[sec]
2024-05-02 05:08:27,103 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 05:08:27,447 - INFO - joeynmt.helpers - delete models/words/5000.ckpt
2024-05-02 05:08:27,449 - INFO - joeynmt.training - Example #0
2024-05-02 05:08:27,449 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 05:08:27,449 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 05:08:27,449 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'ich', 'diese', 'beiden', '<unk>', ',', 'so', 'dass', 'das', 'das', '<unk>', '<unk>', '<unk>', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'hat', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', ',', 'hat', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 05:08:27,450 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 05:08:27,450 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 05:08:27,450 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr ich diese beiden <unk>, so dass das das <unk> <unk> <unk>, die für die meisten drei Millionen Jahre hat die Größe der <unk> <unk> <unk>, hat von 40 Prozent.
2024-05-02 05:08:27,450 - INFO - joeynmt.training - Example #1
2024-05-02 05:08:27,451 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 05:08:27,451 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 05:08:27,451 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', '<unk>', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 05:08:27,451 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 05:08:27,451 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 05:08:27,451 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses <unk>, weil es nicht die <unk> des <unk> <unk>.
2024-05-02 05:08:27,452 - INFO - joeynmt.training - Example #2
2024-05-02 05:08:27,452 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 05:08:27,452 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 05:08:27,452 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', '<unk>', '<unk>', 'ist', ',', 'in', 'einem', 'Sinn', ',', 'der', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 05:08:27,452 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 05:08:27,453 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 05:08:27,453 - INFO - joeynmt.training - 	Hypothesis: Der <unk> <unk> ist, in einem Sinn, der <unk> des <unk> <unk>.
2024-05-02 05:08:27,453 - INFO - joeynmt.training - Example #3
2024-05-02 05:08:27,453 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 05:08:27,453 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 05:08:27,453 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '<unk>', '.', '</s>']
2024-05-02 05:08:27,454 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 05:08:27,454 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 05:08:27,454 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk> <unk>.
2024-05-02 05:08:27,454 - INFO - joeynmt.training - Example #4
2024-05-02 05:08:27,454 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 05:08:27,454 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 05:08:27,454 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'dass', 'das', ',', 'was', 'über', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 05:08:27,455 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 05:08:27,455 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 05:08:27,455 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen, dass das, was über den letzten 25 Jahren passiert ist.
2024-05-02 05:09:15,363 - INFO - joeynmt.training - Epoch   3, Step:     7600, Batch Loss:     1.473042, Batch Acc: 0.541338, Tokens per Sec:     1397, Lr: 0.000300
2024-05-02 05:10:02,597 - INFO - joeynmt.training - Epoch   3, Step:     7700, Batch Loss:     1.420982, Batch Acc: 0.540927, Tokens per Sec:     1437, Lr: 0.000300
2024-05-02 05:10:49,028 - INFO - joeynmt.training - Epoch   3, Step:     7800, Batch Loss:     1.482179, Batch Acc: 0.542673, Tokens per Sec:     1450, Lr: 0.000300
2024-05-02 05:11:33,566 - INFO - joeynmt.training - Epoch   3, Step:     7900, Batch Loss:     1.494578, Batch Acc: 0.540839, Tokens per Sec:     1537, Lr: 0.000300
2024-05-02 05:12:13,126 - INFO - joeynmt.training - Epoch   3, Step:     8000, Batch Loss:     1.396577, Batch Acc: 0.542387, Tokens per Sec:     1757, Lr: 0.000300
2024-05-02 05:12:13,126 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 05:12:13,127 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 05:13:28,299 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.44, acc:   0.53, generation: 75.0903[sec], evaluation: 0.0000[sec]
2024-05-02 05:13:28,301 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 05:13:28,638 - INFO - joeynmt.helpers - delete models/words/5500.ckpt
2024-05-02 05:13:28,640 - INFO - joeynmt.training - Example #0
2024-05-02 05:13:28,640 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 05:13:28,640 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 05:13:28,640 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'ich', 'diese', 'zwei', '<unk>', '<unk>', ',', 'die', '<unk>', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahren', '<unk>', '<unk>', ',', 'die', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', ',', '<unk>', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 05:13:28,641 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 05:13:28,641 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 05:13:28,641 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr ich diese zwei <unk> <unk>, die <unk>, die für die meisten drei Millionen Jahren <unk> <unk>, die die Größe der <unk> <unk> <unk> <unk> <unk>, <unk> von 40 Prozent.
2024-05-02 05:13:28,641 - INFO - joeynmt.training - Example #1
2024-05-02 05:13:28,641 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 05:13:28,641 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 05:13:28,641 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', '<unk>', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 05:13:28,642 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 05:13:28,642 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 05:13:28,642 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> <unk> dieses Problem, weil es nicht die <unk> des <unk> <unk>.
2024-05-02 05:13:28,642 - INFO - joeynmt.training - Example #2
2024-05-02 05:13:28,642 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 05:13:28,642 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 05:13:28,642 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', '<unk>', '<unk>', 'ist', ',', '<unk>', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 05:13:28,643 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 05:13:28,643 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 05:13:28,643 - INFO - joeynmt.training - 	Hypothesis: Der <unk> <unk> ist, <unk> <unk> des <unk> <unk>.
2024-05-02 05:13:28,643 - INFO - joeynmt.training - Example #3
2024-05-02 05:13:28,643 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 05:13:28,643 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 05:13:28,643 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', 'Sommer', '.', '</s>']
2024-05-02 05:13:28,644 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 05:13:28,644 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 05:13:28,644 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in Sommer.
2024-05-02 05:13:28,644 - INFO - joeynmt.training - Example #4
2024-05-02 05:13:28,644 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 05:13:28,644 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 05:13:28,644 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'dass', 'das', '<unk>', '<unk>', ',', 'was', 'über', 'den', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 05:13:28,645 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 05:13:28,645 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 05:13:28,645 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen, dass das <unk> <unk>, was über den 25 Jahren passiert ist.
2024-05-02 05:14:15,940 - INFO - joeynmt.training - Epoch   3, Step:     8100, Batch Loss:     1.483834, Batch Acc: 0.541096, Tokens per Sec:     1410, Lr: 0.000300
2024-05-02 05:14:53,008 - INFO - joeynmt.training - Epoch   3, Step:     8200, Batch Loss:     1.439388, Batch Acc: 0.543696, Tokens per Sec:     1830, Lr: 0.000300
2024-05-02 05:15:28,684 - INFO - joeynmt.training - Epoch   3, Step:     8300, Batch Loss:     1.484323, Batch Acc: 0.546824, Tokens per Sec:     1913, Lr: 0.000300
2024-05-02 05:16:10,358 - INFO - joeynmt.training - Epoch   3, Step:     8400, Batch Loss:     1.334772, Batch Acc: 0.542874, Tokens per Sec:     1633, Lr: 0.000300
2024-05-02 05:16:53,352 - INFO - joeynmt.training - Epoch   3, Step:     8500, Batch Loss:     1.464135, Batch Acc: 0.543987, Tokens per Sec:     1551, Lr: 0.000300
2024-05-02 05:16:53,352 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 05:16:53,352 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 05:17:56,822 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.48, ppl:   4.37, acc:   0.53, generation: 63.3935[sec], evaluation: 0.0000[sec]
2024-05-02 05:17:56,824 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 05:17:57,122 - INFO - joeynmt.helpers - delete models/words/6000.ckpt
2024-05-02 05:17:57,123 - INFO - joeynmt.training - Example #0
2024-05-02 05:17:57,123 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 05:17:57,123 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 05:17:57,123 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'ich', 'diese', 'beiden', '<unk>', 'gezeigt', 'habe', ',', 'so', 'dass', 'der', '<unk>', '<unk>', ',', 'die', 'für', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', '<unk>', ',', 'hat', 'von', '40', 'Prozent', 'der', 'Größe', 'der', '<unk>', '<unk>', '<unk>', ',', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 05:17:57,124 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 05:17:57,124 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 05:17:57,124 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr ich diese beiden <unk> gezeigt habe, so dass der <unk> <unk>, die für die meisten der letzten drei Millionen Jahre <unk> <unk>, hat von 40 Prozent der Größe der <unk> <unk> <unk>, von 40 Prozent.
2024-05-02 05:17:57,124 - INFO - joeynmt.training - Example #1
2024-05-02 05:17:57,124 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 05:17:57,124 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 05:17:57,124 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', 'der', '<unk>', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 05:17:57,125 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 05:17:57,125 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 05:17:57,125 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> der <unk> dieses Problem, weil es nicht die <unk> des <unk> des <unk>.
2024-05-02 05:17:57,125 - INFO - joeynmt.training - Example #2
2024-05-02 05:17:57,125 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 05:17:57,125 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 05:17:57,125 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', ',', 'im', 'Sinne', ',', 'der', '<unk>', 'des', 'globalen', '<unk>', '.', '</s>']
2024-05-02 05:17:57,126 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 05:17:57,126 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 05:17:57,126 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist, im Sinne, der <unk> des globalen <unk>.
2024-05-02 05:17:57,126 - INFO - joeynmt.training - Example #3
2024-05-02 05:17:57,126 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 05:17:57,126 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 05:17:57,126 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'im', 'Sommer', '.', '</s>']
2024-05-02 05:17:57,127 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 05:17:57,127 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 05:17:57,127 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> im Sommer.
2024-05-02 05:17:57,127 - INFO - joeynmt.training - Example #4
2024-05-02 05:17:57,127 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 05:17:57,127 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 05:17:57,127 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'Ihnen', 'zeigen', ',', 'dass', 'ich', 'ein', '<unk>', '<unk>', '<unk>', 'sein', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', '.', '</s>']
2024-05-02 05:17:57,128 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 05:17:57,128 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 05:17:57,128 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die Ihnen zeigen, dass ich ein <unk> <unk> <unk> sein, was in den letzten 25 Jahren passiert.
2024-05-02 05:18:36,997 - INFO - joeynmt.training - Epoch   3, Step:     8600, Batch Loss:     1.365572, Batch Acc: 0.547905, Tokens per Sec:     1671, Lr: 0.000300
2024-05-02 05:19:16,085 - INFO - joeynmt.training - Epoch   3, Step:     8700, Batch Loss:     1.288682, Batch Acc: 0.545051, Tokens per Sec:     1748, Lr: 0.000300
2024-05-02 05:19:59,828 - INFO - joeynmt.training - Epoch   3, Step:     8800, Batch Loss:     1.626605, Batch Acc: 0.544138, Tokens per Sec:     1548, Lr: 0.000300
2024-05-02 05:20:14,817 - INFO - joeynmt.training - Epoch   3: total training loss 4323.26
2024-05-02 05:20:14,818 - INFO - joeynmt.training - EPOCH 4
2024-05-02 05:20:36,934 - INFO - joeynmt.training - Epoch   4, Step:     8900, Batch Loss:     1.364870, Batch Acc: 0.557292, Tokens per Sec:     1822, Lr: 0.000300
2024-05-02 05:21:13,739 - INFO - joeynmt.training - Epoch   4, Step:     9000, Batch Loss:     1.358833, Batch Acc: 0.568440, Tokens per Sec:     1832, Lr: 0.000300
2024-05-02 05:21:13,740 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 05:21:13,740 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 05:22:16,978 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.46, ppl:   4.32, acc:   0.54, generation: 63.1693[sec], evaluation: 0.0000[sec]
2024-05-02 05:22:16,981 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 05:22:17,261 - INFO - joeynmt.helpers - delete models/words/6500.ckpt
2024-05-02 05:22:17,269 - INFO - joeynmt.training - Example #0
2024-05-02 05:22:17,270 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 05:22:17,270 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 05:22:17,270 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', ',', 'ich', 'diese', 'beiden', '<unk>', 'gezeigt', ',', 'dass', 'die', '<unk>', '<unk>', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'hat', 'die', 'Größe', 'der', '<unk>', '<unk>', ',', 'hat', 'von', '40', 'Prozent', 'der', '<unk>', '<unk>', ',', 'hat', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 05:22:17,270 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 05:22:17,270 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 05:22:17,270 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr, ich diese beiden <unk> gezeigt, dass die <unk> <unk>, die für die meisten drei Millionen Jahre hat die Größe der <unk> <unk>, hat von 40 Prozent der <unk> <unk>, hat von 40 Prozent.
2024-05-02 05:22:17,271 - INFO - joeynmt.training - Example #1
2024-05-02 05:22:17,271 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 05:22:17,271 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 05:22:17,271 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 05:22:17,271 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 05:22:17,271 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 05:22:17,271 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses Problem, weil es nicht die <unk> des <unk>.
2024-05-02 05:22:17,272 - INFO - joeynmt.training - Example #2
2024-05-02 05:22:17,272 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 05:22:17,272 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 05:22:17,272 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', ',', 'in', 'einem', 'Sinne', ',', 'das', '<unk>', 'Herz', 'des', 'globalen', '<unk>', '.', '</s>']
2024-05-02 05:22:17,272 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 05:22:17,272 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 05:22:17,272 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist, in einem Sinne, das <unk> Herz des globalen <unk>.
2024-05-02 05:22:17,272 - INFO - joeynmt.training - Example #3
2024-05-02 05:22:17,273 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 05:22:17,273 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 05:22:17,273 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'im', 'Sommer', '.', '</s>']
2024-05-02 05:22:17,273 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 05:22:17,273 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 05:22:17,273 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> im Sommer.
2024-05-02 05:22:17,273 - INFO - joeynmt.training - Example #4
2024-05-02 05:22:17,274 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 05:22:17,274 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 05:22:17,274 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'dass', 'das', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 05:22:17,274 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 05:22:17,274 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 05:22:17,274 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen, dass das, was in den letzten 25 Jahren passiert ist.
2024-05-02 05:22:55,657 - INFO - joeynmt.training - Epoch   4, Step:     9100, Batch Loss:     1.439128, Batch Acc: 0.563522, Tokens per Sec:     1769, Lr: 0.000300
2024-05-02 05:23:33,344 - INFO - joeynmt.training - Epoch   4, Step:     9200, Batch Loss:     1.261545, Batch Acc: 0.569419, Tokens per Sec:     1812, Lr: 0.000300
2024-05-02 05:24:11,413 - INFO - joeynmt.training - Epoch   4, Step:     9300, Batch Loss:     1.364615, Batch Acc: 0.560007, Tokens per Sec:     1777, Lr: 0.000300
2024-05-02 05:24:49,093 - INFO - joeynmt.training - Epoch   4, Step:     9400, Batch Loss:     1.357958, Batch Acc: 0.564706, Tokens per Sec:     1816, Lr: 0.000300
2024-05-02 05:25:27,165 - INFO - joeynmt.training - Epoch   4, Step:     9500, Batch Loss:     1.429159, Batch Acc: 0.562713, Tokens per Sec:     1833, Lr: 0.000300
2024-05-02 05:25:27,166 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 05:25:27,166 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 05:26:27,170 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.46, ppl:   4.32, acc:   0.54, generation: 59.9403[sec], evaluation: 0.0000[sec]
2024-05-02 05:26:27,173 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 05:26:27,434 - INFO - joeynmt.helpers - delete models/words/7000.ckpt
2024-05-02 05:26:27,436 - INFO - joeynmt.training - Example #0
2024-05-02 05:26:27,437 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 05:26:27,437 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 05:26:27,437 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', ',', 'dass', 'die', '<unk>', '<unk>', '<unk>', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', ',', 'hat', 'von', '40', 'Prozent', 'der', '<unk>', '<unk>', '<unk>', ',', 'hat', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 05:26:27,437 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 05:26:27,437 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 05:26:27,437 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk>, dass die <unk> <unk> <unk>, die für die meisten drei Millionen Jahre die Größe der <unk> <unk> <unk>, hat von 40 Prozent der <unk> <unk> <unk>, hat von 40 Prozent.
2024-05-02 05:26:27,437 - INFO - joeynmt.training - Example #1
2024-05-02 05:26:27,438 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 05:26:27,438 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 05:26:27,438 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'der', '<unk>', 'dieses', '<unk>', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 05:26:27,438 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 05:26:27,438 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 05:26:27,438 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> der <unk> dieses <unk> Problem, weil es nicht die <unk> des <unk> des <unk>.
2024-05-02 05:26:27,438 - INFO - joeynmt.training - Example #2
2024-05-02 05:26:27,438 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 05:26:27,439 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 05:26:27,439 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', '<unk>', 'ist', ',', 'in', 'einem', 'Sinn', ',', 'das', '<unk>', 'Herz', 'des', 'globalen', '<unk>', '.', '</s>']
2024-05-02 05:26:27,439 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 05:26:27,439 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 05:26:27,439 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis <unk> ist, in einem Sinn, das <unk> Herz des globalen <unk>.
2024-05-02 05:26:27,439 - INFO - joeynmt.training - Example #3
2024-05-02 05:26:27,439 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 05:26:27,439 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 05:26:27,439 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '.', '</s>']
2024-05-02 05:26:27,440 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 05:26:27,440 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 05:26:27,440 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>.
2024-05-02 05:26:27,440 - INFO - joeynmt.training - Example #4
2024-05-02 05:26:27,440 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 05:26:27,440 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 05:26:27,440 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'nächste', 'Folie', ',', 'dass', 'Sie', 'ein', '<unk>', '<unk>', '<unk>', 'sein', 'wird', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 05:26:27,441 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 05:26:27,441 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 05:26:27,441 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie, dass Sie ein <unk> <unk> <unk> sein wird, was über die letzten 25 Jahren passiert ist.
2024-05-02 05:27:05,594 - INFO - joeynmt.training - Epoch   4, Step:     9600, Batch Loss:     1.342453, Batch Acc: 0.564733, Tokens per Sec:     1781, Lr: 0.000300
2024-05-02 05:27:44,007 - INFO - joeynmt.training - Epoch   4, Step:     9700, Batch Loss:     1.380342, Batch Acc: 0.562339, Tokens per Sec:     1786, Lr: 0.000300
2024-05-02 05:28:22,512 - INFO - joeynmt.training - Epoch   4, Step:     9800, Batch Loss:     1.339729, Batch Acc: 0.564334, Tokens per Sec:     1789, Lr: 0.000300
2024-05-02 05:29:02,217 - INFO - joeynmt.training - Epoch   4, Step:     9900, Batch Loss:     1.451085, Batch Acc: 0.559194, Tokens per Sec:     1709, Lr: 0.000300
2024-05-02 05:29:38,356 - INFO - joeynmt.training - Epoch   4, Step:    10000, Batch Loss:     1.479193, Batch Acc: 0.563708, Tokens per Sec:     1856, Lr: 0.000300
2024-05-02 05:29:38,356 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 05:29:38,356 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 05:30:39,211 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.46, ppl:   4.29, acc:   0.54, generation: 60.7824[sec], evaluation: 0.0000[sec]
2024-05-02 05:30:39,213 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 05:30:39,497 - INFO - joeynmt.helpers - delete models/words/7500.ckpt
2024-05-02 05:30:39,503 - INFO - joeynmt.training - Example #0
2024-05-02 05:30:39,503 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 05:30:39,503 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 05:30:39,503 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', ',', 'das', 'ich', 'diese', 'zwei', '<unk>', '<unk>', ',', 'dass', 'das', '<unk>', 'Eis', '<unk>', ',', 'das', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', '<unk>', '<unk>', ',', 'hat', '<unk>', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 05:30:39,504 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 05:30:39,504 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 05:30:39,504 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr, das ich diese zwei <unk> <unk>, dass das <unk> Eis <unk>, das für die meisten drei Millionen Jahre <unk> <unk>, hat <unk> von 40 Prozent.
2024-05-02 05:30:39,504 - INFO - joeynmt.training - Example #1
2024-05-02 05:30:39,504 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 05:30:39,504 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 05:30:39,504 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'dieses', '<unk>', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', 'zeigen', '.', '</s>']
2024-05-02 05:30:39,505 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 05:30:39,505 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 05:30:39,505 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> dieses <unk> Problem, weil es nicht die <unk> des <unk> zeigen.
2024-05-02 05:30:39,505 - INFO - joeynmt.training - Example #2
2024-05-02 05:30:39,505 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 05:30:39,505 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 05:30:39,505 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', ',', 'dass', 'das', '<unk>', 'Herz', 'des', 'globalen', '<unk>', 'ist', '.', '</s>']
2024-05-02 05:30:39,506 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 05:30:39,506 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 05:30:39,506 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist, dass das <unk> Herz des globalen <unk> ist.
2024-05-02 05:30:39,506 - INFO - joeynmt.training - Example #3
2024-05-02 05:30:39,506 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 05:30:39,506 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 05:30:39,506 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'im', 'Sommer', '.', '</s>']
2024-05-02 05:30:39,507 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 05:30:39,507 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 05:30:39,507 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> im Sommer.
2024-05-02 05:30:39,507 - INFO - joeynmt.training - Example #4
2024-05-02 05:30:39,507 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 05:30:39,507 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 05:30:39,507 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'werden', 'ein', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 05:30:39,507 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 05:30:39,508 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 05:30:39,508 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie, die ich Ihnen zeigen, werden ein <unk> <unk> <unk> <unk>, was in den letzten 25 Jahren passiert ist.
2024-05-02 05:31:20,294 - INFO - joeynmt.training - Epoch   4, Step:    10100, Batch Loss:     1.426107, Batch Acc: 0.563578, Tokens per Sec:     1666, Lr: 0.000300
2024-05-02 05:32:00,269 - INFO - joeynmt.training - Epoch   4, Step:    10200, Batch Loss:     1.358535, Batch Acc: 0.557866, Tokens per Sec:     1664, Lr: 0.000300
2024-05-02 05:32:39,160 - INFO - joeynmt.training - Epoch   4, Step:    10300, Batch Loss:     1.349979, Batch Acc: 0.558311, Tokens per Sec:     1730, Lr: 0.000300
2024-05-02 05:33:19,529 - INFO - joeynmt.training - Epoch   4, Step:    10400, Batch Loss:     1.451057, Batch Acc: 0.563147, Tokens per Sec:     1675, Lr: 0.000300
2024-05-02 05:34:02,917 - INFO - joeynmt.training - Epoch   4, Step:    10500, Batch Loss:     1.402519, Batch Acc: 0.560467, Tokens per Sec:     1525, Lr: 0.000300
2024-05-02 05:34:02,918 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 05:34:02,918 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 05:35:09,096 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.44, ppl:   4.23, acc:   0.54, generation: 66.1123[sec], evaluation: 0.0000[sec]
2024-05-02 05:35:09,099 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 05:35:09,370 - INFO - joeynmt.helpers - delete models/words/8000.ckpt
2024-05-02 05:35:09,372 - INFO - joeynmt.training - Example #0
2024-05-02 05:35:09,372 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 05:35:09,373 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 05:35:09,373 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'ich', 'diese', 'zwei', '<unk>', '<unk>', ',', 'die', 'die', '<unk>', '<unk>', ',', 'die', 'die', '<unk>', '<unk>', ',', 'die', 'die', '<unk>', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'hat', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'hat', 'von', '40', 'Prozent', '<unk>', '<unk>', '.', '</s>']
2024-05-02 05:35:09,373 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 05:35:09,373 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 05:35:09,373 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr ich diese zwei <unk> <unk>, die die <unk> <unk>, die die <unk> <unk>, die die <unk> der letzten drei Millionen Jahre hat die Größe der <unk> <unk> <unk> <unk>, hat von 40 Prozent <unk> <unk>.
2024-05-02 05:35:09,373 - INFO - joeynmt.training - Example #1
2024-05-02 05:35:09,373 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 05:35:09,373 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 05:35:09,374 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', 'nicht', 'zeigen', '.', '</s>']
2024-05-02 05:35:09,374 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 05:35:09,374 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 05:35:09,374 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses Problem, weil es nicht die <unk> des <unk> nicht zeigen.
2024-05-02 05:35:09,374 - INFO - joeynmt.training - Example #2
2024-05-02 05:35:09,374 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 05:35:09,374 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 05:35:09,374 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', ',', 'in', 'einem', 'Sinn', ',', 'der', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 05:35:09,375 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 05:35:09,375 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 05:35:09,375 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist, in einem Sinn, der <unk> des <unk> <unk>.
2024-05-02 05:35:09,375 - INFO - joeynmt.training - Example #3
2024-05-02 05:35:09,375 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 05:35:09,375 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 05:35:09,375 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '.', '</s>']
2024-05-02 05:35:09,376 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 05:35:09,376 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 05:35:09,376 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>.
2024-05-02 05:35:09,376 - INFO - joeynmt.training - Example #4
2024-05-02 05:35:09,376 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 05:35:09,376 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 05:35:09,376 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'dass', 'Sie', 'ein', '<unk>', '<unk>', '<unk>', 'sein', 'werden', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 05:35:09,376 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 05:35:09,377 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 05:35:09,377 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, dass Sie ein <unk> <unk> <unk> sein werden, was in den letzten 25 Jahren passiert ist.
2024-05-02 05:35:50,807 - INFO - joeynmt.training - Epoch   4, Step:    10600, Batch Loss:     1.369425, Batch Acc: 0.559629, Tokens per Sec:     1612, Lr: 0.000300
2024-05-02 05:36:31,695 - INFO - joeynmt.training - Epoch   4, Step:    10700, Batch Loss:     1.408178, Batch Acc: 0.559674, Tokens per Sec:     1624, Lr: 0.000300
2024-05-02 05:37:15,937 - INFO - joeynmt.training - Epoch   4, Step:    10800, Batch Loss:     1.449255, Batch Acc: 0.561669, Tokens per Sec:     1556, Lr: 0.000300
2024-05-02 05:37:59,966 - INFO - joeynmt.training - Epoch   4, Step:    10900, Batch Loss:     1.331126, Batch Acc: 0.562034, Tokens per Sec:     1554, Lr: 0.000300
2024-05-02 05:38:42,214 - INFO - joeynmt.training - Epoch   4, Step:    11000, Batch Loss:     1.337987, Batch Acc: 0.562407, Tokens per Sec:     1642, Lr: 0.000300
2024-05-02 05:38:42,214 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 05:38:42,215 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 05:39:51,525 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.44, ppl:   4.23, acc:   0.54, generation: 69.2245[sec], evaluation: 0.0000[sec]
2024-05-02 05:39:51,869 - INFO - joeynmt.helpers - delete models/words/8500.ckpt
2024-05-02 05:39:51,871 - INFO - joeynmt.training - Example #0
2024-05-02 05:39:51,872 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 05:39:51,872 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 05:39:51,872 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', ',', 'das', 'ich', 'zwei', '<unk>', '<unk>', ',', 'dass', 'das', '<unk>', 'Eis', '<unk>', '<unk>', ',', 'das', 'für', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', 'wurde', '.', '</s>']
2024-05-02 05:39:51,872 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 05:39:51,873 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 05:39:51,873 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr, das ich zwei <unk> <unk>, dass das <unk> Eis <unk> <unk>, das für die meisten der letzten drei Millionen Jahre <unk> wurde.
2024-05-02 05:39:51,873 - INFO - joeynmt.training - Example #1
2024-05-02 05:39:51,873 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 05:39:51,873 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 05:39:51,873 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'des', '<unk>', '<unk>', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 05:39:51,874 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 05:39:51,874 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 05:39:51,874 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> des <unk> <unk>, weil es nicht die <unk> des <unk> <unk>.
2024-05-02 05:39:51,874 - INFO - joeynmt.training - Example #2
2024-05-02 05:39:51,874 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 05:39:51,874 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 05:39:51,874 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', ',', 'im', 'Sinne', 'der', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 05:39:51,875 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 05:39:51,875 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 05:39:51,875 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist, im Sinne der <unk> des <unk>.
2024-05-02 05:39:51,875 - INFO - joeynmt.training - Example #3
2024-05-02 05:39:51,875 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 05:39:51,875 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 05:39:51,875 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'im', 'Sommer', '.', '</s>']
2024-05-02 05:39:51,876 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 05:39:51,876 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 05:39:51,876 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> im Sommer.
2024-05-02 05:39:51,876 - INFO - joeynmt.training - Example #4
2024-05-02 05:39:51,876 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 05:39:51,876 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 05:39:51,876 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', '<unk>', ',', 'werden', 'Sie', 'eine', '<unk>', '<unk>', '<unk>', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 05:39:51,877 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 05:39:51,877 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 05:39:51,877 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen <unk>, werden Sie eine <unk> <unk> <unk>, was in den letzten 25 Jahren passiert ist.
2024-05-02 05:40:36,205 - INFO - joeynmt.training - Epoch   4, Step:    11100, Batch Loss:     1.305522, Batch Acc: 0.557080, Tokens per Sec:     1514, Lr: 0.000300
2024-05-02 05:41:21,495 - INFO - joeynmt.training - Epoch   4, Step:    11200, Batch Loss:     1.293682, Batch Acc: 0.560527, Tokens per Sec:     1509, Lr: 0.000300
2024-05-02 05:42:09,440 - INFO - joeynmt.training - Epoch   4, Step:    11300, Batch Loss:     1.341269, Batch Acc: 0.559445, Tokens per Sec:     1416, Lr: 0.000300
2024-05-02 05:42:59,454 - INFO - joeynmt.training - Epoch   4, Step:    11400, Batch Loss:     1.486137, Batch Acc: 0.565032, Tokens per Sec:     1379, Lr: 0.000300
2024-05-02 05:43:46,607 - INFO - joeynmt.training - Epoch   4, Step:    11500, Batch Loss:     1.418890, Batch Acc: 0.559384, Tokens per Sec:     1444, Lr: 0.000300
2024-05-02 05:43:46,607 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 05:43:46,608 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 05:44:51,752 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.43, ppl:   4.20, acc:   0.54, generation: 65.0681[sec], evaluation: 0.0000[sec]
2024-05-02 05:44:51,755 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 05:44:52,060 - INFO - joeynmt.helpers - delete models/words/9000.ckpt
2024-05-02 05:44:52,061 - INFO - joeynmt.training - Example #0
2024-05-02 05:44:52,061 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 05:44:52,061 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 05:44:52,062 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'beiden', '<unk>', '<unk>', ',', 'die', 'das', '<unk>', '<unk>', ',', 'das', 'für', 'die', 'meisten', '<unk>', '<unk>', ',', 'die', 'für', 'die', '<unk>', '<unk>', '<unk>', ',', 'hat', 'von', '40', 'Prozent', '<unk>', '<unk>', '.', '</s>']
2024-05-02 05:44:52,062 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 05:44:52,062 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 05:44:52,062 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese beiden <unk> <unk>, die das <unk> <unk>, das für die meisten <unk> <unk>, die für die <unk> <unk> <unk>, hat von 40 Prozent <unk> <unk>.
2024-05-02 05:44:52,063 - INFO - joeynmt.training - Example #1
2024-05-02 05:44:52,063 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 05:44:52,063 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 05:44:52,063 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', '<unk>', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 05:44:52,063 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 05:44:52,063 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 05:44:52,063 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> <unk> dieses Problem, weil es nicht die <unk> des <unk> <unk>.
2024-05-02 05:44:52,064 - INFO - joeynmt.training - Example #2
2024-05-02 05:44:52,064 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 05:44:52,064 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 05:44:52,064 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', ',', 'in', 'einem', 'Sinn', ',', 'das', '<unk>', 'Herz', 'des', '<unk>', '.', '</s>']
2024-05-02 05:44:52,064 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 05:44:52,064 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 05:44:52,064 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist, in einem Sinn, das <unk> Herz des <unk>.
2024-05-02 05:44:52,065 - INFO - joeynmt.training - Example #3
2024-05-02 05:44:52,065 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 05:44:52,065 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 05:44:52,065 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'im', 'Sommer', '.', '</s>']
2024-05-02 05:44:52,065 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 05:44:52,065 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 05:44:52,065 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> im Sommer.
2024-05-02 05:44:52,065 - INFO - joeynmt.training - Example #4
2024-05-02 05:44:52,066 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 05:44:52,066 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 05:44:52,066 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', 'zeigen', ',', 'dass', 'Sie', 'eine', '<unk>', '<unk>', '<unk>', 'sein', 'wird', ',', 'was', 'über', 'den', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 05:44:52,066 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 05:44:52,066 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 05:44:52,066 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeigen, dass Sie eine <unk> <unk> <unk> sein wird, was über den 25 Jahren passiert ist.
2024-05-02 05:45:37,179 - INFO - joeynmt.training - Epoch   4, Step:    11600, Batch Loss:     1.333432, Batch Acc: 0.561495, Tokens per Sec:     1473, Lr: 0.000300
2024-05-02 05:46:19,269 - INFO - joeynmt.training - Epoch   4, Step:    11700, Batch Loss:     1.388427, Batch Acc: 0.558411, Tokens per Sec:     1589, Lr: 0.000300
2024-05-02 05:46:59,503 - INFO - joeynmt.training - Epoch   4: total training loss 4095.59
2024-05-02 05:46:59,503 - INFO - joeynmt.training - EPOCH 5
2024-05-02 05:47:03,786 - INFO - joeynmt.training - Epoch   5, Step:    11800, Batch Loss:     1.199626, Batch Acc: 0.591713, Tokens per Sec:     1466, Lr: 0.000300
2024-05-02 05:47:41,328 - INFO - joeynmt.training - Epoch   5, Step:    11900, Batch Loss:     1.284734, Batch Acc: 0.586236, Tokens per Sec:     1791, Lr: 0.000300
2024-05-02 05:48:20,437 - INFO - joeynmt.training - Epoch   5, Step:    12000, Batch Loss:     1.352299, Batch Acc: 0.581562, Tokens per Sec:     1738, Lr: 0.000300
2024-05-02 05:48:20,438 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 05:48:20,438 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 05:49:19,483 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.15, acc:   0.55, generation: 58.9847[sec], evaluation: 0.0000[sec]
2024-05-02 05:49:19,485 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 05:49:19,735 - INFO - joeynmt.helpers - delete models/words/9500.ckpt
2024-05-02 05:49:19,736 - INFO - joeynmt.training - Example #0
2024-05-02 05:49:19,736 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 05:49:19,736 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 05:49:19,736 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', ',', 'die', '<unk>', '<unk>', '<unk>', ',', 'die', 'für', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', 'war', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', ',', 'hat', 'von', '<unk>', '<unk>', '<unk>', ',', 'hat', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 05:49:19,736 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 05:49:19,736 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 05:49:19,736 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk>, die <unk> <unk> <unk>, die für die letzten drei Millionen Jahre war die Größe der <unk> <unk> <unk>, hat von <unk> <unk> <unk>, hat von 40 Prozent.
2024-05-02 05:49:19,737 - INFO - joeynmt.training - Example #1
2024-05-02 05:49:19,737 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 05:49:19,737 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 05:49:19,737 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'des', '<unk>', '<unk>', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 05:49:19,737 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 05:49:19,737 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 05:49:19,737 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> des <unk> <unk>, weil es nicht die <unk> des <unk> des <unk>.
2024-05-02 05:49:19,737 - INFO - joeynmt.training - Example #2
2024-05-02 05:49:19,738 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 05:49:19,738 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 05:49:19,738 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', ',', 'in', 'einem', 'Sinn', ',', 'der', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 05:49:19,738 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 05:49:19,738 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 05:49:19,738 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist, in einem Sinn, der <unk> des <unk> <unk>.
2024-05-02 05:49:19,738 - INFO - joeynmt.training - Example #3
2024-05-02 05:49:19,738 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 05:49:19,738 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 05:49:19,738 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '.', '</s>']
2024-05-02 05:49:19,739 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 05:49:19,739 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 05:49:19,739 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk>.
2024-05-02 05:49:19,739 - INFO - joeynmt.training - Example #4
2024-05-02 05:49:19,739 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 05:49:19,739 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 05:49:19,739 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'dass', 'das', '<unk>', '<unk>', '<unk>', 'sein', 'wird', '.', '</s>']
2024-05-02 05:49:19,740 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 05:49:19,740 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 05:49:19,740 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen, dass das <unk> <unk> <unk> sein wird.
2024-05-02 05:50:03,761 - INFO - joeynmt.training - Epoch   5, Step:    12100, Batch Loss:     1.335922, Batch Acc: 0.582595, Tokens per Sec:     1530, Lr: 0.000300
2024-05-02 05:50:43,764 - INFO - joeynmt.training - Epoch   5, Step:    12200, Batch Loss:     1.150503, Batch Acc: 0.583473, Tokens per Sec:     1690, Lr: 0.000300
2024-05-02 05:51:21,072 - INFO - joeynmt.training - Epoch   5, Step:    12300, Batch Loss:     1.250480, Batch Acc: 0.577645, Tokens per Sec:     1814, Lr: 0.000300
2024-05-02 05:52:06,799 - INFO - joeynmt.training - Epoch   5, Step:    12400, Batch Loss:     1.523971, Batch Acc: 0.578313, Tokens per Sec:     1499, Lr: 0.000300
2024-05-02 05:52:50,185 - INFO - joeynmt.training - Epoch   5, Step:    12500, Batch Loss:     1.544552, Batch Acc: 0.575062, Tokens per Sec:     1591, Lr: 0.000300
2024-05-02 05:52:50,185 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 05:52:50,186 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 05:53:56,127 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.43, ppl:   4.16, acc:   0.55, generation: 65.8710[sec], evaluation: 0.0000[sec]
2024-05-02 05:53:56,425 - INFO - joeynmt.helpers - delete models/words/10000.ckpt
2024-05-02 05:53:56,426 - INFO - joeynmt.training - Example #0
2024-05-02 05:53:56,426 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 05:53:56,426 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 05:53:56,427 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', ',', 'die', '<unk>', '<unk>', ',', 'die', 'die', '<unk>', '<unk>', ',', 'die', 'für', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', 'war', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-02 05:53:56,427 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 05:53:56,427 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 05:53:56,427 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk>, die <unk> <unk>, die die <unk> <unk>, die für die letzten drei Millionen Jahre war die Größe der <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>.
2024-05-02 05:53:56,427 - INFO - joeynmt.training - Example #1
2024-05-02 05:53:56,427 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 05:53:56,427 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 05:53:56,428 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', '<unk>', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', 'nicht', 'zeigen', '.', '</s>']
2024-05-02 05:53:56,428 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 05:53:56,428 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 05:53:56,428 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> <unk> Problem, weil es nicht die <unk> des <unk> nicht zeigen.
2024-05-02 05:53:56,428 - INFO - joeynmt.training - Example #2
2024-05-02 05:53:56,429 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 05:53:56,429 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 05:53:56,429 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', '<unk>', 'ist', ',', '<unk>', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 05:53:56,429 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 05:53:56,429 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 05:53:56,429 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis <unk> ist, <unk> <unk> des <unk> <unk>.
2024-05-02 05:53:56,429 - INFO - joeynmt.training - Example #3
2024-05-02 05:53:56,429 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 05:53:56,430 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 05:53:56,430 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Sie', '<unk>', 'in', '<unk>', 'und', '<unk>', 'im', 'Sommer', '.', '</s>']
2024-05-02 05:53:56,430 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 05:53:56,430 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 05:53:56,430 - INFO - joeynmt.training - 	Hypothesis: Sie <unk> in <unk> und <unk> im Sommer.
2024-05-02 05:53:56,430 - INFO - joeynmt.training - Example #4
2024-05-02 05:53:56,430 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 05:53:56,430 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 05:53:56,430 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'Ihnen', 'zeige', ',', 'wird', 'eine', '<unk>', '<unk>', '<unk>', 'sein', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 05:53:56,431 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 05:53:56,431 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 05:53:56,431 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, Ihnen zeige, wird eine <unk> <unk> <unk> sein, was in den letzten 25 Jahren passiert ist.
2024-05-02 05:54:31,794 - INFO - joeynmt.training - Epoch   5, Step:    12600, Batch Loss:     1.280081, Batch Acc: 0.575208, Tokens per Sec:     1900, Lr: 0.000300
2024-05-02 05:55:07,454 - INFO - joeynmt.training - Epoch   5, Step:    12700, Batch Loss:     1.328060, Batch Acc: 0.574001, Tokens per Sec:     1894, Lr: 0.000300
2024-05-02 05:55:41,879 - INFO - joeynmt.training - Epoch   5, Step:    12800, Batch Loss:     1.305418, Batch Acc: 0.576056, Tokens per Sec:     1963, Lr: 0.000300
2024-05-02 05:56:19,595 - INFO - joeynmt.training - Epoch   5, Step:    12900, Batch Loss:     1.224670, Batch Acc: 0.576989, Tokens per Sec:     1808, Lr: 0.000300
2024-05-02 05:56:53,479 - INFO - joeynmt.training - Epoch   5, Step:    13000, Batch Loss:     1.272695, Batch Acc: 0.574940, Tokens per Sec:     1969, Lr: 0.000300
2024-05-02 05:56:53,481 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 05:56:53,481 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 05:57:46,611 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.13, acc:   0.55, generation: 53.0670[sec], evaluation: 0.0000[sec]
2024-05-02 05:57:46,616 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 05:57:46,903 - INFO - joeynmt.helpers - delete models/words/11000.ckpt
2024-05-02 05:57:46,905 - INFO - joeynmt.training - Example #0
2024-05-02 05:57:46,905 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 05:57:46,906 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 05:57:46,906 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', ',', 'ich', 'diese', 'zwei', '<unk>', '<unk>', ',', 'dass', 'das', '<unk>', 'Eis', '<unk>', ',', 'das', 'für', 'die', 'meisten', 'der', '<unk>', '<unk>', '<unk>', ',', 'die', '<unk>', '<unk>', '<unk>', ',', 'hat', 'von', '40', 'Prozent', '<unk>', '<unk>', ',', '<unk>', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 05:57:46,906 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 05:57:46,906 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 05:57:46,906 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr, ich diese zwei <unk> <unk>, dass das <unk> Eis <unk>, das für die meisten der <unk> <unk> <unk>, die <unk> <unk> <unk>, hat von 40 Prozent <unk> <unk>, <unk> von 40 Prozent.
2024-05-02 05:57:46,906 - INFO - joeynmt.training - Example #1
2024-05-02 05:57:46,907 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 05:57:46,907 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 05:57:46,907 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', '<unk>', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', 'nicht', 'die', '<unk>', 'des', '<unk>', 'zeigen', '.', '</s>']
2024-05-02 05:57:46,907 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 05:57:46,907 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 05:57:46,907 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> <unk> Problem, weil es nicht die <unk> des <unk> nicht die <unk> des <unk> zeigen.
2024-05-02 05:57:46,907 - INFO - joeynmt.training - Example #2
2024-05-02 05:57:46,907 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 05:57:46,907 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 05:57:46,908 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', ',', 'in', 'einem', '<unk>', ',', 'der', '<unk>', 'Herz', 'des', 'globalen', '<unk>', '.', '</s>']
2024-05-02 05:57:46,908 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 05:57:46,908 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 05:57:46,908 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist, in einem <unk>, der <unk> Herz des globalen <unk>.
2024-05-02 05:57:46,908 - INFO - joeynmt.training - Example #3
2024-05-02 05:57:46,908 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 05:57:46,908 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 05:57:46,908 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'im', 'Sommer', '.', '</s>']
2024-05-02 05:57:46,909 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 05:57:46,909 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 05:57:46,909 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> im Sommer.
2024-05-02 05:57:46,909 - INFO - joeynmt.training - Example #4
2024-05-02 05:57:46,909 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 05:57:46,909 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 05:57:46,909 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'dass', 'Sie', 'eine', '<unk>', '<unk>', 'von', 'den', 'letzten', '25', 'Jahren', '.', '</s>']
2024-05-02 05:57:46,910 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 05:57:46,910 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 05:57:46,910 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, dass Sie eine <unk> <unk> von den letzten 25 Jahren.
2024-05-02 05:58:24,837 - INFO - joeynmt.training - Epoch   5, Step:    13100, Batch Loss:     1.279774, Batch Acc: 0.575949, Tokens per Sec:     1792, Lr: 0.000300
2024-05-02 05:58:58,875 - INFO - joeynmt.training - Epoch   5, Step:    13200, Batch Loss:     1.339190, Batch Acc: 0.577305, Tokens per Sec:     1974, Lr: 0.000300
2024-05-02 05:59:35,260 - INFO - joeynmt.training - Epoch   5, Step:    13300, Batch Loss:     1.235794, Batch Acc: 0.579196, Tokens per Sec:     1902, Lr: 0.000300
2024-05-02 06:00:09,957 - INFO - joeynmt.training - Epoch   5, Step:    13400, Batch Loss:     1.312089, Batch Acc: 0.573291, Tokens per Sec:     1999, Lr: 0.000300
2024-05-02 06:00:45,160 - INFO - joeynmt.training - Epoch   5, Step:    13500, Batch Loss:     1.258487, Batch Acc: 0.574446, Tokens per Sec:     1841, Lr: 0.000300
2024-05-02 06:00:45,160 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 06:00:45,160 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 06:01:40,396 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.41, ppl:   4.08, acc:   0.55, generation: 55.1755[sec], evaluation: 0.0000[sec]
2024-05-02 06:01:40,399 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 06:01:40,661 - INFO - joeynmt.helpers - delete models/words/10500.ckpt
2024-05-02 06:01:40,662 - INFO - joeynmt.training - Example #0
2024-05-02 06:01:40,663 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 06:01:40,663 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 06:01:40,663 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', ',', 'das', 'ich', 'diese', 'beiden', '<unk>', '<unk>', ',', 'dass', 'das', '<unk>', '<unk>', ',', 'das', '<unk>', '<unk>', ',', 'das', 'für', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', '<unk>', 'hat', ',', 'hat', 'von', '40', 'Prozent', '<unk>', '<unk>', '<unk>', ',', '<unk>', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 06:01:40,663 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 06:01:40,663 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 06:01:40,663 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr, das ich diese beiden <unk> <unk>, dass das <unk> <unk>, das <unk> <unk>, das für die letzten drei Millionen Jahre <unk> <unk> hat, hat von 40 Prozent <unk> <unk> <unk>, <unk> von 40 Prozent.
2024-05-02 06:01:40,663 - INFO - joeynmt.training - Example #1
2024-05-02 06:01:40,664 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 06:01:40,664 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 06:01:40,664 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'das', '<unk>', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', 'nicht', '<unk>', '.', '</s>']
2024-05-02 06:01:40,664 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 06:01:40,664 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 06:01:40,664 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> das <unk> dieses Problem, weil es nicht die <unk> des <unk> nicht <unk>.
2024-05-02 06:01:40,665 - INFO - joeynmt.training - Example #2
2024-05-02 06:01:40,665 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 06:01:40,665 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 06:01:40,665 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', ',', 'in', 'einem', 'Sinn', ',', 'der', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 06:01:40,665 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 06:01:40,665 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 06:01:40,665 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist, in einem Sinn, der <unk> des <unk> <unk>.
2024-05-02 06:01:40,665 - INFO - joeynmt.training - Example #3
2024-05-02 06:01:40,666 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 06:01:40,666 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 06:01:40,666 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'im', 'Sommer', '.', '</s>']
2024-05-02 06:01:40,666 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 06:01:40,666 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 06:01:40,666 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> im Sommer.
2024-05-02 06:01:40,666 - INFO - joeynmt.training - Example #4
2024-05-02 06:01:40,666 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 06:01:40,667 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 06:01:40,667 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'dass', 'ich', 'ein', '<unk>', '<unk>', 'sein', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 06:01:40,667 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 06:01:40,667 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 06:01:40,667 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen, dass ich ein <unk> <unk> sein, was in den letzten 25 Jahren passiert ist.
2024-05-02 06:02:24,664 - INFO - joeynmt.training - Epoch   5, Step:    13600, Batch Loss:     1.236176, Batch Acc: 0.569758, Tokens per Sec:     1540, Lr: 0.000300
2024-05-02 06:03:10,383 - INFO - joeynmt.training - Epoch   5, Step:    13700, Batch Loss:     1.388260, Batch Acc: 0.580819, Tokens per Sec:     1474, Lr: 0.000300
2024-05-02 06:03:52,907 - INFO - joeynmt.training - Epoch   5, Step:    13800, Batch Loss:     1.292070, Batch Acc: 0.574095, Tokens per Sec:     1559, Lr: 0.000300
2024-05-02 06:04:36,109 - INFO - joeynmt.training - Epoch   5, Step:    13900, Batch Loss:     1.335325, Batch Acc: 0.577326, Tokens per Sec:     1536, Lr: 0.000300
2024-05-02 06:05:16,685 - INFO - joeynmt.training - Epoch   5, Step:    14000, Batch Loss:     1.390293, Batch Acc: 0.571323, Tokens per Sec:     1738, Lr: 0.000300
2024-05-02 06:05:16,686 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 06:05:16,686 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 06:06:12,376 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.41, ppl:   4.09, acc:   0.55, generation: 55.6001[sec], evaluation: 0.0000[sec]
2024-05-02 06:06:12,643 - INFO - joeynmt.helpers - delete models/words/11500.ckpt
2024-05-02 06:06:12,648 - INFO - joeynmt.training - Example #0
2024-05-02 06:06:12,648 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 06:06:12,648 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 06:06:12,648 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', ',', 'ich', 'diese', 'zwei', '<unk>', 'gezeigt', ',', 'dass', 'das', '<unk>', 'Eis', '<unk>', ',', 'das', 'für', 'die', 'meisten', '<unk>', '<unk>', ',', 'die', 'die', '<unk>', 'drei', 'Millionen', 'Jahre', 'lang', 'gezeigt', 'habe', '.', '</s>']
2024-05-02 06:06:12,649 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 06:06:12,649 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 06:06:12,649 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr, ich diese zwei <unk> gezeigt, dass das <unk> Eis <unk>, das für die meisten <unk> <unk>, die die <unk> drei Millionen Jahre lang gezeigt habe.
2024-05-02 06:06:12,649 - INFO - joeynmt.training - Example #1
2024-05-02 06:06:12,649 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 06:06:12,649 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 06:06:12,649 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'das', '<unk>', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 06:06:12,650 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 06:06:12,650 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 06:06:12,650 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> das <unk> dieses Problem, weil es nicht die <unk> des <unk> <unk>.
2024-05-02 06:06:12,650 - INFO - joeynmt.training - Example #2
2024-05-02 06:06:12,650 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 06:06:12,650 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 06:06:12,650 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', ',', 'in', 'einem', 'Sinne', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 06:06:12,651 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 06:06:12,651 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 06:06:12,651 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist, in einem Sinne des <unk> <unk>.
2024-05-02 06:06:12,651 - INFO - joeynmt.training - Example #3
2024-05-02 06:06:12,651 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 06:06:12,651 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 06:06:12,651 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'im', 'Sommer', '.', '</s>']
2024-05-02 06:06:12,652 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 06:06:12,652 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 06:06:12,652 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> im Sommer.
2024-05-02 06:06:12,652 - INFO - joeynmt.training - Example #4
2024-05-02 06:06:12,652 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 06:06:12,652 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 06:06:12,652 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'zeige', ',', 'dass', 'Sie', '<unk>', '<unk>', 'sein', 'wird', ',', 'was', 'über', 'den', 'letzten', '25', 'Jahre', '<unk>', '.', '</s>']
2024-05-02 06:06:12,652 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 06:06:12,652 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 06:06:12,653 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich zeige, dass Sie <unk> <unk> sein wird, was über den letzten 25 Jahre <unk>.
2024-05-02 06:06:50,923 - INFO - joeynmt.training - Epoch   5, Step:    14100, Batch Loss:     1.292425, Batch Acc: 0.571387, Tokens per Sec:     1765, Lr: 0.000300
2024-05-02 06:07:29,501 - INFO - joeynmt.training - Epoch   5, Step:    14200, Batch Loss:     1.366855, Batch Acc: 0.572561, Tokens per Sec:     1733, Lr: 0.000300
2024-05-02 06:08:06,600 - INFO - joeynmt.training - Epoch   5, Step:    14300, Batch Loss:     1.449819, Batch Acc: 0.570688, Tokens per Sec:     1836, Lr: 0.000300
2024-05-02 06:08:39,638 - INFO - joeynmt.training - Epoch   5, Step:    14400, Batch Loss:     1.320760, Batch Acc: 0.572978, Tokens per Sec:     2054, Lr: 0.000300
2024-05-02 06:09:10,460 - INFO - joeynmt.training - Epoch   5, Step:    14500, Batch Loss:     1.293337, Batch Acc: 0.578746, Tokens per Sec:     2220, Lr: 0.000300
2024-05-02 06:09:10,461 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 06:09:10,461 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 06:09:44,921 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.40, ppl:   4.07, acc:   0.55, generation: 34.4067[sec], evaluation: 0.0000[sec]
2024-05-02 06:09:44,923 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 06:09:45,134 - INFO - joeynmt.helpers - delete models/words/12500.ckpt
2024-05-02 06:09:45,135 - INFO - joeynmt.training - Example #0
2024-05-02 06:09:45,135 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 06:09:45,135 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 06:09:45,135 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', ',', 'ich', 'diese', 'zwei', '<unk>', 'so', '<unk>', ',', 'dass', 'die', '<unk>', '<unk>', ',', 'die', 'für', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'hat', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 06:09:45,135 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 06:09:45,135 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 06:09:45,135 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr, ich diese zwei <unk> so <unk>, dass die <unk> <unk>, die für die meisten der letzten drei Millionen Jahre die Größe der <unk> <unk> <unk> <unk>, hat von 40 Prozent.
2024-05-02 06:09:45,135 - INFO - joeynmt.training - Example #1
2024-05-02 06:09:45,136 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 06:09:45,136 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 06:09:45,136 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 06:09:45,136 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 06:09:45,136 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 06:09:45,136 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses Problem, weil es nicht die <unk> des <unk> des <unk>.
2024-05-02 06:09:45,136 - INFO - joeynmt.training - Example #2
2024-05-02 06:09:45,136 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 06:09:45,136 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 06:09:45,136 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', '<unk>', '<unk>', 'ist', ',', 'in', 'einem', 'Sinne', ',', 'der', '<unk>', 'des', 'globalen', '<unk>', '.', '</s>']
2024-05-02 06:09:45,137 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 06:09:45,137 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 06:09:45,137 - INFO - joeynmt.training - 	Hypothesis: Der <unk> <unk> ist, in einem Sinne, der <unk> des globalen <unk>.
2024-05-02 06:09:45,137 - INFO - joeynmt.training - Example #3
2024-05-02 06:09:45,137 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 06:09:45,137 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 06:09:45,137 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '.', '</s>']
2024-05-02 06:09:45,137 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 06:09:45,137 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 06:09:45,138 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>.
2024-05-02 06:09:45,138 - INFO - joeynmt.training - Example #4
2024-05-02 06:09:45,138 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 06:09:45,138 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 06:09:45,138 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'werden', 'Sie', 'eine', '<unk>', '<unk>', 'sein', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahre', 'passiert', 'ist', '.', '</s>']
2024-05-02 06:09:45,138 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 06:09:45,138 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 06:09:45,138 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen, werden Sie eine <unk> <unk> sein, was über die letzten 25 Jahre passiert ist.
2024-05-02 06:10:14,898 - INFO - joeynmt.training - Epoch   5, Step:    14600, Batch Loss:     1.313687, Batch Acc: 0.572943, Tokens per Sec:     2322, Lr: 0.000300
2024-05-02 06:10:43,627 - INFO - joeynmt.training - Epoch   5, Step:    14700, Batch Loss:     1.356203, Batch Acc: 0.574738, Tokens per Sec:     2344, Lr: 0.000300
2024-05-02 06:10:57,417 - INFO - joeynmt.training - Epoch   5: total training loss 3937.62
2024-05-02 06:10:57,417 - INFO - joeynmt.training - EPOCH 6
2024-05-02 06:11:13,352 - INFO - joeynmt.training - Epoch   6, Step:    14800, Batch Loss:     1.373335, Batch Acc: 0.594031, Tokens per Sec:     2286, Lr: 0.000300
2024-05-02 06:11:42,274 - INFO - joeynmt.training - Epoch   6, Step:    14900, Batch Loss:     1.227544, Batch Acc: 0.595238, Tokens per Sec:     2339, Lr: 0.000300
2024-05-02 06:12:11,087 - INFO - joeynmt.training - Epoch   6, Step:    15000, Batch Loss:     1.209591, Batch Acc: 0.593385, Tokens per Sec:     2327, Lr: 0.000300
2024-05-02 06:12:11,087 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 06:12:11,087 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 06:12:51,638 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.40, ppl:   4.07, acc:   0.55, generation: 40.4983[sec], evaluation: 0.0000[sec]
2024-05-02 06:12:51,640 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 06:12:51,842 - INFO - joeynmt.helpers - delete models/words/12000.ckpt
2024-05-02 06:12:51,843 - INFO - joeynmt.training - Example #0
2024-05-02 06:12:51,843 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 06:12:51,843 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 06:12:51,843 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'zeigte', 'ich', 'diese', 'zwei', '<unk>', '<unk>', ',', 'dass', 'das', '<unk>', 'Eis', '<unk>', ',', 'das', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', '<unk>', ',', 'die', '<unk>', 'der', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '<unk>', 'hat', ',', '<unk>', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 06:12:51,843 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 06:12:51,843 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 06:12:51,843 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr zeigte ich diese zwei <unk> <unk>, dass das <unk> Eis <unk>, das für die meisten drei Millionen Jahre <unk>, die <unk> der Größe der <unk> <unk> <unk> <unk> hat, <unk> von 40 Prozent.
2024-05-02 06:12:51,844 - INFO - joeynmt.training - Example #1
2024-05-02 06:12:51,844 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 06:12:51,844 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 06:12:51,844 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'dieses', '<unk>', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', 'nicht', '<unk>', '.', '</s>']
2024-05-02 06:12:51,844 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 06:12:51,844 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 06:12:51,844 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> dieses <unk> Problem, weil es nicht die <unk> des <unk> nicht <unk>.
2024-05-02 06:12:51,844 - INFO - joeynmt.training - Example #2
2024-05-02 06:12:51,844 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 06:12:51,844 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 06:12:51,844 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', '<unk>', 'ist', ',', 'im', 'Sinne', 'des', '<unk>', 'des', 'globalen', '<unk>', '.', '</s>']
2024-05-02 06:12:51,845 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 06:12:51,845 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 06:12:51,845 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis <unk> ist, im Sinne des <unk> des globalen <unk>.
2024-05-02 06:12:51,845 - INFO - joeynmt.training - Example #3
2024-05-02 06:12:51,845 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 06:12:51,845 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 06:12:51,845 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'im', 'Sommer', '.', '</s>']
2024-05-02 06:12:51,845 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 06:12:51,846 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 06:12:51,846 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> im Sommer.
2024-05-02 06:12:51,846 - INFO - joeynmt.training - Example #4
2024-05-02 06:12:51,846 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 06:12:51,846 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 06:12:51,846 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'dass', 'das', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 06:12:51,846 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 06:12:51,846 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 06:12:51,846 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen, dass das, was in den letzten 25 Jahren passiert ist.
2024-05-02 06:13:19,938 - INFO - joeynmt.training - Epoch   6, Step:    15100, Batch Loss:     1.255412, Batch Acc: 0.594941, Tokens per Sec:     2408, Lr: 0.000300
2024-05-02 06:13:46,374 - INFO - joeynmt.training - Epoch   6, Step:    15200, Batch Loss:     1.204312, Batch Acc: 0.593182, Tokens per Sec:     2588, Lr: 0.000300
2024-05-02 06:14:12,547 - INFO - joeynmt.training - Epoch   6, Step:    15300, Batch Loss:     1.179080, Batch Acc: 0.591922, Tokens per Sec:     2609, Lr: 0.000300
2024-05-02 06:14:39,038 - INFO - joeynmt.training - Epoch   6, Step:    15400, Batch Loss:     1.210629, Batch Acc: 0.591033, Tokens per Sec:     2549, Lr: 0.000300
2024-05-02 06:15:05,480 - INFO - joeynmt.training - Epoch   6, Step:    15500, Batch Loss:     1.377535, Batch Acc: 0.589102, Tokens per Sec:     2591, Lr: 0.000300
2024-05-02 06:15:05,480 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 06:15:05,480 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 06:15:46,106 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.40, ppl:   4.07, acc:   0.55, generation: 40.5747[sec], evaluation: 0.0000[sec]
2024-05-02 06:15:46,107 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 06:15:46,313 - INFO - joeynmt.helpers - delete models/words/13000.ckpt
2024-05-02 06:15:46,313 - INFO - joeynmt.training - Example #0
2024-05-02 06:15:46,313 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 06:15:46,313 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 06:15:46,313 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', ',', 'ich', 'diese', 'zwei', '<unk>', 'gezeigt', ',', 'dass', 'das', '<unk>', 'Eis', '<unk>', ',', 'das', 'das', '<unk>', 'Eis', '<unk>', ',', 'das', 'für', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', ',', 'die', '<unk>', '<unk>', '<unk>', '<unk>', 'hat', '.', '</s>']
2024-05-02 06:15:46,314 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 06:15:46,314 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 06:15:46,314 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr, ich diese zwei <unk> gezeigt, dass das <unk> Eis <unk>, das das <unk> Eis <unk>, das für die letzten drei Millionen Jahre <unk>, die <unk> <unk> <unk> <unk> hat.
2024-05-02 06:15:46,314 - INFO - joeynmt.training - Example #1
2024-05-02 06:15:46,314 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 06:15:46,314 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 06:15:46,314 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', 'zeigen', '.', '</s>']
2024-05-02 06:15:46,315 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 06:15:46,315 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 06:15:46,315 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses Problem, weil es nicht die <unk> des <unk> zeigen.
2024-05-02 06:15:46,315 - INFO - joeynmt.training - Example #2
2024-05-02 06:15:46,315 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 06:15:46,315 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 06:15:46,315 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', ',', 'in', 'einem', 'Sinne', ',', 'der', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 06:15:46,315 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 06:15:46,315 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 06:15:46,315 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist, in einem Sinne, der <unk> des <unk> <unk>.
2024-05-02 06:15:46,315 - INFO - joeynmt.training - Example #3
2024-05-02 06:15:46,316 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 06:15:46,316 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 06:15:46,316 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '<unk>', '.', '</s>']
2024-05-02 06:15:46,316 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 06:15:46,316 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 06:15:46,316 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> <unk>.
2024-05-02 06:15:46,316 - INFO - joeynmt.training - Example #4
2024-05-02 06:15:46,316 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 06:15:46,316 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 06:15:46,316 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'dass', 'das', 'das', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 06:15:46,317 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 06:15:46,317 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 06:15:46,317 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen, dass das das, was in den letzten 25 Jahren passiert ist.
2024-05-02 06:16:13,349 - INFO - joeynmt.training - Epoch   6, Step:    15600, Batch Loss:     1.308109, Batch Acc: 0.593112, Tokens per Sec:     2505, Lr: 0.000300
2024-05-02 06:16:40,320 - INFO - joeynmt.training - Epoch   6, Step:    15700, Batch Loss:     1.288044, Batch Acc: 0.586190, Tokens per Sec:     2565, Lr: 0.000300
2024-05-02 06:17:07,288 - INFO - joeynmt.training - Epoch   6, Step:    15800, Batch Loss:     1.208555, Batch Acc: 0.589827, Tokens per Sec:     2532, Lr: 0.000300
2024-05-02 06:17:33,967 - INFO - joeynmt.training - Epoch   6, Step:    15900, Batch Loss:     1.427068, Batch Acc: 0.584614, Tokens per Sec:     2471, Lr: 0.000300
2024-05-02 06:17:59,969 - INFO - joeynmt.training - Epoch   6, Step:    16000, Batch Loss:     1.199786, Batch Acc: 0.592469, Tokens per Sec:     2620, Lr: 0.000300
2024-05-02 06:17:59,969 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 06:17:59,969 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 06:18:39,223 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.40, ppl:   4.07, acc:   0.55, generation: 39.2032[sec], evaluation: 0.0000[sec]
2024-05-02 06:18:39,432 - INFO - joeynmt.helpers - delete models/words/14000.ckpt
2024-05-02 06:18:39,435 - INFO - joeynmt.training - Example #0
2024-05-02 06:18:39,435 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 06:18:39,435 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 06:18:39,435 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diesen', 'zwei', '<unk>', ',', 'so', 'dass', '<unk>', '<unk>', ',', 'dass', 'das', '<unk>', '<unk>', '<unk>', ',', 'das', 'für', 'die', '<unk>', '<unk>', '<unk>', ',', '<unk>', 'von', '40', '%', '<unk>', '<unk>', ',', '<unk>', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 06:18:39,436 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 06:18:39,436 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 06:18:39,436 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diesen zwei <unk>, so dass <unk> <unk>, dass das <unk> <unk> <unk>, das für die <unk> <unk> <unk>, <unk> von 40% <unk> <unk>, <unk> von 40 Prozent.
2024-05-02 06:18:39,436 - INFO - joeynmt.training - Example #1
2024-05-02 06:18:39,436 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 06:18:39,436 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 06:18:39,436 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', '<unk>', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'das', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 06:18:39,437 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 06:18:39,437 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 06:18:39,437 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> <unk> dieses Problem, weil es nicht das <unk> des <unk>.
2024-05-02 06:18:39,437 - INFO - joeynmt.training - Example #2
2024-05-02 06:18:39,437 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 06:18:39,437 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 06:18:39,437 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', ',', 'in', 'einem', 'Sinne', 'des', '<unk>', 'des', 'globalen', '<unk>', '.', '</s>']
2024-05-02 06:18:39,437 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 06:18:39,437 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 06:18:39,438 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist, in einem Sinne des <unk> des globalen <unk>.
2024-05-02 06:18:39,438 - INFO - joeynmt.training - Example #3
2024-05-02 06:18:39,438 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 06:18:39,438 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 06:18:39,438 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'im', 'Sommer', '.', '</s>']
2024-05-02 06:18:39,438 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 06:18:39,438 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 06:18:39,438 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> im Sommer.
2024-05-02 06:18:39,438 - INFO - joeynmt.training - Example #4
2024-05-02 06:18:39,438 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 06:18:39,439 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 06:18:39,439 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'dass', 'Sie', 'eine', '<unk>', '<unk>', '<unk>', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 06:18:39,439 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 06:18:39,439 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 06:18:39,439 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, dass Sie eine <unk> <unk> <unk>, was in den letzten 25 Jahren passiert ist.
2024-05-02 06:19:06,293 - INFO - joeynmt.training - Epoch   6, Step:    16100, Batch Loss:     1.269365, Batch Acc: 0.581827, Tokens per Sec:     2539, Lr: 0.000300
2024-05-02 06:19:33,380 - INFO - joeynmt.training - Epoch   6, Step:    16200, Batch Loss:     1.350469, Batch Acc: 0.592753, Tokens per Sec:     2521, Lr: 0.000300
2024-05-02 06:20:00,390 - INFO - joeynmt.training - Epoch   6, Step:    16300, Batch Loss:     1.198456, Batch Acc: 0.583175, Tokens per Sec:     2584, Lr: 0.000300
2024-05-02 06:20:26,970 - INFO - joeynmt.training - Epoch   6, Step:    16400, Batch Loss:     1.307222, Batch Acc: 0.583927, Tokens per Sec:     2513, Lr: 0.000300
2024-05-02 06:20:53,542 - INFO - joeynmt.training - Epoch   6, Step:    16500, Batch Loss:     1.423021, Batch Acc: 0.584430, Tokens per Sec:     2485, Lr: 0.000300
2024-05-02 06:20:53,542 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 06:20:53,542 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 06:21:33,902 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.02, acc:   0.56, generation: 40.3095[sec], evaluation: 0.0000[sec]
2024-05-02 06:21:33,904 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 06:21:34,103 - INFO - joeynmt.helpers - delete models/words/13500.ckpt
2024-05-02 06:21:34,103 - INFO - joeynmt.training - Example #0
2024-05-02 06:21:34,103 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 06:21:34,103 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 06:21:34,103 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', '<unk>', 'ich', 'diese', 'beiden', '<unk>', '<unk>', ',', 'so', 'dass', 'die', '<unk>', '<unk>', '<unk>', ',', 'die', 'für', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', 'waren', 'die', 'Größe', 'der', 'Größe', '<unk>', '<unk>', '<unk>', ',', 'hat', 'von', '40', 'Prozent', '<unk>', '<unk>', '.', '</s>']
2024-05-02 06:21:34,104 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 06:21:34,104 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 06:21:34,104 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr <unk> ich diese beiden <unk> <unk>, so dass die <unk> <unk> <unk>, die für die letzten drei Millionen Jahre waren die Größe der Größe <unk> <unk> <unk>, hat von 40 Prozent <unk> <unk>.
2024-05-02 06:21:34,104 - INFO - joeynmt.training - Example #1
2024-05-02 06:21:34,104 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 06:21:34,104 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 06:21:34,104 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', '<unk>', '<unk>', 'dieses', '<unk>', 'Problem', ',', 'weil', 'es', 'die', '<unk>', 'nicht', 'das', '<unk>', 'des', '<unk>', 'zeigen', '.', '</s>']
2024-05-02 06:21:34,104 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 06:21:34,105 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 06:21:34,105 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> <unk> <unk> dieses <unk> Problem, weil es die <unk> nicht das <unk> des <unk> zeigen.
2024-05-02 06:21:34,105 - INFO - joeynmt.training - Example #2
2024-05-02 06:21:34,105 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 06:21:34,105 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 06:21:34,105 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', ',', 'in', 'einem', 'Sinn', ',', 'das', '<unk>', 'Herz', 'des', '<unk>', '.', '</s>']
2024-05-02 06:21:34,105 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 06:21:34,105 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 06:21:34,105 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist, in einem Sinn, das <unk> Herz des <unk>.
2024-05-02 06:21:34,105 - INFO - joeynmt.training - Example #3
2024-05-02 06:21:34,105 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 06:21:34,105 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 06:21:34,106 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'im', 'Sommer', '.', '</s>']
2024-05-02 06:21:34,106 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 06:21:34,106 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 06:21:34,106 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> im Sommer.
2024-05-02 06:21:34,106 - INFO - joeynmt.training - Example #4
2024-05-02 06:21:34,106 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 06:21:34,106 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 06:21:34,106 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'dass', 'Sie', 'ein', '<unk>', '<unk>', 'sein', 'wird', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 06:21:34,107 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 06:21:34,107 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 06:21:34,107 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, dass Sie ein <unk> <unk> sein wird, was in den letzten 25 Jahren passiert ist.
2024-05-02 06:22:00,449 - INFO - joeynmt.training - Epoch   6, Step:    16600, Batch Loss:     1.246681, Batch Acc: 0.586882, Tokens per Sec:     2553, Lr: 0.000300
2024-05-02 06:22:27,172 - INFO - joeynmt.training - Epoch   6, Step:    16700, Batch Loss:     1.224997, Batch Acc: 0.580753, Tokens per Sec:     2561, Lr: 0.000300
2024-05-02 06:22:53,732 - INFO - joeynmt.training - Epoch   6, Step:    16800, Batch Loss:     1.315203, Batch Acc: 0.582613, Tokens per Sec:     2503, Lr: 0.000300
2024-05-02 06:23:20,231 - INFO - joeynmt.training - Epoch   6, Step:    16900, Batch Loss:     1.244192, Batch Acc: 0.585153, Tokens per Sec:     2560, Lr: 0.000300
2024-05-02 06:23:47,147 - INFO - joeynmt.training - Epoch   6, Step:    17000, Batch Loss:     1.367613, Batch Acc: 0.583369, Tokens per Sec:     2538, Lr: 0.000300
2024-05-02 06:23:47,148 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 06:23:47,148 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 06:24:25,216 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.01, acc:   0.56, generation: 38.0181[sec], evaluation: 0.0000[sec]
2024-05-02 06:24:25,218 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 06:24:25,421 - INFO - joeynmt.helpers - delete models/words/14500.ckpt
2024-05-02 06:24:25,425 - INFO - joeynmt.training - Example #0
2024-05-02 06:24:25,425 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 06:24:25,425 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 06:24:25,425 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'beiden', '<unk>', '<unk>', 'gezeigt', ',', 'dass', 'das', '<unk>', '<unk>', ',', 'das', 'das', '<unk>', '<unk>', ',', 'das', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', '<unk>', 'hat', ',', 'hat', 'sich', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', ',', 'hat', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 06:24:25,426 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 06:24:25,426 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 06:24:25,426 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese beiden <unk> <unk> gezeigt, dass das <unk> <unk>, das das <unk> <unk>, das die meisten drei Millionen Jahre <unk> hat, hat sich die Größe der <unk> <unk> <unk>, hat von 40 Prozent.
2024-05-02 06:24:25,426 - INFO - joeynmt.training - Example #1
2024-05-02 06:24:25,426 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 06:24:25,426 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 06:24:25,426 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', '<unk>', 'das', '<unk>', 'dieses', '<unk>', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 06:24:25,426 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 06:24:25,426 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 06:24:25,426 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> <unk> das <unk> dieses <unk> Problem, weil es nicht die <unk> des <unk> <unk>.
2024-05-02 06:24:25,427 - INFO - joeynmt.training - Example #2
2024-05-02 06:24:25,427 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 06:24:25,427 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 06:24:25,427 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', '<unk>', 'ist', ',', 'in', 'einem', 'Sinne', ',', 'das', '<unk>', 'Herz', 'des', 'globalen', '<unk>', '.', '</s>']
2024-05-02 06:24:25,427 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 06:24:25,427 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 06:24:25,427 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> <unk> ist, in einem Sinne, das <unk> Herz des globalen <unk>.
2024-05-02 06:24:25,427 - INFO - joeynmt.training - Example #3
2024-05-02 06:24:25,427 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 06:24:25,427 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 06:24:25,427 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '.', '</s>']
2024-05-02 06:24:25,428 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 06:24:25,428 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 06:24:25,428 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk>.
2024-05-02 06:24:25,428 - INFO - joeynmt.training - Example #4
2024-05-02 06:24:25,428 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 06:24:25,428 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 06:24:25,428 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'dass', 'Sie', 'ein', '<unk>', '<unk>', 'sein', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 06:24:25,428 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 06:24:25,428 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 06:24:25,429 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen, dass Sie ein <unk> <unk> sein, was in den letzten 25 Jahren passiert ist.
2024-05-02 06:24:51,879 - INFO - joeynmt.training - Epoch   6, Step:    17100, Batch Loss:     1.335876, Batch Acc: 0.583298, Tokens per Sec:     2543, Lr: 0.000300
2024-05-02 06:25:19,212 - INFO - joeynmt.training - Epoch   6, Step:    17200, Batch Loss:     1.319143, Batch Acc: 0.585659, Tokens per Sec:     2534, Lr: 0.000300
2024-05-02 06:25:45,607 - INFO - joeynmt.training - Epoch   6, Step:    17300, Batch Loss:     1.248307, Batch Acc: 0.585151, Tokens per Sec:     2508, Lr: 0.000300
2024-05-02 06:26:12,592 - INFO - joeynmt.training - Epoch   6, Step:    17400, Batch Loss:     1.275078, Batch Acc: 0.582917, Tokens per Sec:     2522, Lr: 0.000300
2024-05-02 06:26:39,155 - INFO - joeynmt.training - Epoch   6, Step:    17500, Batch Loss:     1.372259, Batch Acc: 0.579988, Tokens per Sec:     2548, Lr: 0.000300
2024-05-02 06:26:39,155 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 06:26:39,155 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 06:27:14,411 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.99, acc:   0.56, generation: 35.2066[sec], evaluation: 0.0000[sec]
2024-05-02 06:27:14,412 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 06:27:14,616 - INFO - joeynmt.helpers - delete models/words/16000.ckpt
2024-05-02 06:27:14,620 - INFO - joeynmt.training - Example #0
2024-05-02 06:27:14,620 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 06:27:14,620 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 06:27:14,620 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', ',', 'ich', 'diese', 'zwei', '<unk>', '<unk>', ',', 'dass', 'das', 'das', '<unk>', 'Eis', '<unk>', ',', 'das', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'die', 'die', '<unk>', '<unk>', ',', 'die', 'die', '<unk>', '<unk>']
2024-05-02 06:27:14,621 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 06:27:14,621 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 06:27:14,621 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr, ich diese zwei <unk> <unk>, dass das das <unk> Eis <unk>, das die für die meisten drei Millionen Jahre die Größe der <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>, die die <unk> <unk>, die die <unk> <unk>
2024-05-02 06:27:14,621 - INFO - joeynmt.training - Example #1
2024-05-02 06:27:14,621 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 06:27:14,621 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 06:27:14,621 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'weil', 'es', 'die', '<unk>', 'nicht', 'die', '<unk>', 'des', '<unk>', 'zeigen', '.', '</s>']
2024-05-02 06:27:14,621 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 06:27:14,621 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 06:27:14,621 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> <unk> <unk> <unk>, weil es die <unk> nicht die <unk> des <unk> zeigen.
2024-05-02 06:27:14,621 - INFO - joeynmt.training - Example #2
2024-05-02 06:27:14,622 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 06:27:14,622 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 06:27:14,622 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', '<unk>', '<unk>', 'ist', 'in', 'einem', 'Sinne', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 06:27:14,622 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 06:27:14,622 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 06:27:14,622 - INFO - joeynmt.training - 	Hypothesis: Der <unk> <unk> ist in einem Sinne des <unk> <unk>.
2024-05-02 06:27:14,622 - INFO - joeynmt.training - Example #3
2024-05-02 06:27:14,622 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 06:27:14,622 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 06:27:14,622 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '.', '</s>']
2024-05-02 06:27:14,623 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 06:27:14,623 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 06:27:14,623 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk>.
2024-05-02 06:27:14,623 - INFO - joeynmt.training - Example #4
2024-05-02 06:27:14,623 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 06:27:14,623 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 06:27:14,623 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'dass', 'Sie', 'ein', '<unk>', '<unk>', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 06:27:14,623 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 06:27:14,623 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 06:27:14,623 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen, dass Sie ein <unk> <unk> von dem, was in den letzten 25 Jahren passiert ist.
2024-05-02 06:27:40,980 - INFO - joeynmt.training - Epoch   6, Step:    17600, Batch Loss:     1.496627, Batch Acc: 0.585588, Tokens per Sec:     2513, Lr: 0.000300
2024-05-02 06:28:07,392 - INFO - joeynmt.training - Epoch   6, Step:    17700, Batch Loss:     1.312195, Batch Acc: 0.588308, Tokens per Sec:     2556, Lr: 0.000300
2024-05-02 06:28:07,619 - INFO - joeynmt.training - Epoch   6: total training loss 3808.46
2024-05-02 06:28:07,619 - INFO - joeynmt.training - EPOCH 7
2024-05-02 06:28:34,120 - INFO - joeynmt.training - Epoch   7, Step:    17800, Batch Loss:     1.221814, Batch Acc: 0.604021, Tokens per Sec:     2569, Lr: 0.000300
2024-05-02 06:29:00,119 - INFO - joeynmt.training - Epoch   7, Step:    17900, Batch Loss:     1.310434, Batch Acc: 0.605286, Tokens per Sec:     2583, Lr: 0.000300
2024-05-02 06:29:26,671 - INFO - joeynmt.training - Epoch   7, Step:    18000, Batch Loss:     1.244587, Batch Acc: 0.604106, Tokens per Sec:     2509, Lr: 0.000300
2024-05-02 06:29:26,672 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 06:29:26,672 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 06:30:01,853 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.01, acc:   0.56, generation: 35.1316[sec], evaluation: 0.0000[sec]
2024-05-02 06:30:02,055 - INFO - joeynmt.helpers - delete models/words/15000.ckpt
2024-05-02 06:30:02,060 - INFO - joeynmt.training - Example #0
2024-05-02 06:30:02,060 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 06:30:02,060 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 06:30:02,060 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', '<unk>', 'ich', 'diese', 'beiden', '<unk>', 'gezeigt', ',', 'so', 'dass', '<unk>', 'Eis', '<unk>', ',', 'das', '<unk>', '<unk>', ',', 'das', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', '<unk>', 'hat', ',', 'hat', '<unk>', '<unk>', '<unk>', ',', 'hat', '<unk>', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 06:30:02,060 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 06:30:02,060 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 06:30:02,060 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr <unk> ich diese beiden <unk> gezeigt, so dass <unk> Eis <unk>, das <unk> <unk>, das für die meisten drei Millionen Jahre <unk> hat, hat <unk> <unk> <unk>, hat <unk> von 40 Prozent.
2024-05-02 06:30:02,061 - INFO - joeynmt.training - Example #1
2024-05-02 06:30:02,061 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 06:30:02,061 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 06:30:02,061 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', '<unk>', '<unk>', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 06:30:02,061 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 06:30:02,061 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 06:30:02,061 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> <unk> <unk>, weil es nicht die <unk> des <unk> <unk>.
2024-05-02 06:30:02,061 - INFO - joeynmt.training - Example #2
2024-05-02 06:30:02,061 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 06:30:02,061 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 06:30:02,061 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', ',', 'in', 'einem', 'Sinne', ',', 'das', '<unk>', 'Herz', 'des', '<unk>', '.', '</s>']
2024-05-02 06:30:02,062 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 06:30:02,062 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 06:30:02,062 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist, in einem Sinne, das <unk> Herz des <unk>.
2024-05-02 06:30:02,062 - INFO - joeynmt.training - Example #3
2024-05-02 06:30:02,062 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 06:30:02,062 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 06:30:02,062 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '.', '</s>']
2024-05-02 06:30:02,062 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 06:30:02,063 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 06:30:02,063 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>.
2024-05-02 06:30:02,063 - INFO - joeynmt.training - Example #4
2024-05-02 06:30:02,063 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 06:30:02,063 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 06:30:02,063 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'dass', 'ich', '<unk>', '<unk>', '<unk>', 'dessen', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 06:30:02,063 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 06:30:02,063 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 06:30:02,063 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, dass ich <unk> <unk> <unk> dessen, was in den letzten 25 Jahren passiert ist.
2024-05-02 06:30:28,970 - INFO - joeynmt.training - Epoch   7, Step:    18100, Batch Loss:     1.235423, Batch Acc: 0.604601, Tokens per Sec:     2525, Lr: 0.000300
2024-05-02 06:30:55,377 - INFO - joeynmt.training - Epoch   7, Step:    18200, Batch Loss:     1.261789, Batch Acc: 0.605024, Tokens per Sec:     2551, Lr: 0.000300
2024-05-02 06:31:21,412 - INFO - joeynmt.training - Epoch   7, Step:    18300, Batch Loss:     1.230070, Batch Acc: 0.602641, Tokens per Sec:     2598, Lr: 0.000300
2024-05-02 06:31:48,093 - INFO - joeynmt.training - Epoch   7, Step:    18400, Batch Loss:     1.322291, Batch Acc: 0.601630, Tokens per Sec:     2579, Lr: 0.000300
2024-05-02 06:32:14,189 - INFO - joeynmt.training - Epoch   7, Step:    18500, Batch Loss:     1.217417, Batch Acc: 0.600836, Tokens per Sec:     2602, Lr: 0.000300
2024-05-02 06:32:14,190 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 06:32:14,190 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 06:32:50,541 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.03, acc:   0.56, generation: 36.3024[sec], evaluation: 0.0000[sec]
2024-05-02 06:32:50,738 - INFO - joeynmt.helpers - delete models/words/15500.ckpt
2024-05-02 06:32:50,741 - INFO - joeynmt.training - Example #0
2024-05-02 06:32:50,742 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 06:32:50,742 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 06:32:50,742 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', ',', 'ich', 'diese', 'beiden', '<unk>', 'gezeigt', ',', 'dass', 'das', '<unk>', 'Eis', '<unk>', ',', 'das', 'für', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-02 06:32:50,742 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 06:32:50,742 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 06:32:50,742 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr, ich diese beiden <unk> gezeigt, dass das <unk> Eis <unk>, das für die meisten der letzten drei Millionen Jahre die Größe der <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>.
2024-05-02 06:32:50,742 - INFO - joeynmt.training - Example #1
2024-05-02 06:32:50,742 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 06:32:50,742 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 06:32:50,743 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'des', '<unk>', '<unk>', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 06:32:50,743 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 06:32:50,743 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 06:32:50,743 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> des <unk> <unk>, weil es nicht die <unk> des <unk> <unk>.
2024-05-02 06:32:50,743 - INFO - joeynmt.training - Example #2
2024-05-02 06:32:50,743 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 06:32:50,743 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 06:32:50,743 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', ',', 'in', '<unk>', 'Sinne', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 06:32:50,743 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 06:32:50,744 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 06:32:50,744 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist, in <unk> Sinne des <unk> <unk>.
2024-05-02 06:32:50,744 - INFO - joeynmt.training - Example #3
2024-05-02 06:32:50,744 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 06:32:50,744 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 06:32:50,744 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '.', '</s>']
2024-05-02 06:32:50,744 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 06:32:50,744 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 06:32:50,744 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk>.
2024-05-02 06:32:50,744 - INFO - joeynmt.training - Example #4
2024-05-02 06:32:50,744 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 06:32:50,744 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 06:32:50,745 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'dass', 'das', '<unk>', '<unk>', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 06:32:50,745 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 06:32:50,745 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 06:32:50,745 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen, dass das <unk> <unk> von dem, was in den letzten 25 Jahren passiert ist.
2024-05-02 06:33:16,717 - INFO - joeynmt.training - Epoch   7, Step:    18600, Batch Loss:     1.262135, Batch Acc: 0.599692, Tokens per Sec:     2675, Lr: 0.000300
2024-05-02 06:33:42,703 - INFO - joeynmt.training - Epoch   7, Step:    18700, Batch Loss:     1.301511, Batch Acc: 0.594336, Tokens per Sec:     2670, Lr: 0.000300
2024-05-02 06:34:09,213 - INFO - joeynmt.training - Epoch   7, Step:    18800, Batch Loss:     1.162060, Batch Acc: 0.600133, Tokens per Sec:     2494, Lr: 0.000300
2024-05-02 06:34:35,345 - INFO - joeynmt.training - Epoch   7, Step:    18900, Batch Loss:     1.362514, Batch Acc: 0.597628, Tokens per Sec:     2614, Lr: 0.000300
2024-05-02 06:35:01,621 - INFO - joeynmt.training - Epoch   7, Step:    19000, Batch Loss:     1.163898, Batch Acc: 0.599185, Tokens per Sec:     2568, Lr: 0.000300
2024-05-02 06:35:01,621 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 06:35:01,622 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 06:35:40,766 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.01, acc:   0.56, generation: 39.0955[sec], evaluation: 0.0000[sec]
2024-05-02 06:35:40,974 - INFO - joeynmt.helpers - delete models/words/18500.ckpt
2024-05-02 06:35:40,976 - INFO - joeynmt.helpers - delete /Users/Iris/test/mt-exercise-5/models/words/18500.ckpt
2024-05-02 06:35:40,976 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /Users/Iris/test/mt-exercise-5/models/words/18500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/Users/Iris/test/mt-exercise-5/models/words/18500.ckpt')
2024-05-02 06:35:40,977 - INFO - joeynmt.training - Example #0
2024-05-02 06:35:40,977 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 06:35:40,977 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 06:35:40,977 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', ',', 'ich', 'diese', 'beiden', '<unk>', 'so', '<unk>', ',', 'dass', 'das', '<unk>', '<unk>', ',', 'das', 'für', 'die', 'meisten', '<unk>', 'drei', 'Millionen', 'Jahre', '<unk>', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', ',', 'hat', 'von', '40', 'Prozent', '<unk>', '<unk>', '.', '</s>']
2024-05-02 06:35:40,977 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 06:35:40,977 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 06:35:40,977 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr, ich diese beiden <unk> so <unk>, dass das <unk> <unk>, das für die meisten <unk> drei Millionen Jahre <unk> die Größe der <unk> <unk> <unk>, hat von 40 Prozent <unk> <unk>.
2024-05-02 06:35:40,978 - INFO - joeynmt.training - Example #1
2024-05-02 06:35:40,978 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 06:35:40,978 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 06:35:40,978 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', '<unk>', '<unk>', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 06:35:40,978 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 06:35:40,978 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 06:35:40,978 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> <unk> <unk>, weil es nicht die <unk> des <unk>.
2024-05-02 06:35:40,978 - INFO - joeynmt.training - Example #2
2024-05-02 06:35:40,978 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 06:35:40,978 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 06:35:40,978 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', 'einem', 'Sinne', ',', 'der', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 06:35:40,979 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 06:35:40,979 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 06:35:40,979 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in einem Sinne, der <unk> des <unk> <unk>.
2024-05-02 06:35:40,979 - INFO - joeynmt.training - Example #3
2024-05-02 06:35:40,979 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 06:35:40,979 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 06:35:40,979 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'im', 'Sommer', '.', '</s>']
2024-05-02 06:35:40,979 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 06:35:40,979 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 06:35:40,980 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> im Sommer.
2024-05-02 06:35:40,980 - INFO - joeynmt.training - Example #4
2024-05-02 06:35:40,980 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 06:35:40,980 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 06:35:40,980 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'dass', 'das', '<unk>', '<unk>', 'sein', 'werden', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 06:35:40,980 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 06:35:40,980 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 06:35:40,980 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen, dass das <unk> <unk> sein werden, was in den letzten 25 Jahren passiert ist.
2024-05-02 06:36:07,668 - INFO - joeynmt.training - Epoch   7, Step:    19100, Batch Loss:     1.260152, Batch Acc: 0.598202, Tokens per Sec:     2531, Lr: 0.000300
2024-05-02 06:36:34,230 - INFO - joeynmt.training - Epoch   7, Step:    19200, Batch Loss:     1.296928, Batch Acc: 0.598765, Tokens per Sec:     2535, Lr: 0.000300
2024-05-02 06:37:00,791 - INFO - joeynmt.training - Epoch   7, Step:    19300, Batch Loss:     1.278194, Batch Acc: 0.595115, Tokens per Sec:     2620, Lr: 0.000300
2024-05-02 06:37:26,747 - INFO - joeynmt.training - Epoch   7, Step:    19400, Batch Loss:     1.178632, Batch Acc: 0.592779, Tokens per Sec:     2685, Lr: 0.000300
2024-05-02 06:37:53,519 - INFO - joeynmt.training - Epoch   7, Step:    19500, Batch Loss:     1.163594, Batch Acc: 0.601870, Tokens per Sec:     2541, Lr: 0.000300
2024-05-02 06:37:53,519 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 06:37:53,519 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 06:38:29,428 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.01, acc:   0.56, generation: 35.8594[sec], evaluation: 0.0000[sec]
2024-05-02 06:38:29,622 - INFO - joeynmt.helpers - delete models/words/16500.ckpt
2024-05-02 06:38:29,627 - INFO - joeynmt.training - Example #0
2024-05-02 06:38:29,628 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 06:38:29,628 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 06:38:29,628 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'zeigte', 'ich', 'diese', 'beiden', '<unk>', ',', 'dass', 'die', '<unk>', '<unk>', ',', 'die', 'für', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'war', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', ',', '<unk>', 'von', '40', 'Prozent', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'hat', '<unk>', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 06:38:29,628 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 06:38:29,628 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 06:38:29,628 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr zeigte ich diese beiden <unk>, dass die <unk> <unk>, die für die meisten der letzten drei Millionen Jahre war die Größe der <unk> <unk> <unk>, <unk> von 40 Prozent <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>, hat <unk> von 40 Prozent.
2024-05-02 06:38:29,628 - INFO - joeynmt.training - Example #1
2024-05-02 06:38:29,628 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 06:38:29,628 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 06:38:29,628 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'dieses', '<unk>', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 06:38:29,629 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 06:38:29,629 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 06:38:29,629 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> dieses <unk> Problem, weil es nicht die <unk> des <unk> <unk>.
2024-05-02 06:38:29,629 - INFO - joeynmt.training - Example #2
2024-05-02 06:38:29,629 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 06:38:29,629 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 06:38:29,629 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', ',', 'in', 'einem', 'Sinne', 'des', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 06:38:29,629 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 06:38:29,629 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 06:38:29,630 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist, in einem Sinne des <unk> des <unk>.
2024-05-02 06:38:29,630 - INFO - joeynmt.training - Example #3
2024-05-02 06:38:29,630 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 06:38:29,630 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 06:38:29,630 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'im', 'Sommer', '.', '</s>']
2024-05-02 06:38:29,630 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 06:38:29,630 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 06:38:29,630 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> im Sommer.
2024-05-02 06:38:29,630 - INFO - joeynmt.training - Example #4
2024-05-02 06:38:29,630 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 06:38:29,630 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 06:38:29,630 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'werden', 'Sie', 'eine', '<unk>', '<unk>', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 06:38:29,631 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 06:38:29,631 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 06:38:29,631 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, werden Sie eine <unk> <unk> von dem, was in den letzten 25 Jahren passiert ist.
2024-05-02 06:38:57,035 - INFO - joeynmt.training - Epoch   7, Step:    19600, Batch Loss:     1.348455, Batch Acc: 0.594643, Tokens per Sec:     2471, Lr: 0.000300
2024-05-02 06:39:23,222 - INFO - joeynmt.training - Epoch   7, Step:    19700, Batch Loss:     1.200129, Batch Acc: 0.597478, Tokens per Sec:     2559, Lr: 0.000300
2024-05-02 06:39:50,554 - INFO - joeynmt.training - Epoch   7, Step:    19800, Batch Loss:     1.204820, Batch Acc: 0.592755, Tokens per Sec:     2519, Lr: 0.000300
2024-05-02 06:40:16,361 - INFO - joeynmt.training - Epoch   7, Step:    19900, Batch Loss:     1.265156, Batch Acc: 0.591376, Tokens per Sec:     2611, Lr: 0.000300
2024-05-02 06:40:42,729 - INFO - joeynmt.training - Epoch   7, Step:    20000, Batch Loss:     1.330791, Batch Acc: 0.590376, Tokens per Sec:     2590, Lr: 0.000300
2024-05-02 06:40:42,730 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 06:40:42,730 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 06:41:19,206 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.99, acc:   0.56, generation: 36.4263[sec], evaluation: 0.0000[sec]
2024-05-02 06:41:19,407 - INFO - joeynmt.helpers - delete models/words/19500.ckpt
2024-05-02 06:41:19,410 - INFO - joeynmt.helpers - delete /Users/Iris/test/mt-exercise-5/models/words/19500.ckpt
2024-05-02 06:41:19,410 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /Users/Iris/test/mt-exercise-5/models/words/19500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/Users/Iris/test/mt-exercise-5/models/words/19500.ckpt')
2024-05-02 06:41:19,410 - INFO - joeynmt.training - Example #0
2024-05-02 06:41:19,410 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 06:41:19,410 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 06:41:19,410 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', ',', 'dass', 'das', '<unk>', 'Eis', '<unk>', ',', 'das', '<unk>', '<unk>', ',', 'das', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'war', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 'hat', ',', 'hat', '<unk>', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 06:41:19,410 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 06:41:19,411 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 06:41:19,411 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk>, dass das <unk> Eis <unk>, das <unk> <unk>, das für die meisten drei Millionen Jahre war die Größe der <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> hat, hat <unk> von 40 Prozent.
2024-05-02 06:41:19,411 - INFO - joeynmt.training - Example #1
2024-05-02 06:41:19,411 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 06:41:19,411 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 06:41:19,411 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', 'die', '<unk>', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 06:41:19,411 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 06:41:19,411 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 06:41:19,411 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> die <unk> dieses Problem, weil es nicht die <unk> des <unk> <unk>.
2024-05-02 06:41:19,411 - INFO - joeynmt.training - Example #2
2024-05-02 06:41:19,411 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 06:41:19,412 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 06:41:19,412 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', ',', 'in', 'einem', 'Sinn', ',', 'der', '<unk>', 'des', 'globalen', '<unk>', '.', '</s>']
2024-05-02 06:41:19,412 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 06:41:19,412 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 06:41:19,412 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist, in einem Sinn, der <unk> des globalen <unk>.
2024-05-02 06:41:19,412 - INFO - joeynmt.training - Example #3
2024-05-02 06:41:19,412 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 06:41:19,412 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 06:41:19,412 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '<unk>', '.', '</s>']
2024-05-02 06:41:19,413 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 06:41:19,413 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 06:41:19,413 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> <unk>.
2024-05-02 06:41:19,413 - INFO - joeynmt.training - Example #4
2024-05-02 06:41:19,413 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 06:41:19,413 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 06:41:19,413 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'zeige', ',', 'dass', 'Ihnen', 'eine', '<unk>', '<unk>', '<unk>', 'davon', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 06:41:19,413 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 06:41:19,413 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 06:41:19,413 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich zeige, dass Ihnen eine <unk> <unk> <unk> davon, was in den letzten 25 Jahren passiert ist.
2024-05-02 06:41:45,606 - INFO - joeynmt.training - Epoch   7, Step:    20100, Batch Loss:     1.183952, Batch Acc: 0.594483, Tokens per Sec:     2579, Lr: 0.000300
2024-05-02 06:42:11,758 - INFO - joeynmt.training - Epoch   7, Step:    20200, Batch Loss:     1.446875, Batch Acc: 0.589365, Tokens per Sec:     2633, Lr: 0.000300
2024-05-02 06:42:38,138 - INFO - joeynmt.training - Epoch   7, Step:    20300, Batch Loss:     1.255750, Batch Acc: 0.591790, Tokens per Sec:     2569, Lr: 0.000300
2024-05-02 06:43:04,365 - INFO - joeynmt.training - Epoch   7, Step:    20400, Batch Loss:     1.283882, Batch Acc: 0.592527, Tokens per Sec:     2569, Lr: 0.000300
2024-05-02 06:43:30,208 - INFO - joeynmt.training - Epoch   7, Step:    20500, Batch Loss:     1.235873, Batch Acc: 0.593706, Tokens per Sec:     2639, Lr: 0.000300
2024-05-02 06:43:30,208 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 06:43:30,208 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 06:44:01,813 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.96, acc:   0.56, generation: 31.5556[sec], evaluation: 0.0000[sec]
2024-05-02 06:44:01,814 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 06:44:02,016 - INFO - joeynmt.helpers - delete models/words/17000.ckpt
2024-05-02 06:44:02,019 - INFO - joeynmt.training - Example #0
2024-05-02 06:44:02,019 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 06:44:02,019 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 06:44:02,019 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', 'gezeigt', ',', 'dass', 'das', '<unk>', 'Eis', '<unk>', ',', 'das', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '<unk>', 'hat', ',', 'hat', 'von', '40', 'Prozent', '<unk>', '.', '</s>']
2024-05-02 06:44:02,020 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 06:44:02,020 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 06:44:02,020 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> gezeigt, dass das <unk> Eis <unk>, das für die meisten drei Millionen Jahre lang die Größe der <unk> <unk> <unk> <unk> hat, hat von 40 Prozent <unk>.
2024-05-02 06:44:02,020 - INFO - joeynmt.training - Example #1
2024-05-02 06:44:02,020 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 06:44:02,020 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 06:44:02,020 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', '<unk>', 'das', '<unk>', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', 'nicht', 'das', 'Eis', '.', '</s>']
2024-05-02 06:44:02,020 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 06:44:02,021 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 06:44:02,021 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> <unk> das <unk> Problem, weil es nicht die <unk> des <unk> nicht das Eis.
2024-05-02 06:44:02,021 - INFO - joeynmt.training - Example #2
2024-05-02 06:44:02,021 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 06:44:02,021 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 06:44:02,021 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', 'einem', 'Sinne', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 06:44:02,021 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 06:44:02,021 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 06:44:02,021 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in einem Sinne <unk> des <unk>.
2024-05-02 06:44:02,021 - INFO - joeynmt.training - Example #3
2024-05-02 06:44:02,021 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 06:44:02,022 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 06:44:02,022 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '.', '</s>']
2024-05-02 06:44:02,022 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 06:44:02,022 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 06:44:02,022 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk>.
2024-05-02 06:44:02,022 - INFO - joeynmt.training - Example #4
2024-05-02 06:44:02,022 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 06:44:02,022 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 06:44:02,022 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'dass', 'das', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 06:44:02,023 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 06:44:02,023 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 06:44:02,023 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen, dass das, was in den letzten 25 Jahren passiert ist.
2024-05-02 06:44:28,630 - INFO - joeynmt.training - Epoch   7, Step:    20600, Batch Loss:     1.153914, Batch Acc: 0.591843, Tokens per Sec:     2537, Lr: 0.000300
2024-05-02 06:44:40,538 - INFO - joeynmt.training - Epoch   7: total training loss 3687.20
2024-05-02 06:44:40,538 - INFO - joeynmt.training - EPOCH 8
2024-05-02 06:44:54,432 - INFO - joeynmt.training - Epoch   8, Step:    20700, Batch Loss:     1.309199, Batch Acc: 0.620715, Tokens per Sec:     2755, Lr: 0.000300
2024-05-02 06:45:21,289 - INFO - joeynmt.training - Epoch   8, Step:    20800, Batch Loss:     1.115363, Batch Acc: 0.612573, Tokens per Sec:     2534, Lr: 0.000300
2024-05-02 06:45:47,803 - INFO - joeynmt.training - Epoch   8, Step:    20900, Batch Loss:     1.264227, Batch Acc: 0.614013, Tokens per Sec:     2623, Lr: 0.000300
2024-05-02 06:46:13,834 - INFO - joeynmt.training - Epoch   8, Step:    21000, Batch Loss:     1.168255, Batch Acc: 0.616397, Tokens per Sec:     2646, Lr: 0.000300
2024-05-02 06:46:13,835 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 06:46:13,835 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 06:46:53,722 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.98, acc:   0.56, generation: 39.8373[sec], evaluation: 0.0000[sec]
2024-05-02 06:46:53,922 - INFO - joeynmt.helpers - delete models/words/18000.ckpt
2024-05-02 06:46:53,925 - INFO - joeynmt.training - Example #0
2024-05-02 06:46:53,925 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 06:46:53,925 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 06:46:53,925 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'beiden', '<unk>', 'gezeigt', ',', 'dass', 'das', '<unk>', 'Eis', '<unk>', ',', 'das', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'der', 'Größe', 'der', '<unk>', '<unk>', '<unk>', ',', 'hat', 'von', '40', 'Prozent', '<unk>', '.', '</s>']
2024-05-02 06:46:53,925 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 06:46:53,926 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 06:46:53,926 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese beiden <unk> gezeigt, dass das <unk> Eis <unk>, das für die meisten drei Millionen Jahre der Größe der <unk> <unk> <unk>, hat von 40 Prozent <unk>.
2024-05-02 06:46:53,926 - INFO - joeynmt.training - Example #1
2024-05-02 06:46:53,926 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 06:46:53,926 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 06:46:53,926 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', 'ist', 'das', '<unk>', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', 'zeigen', '.', '</s>']
2024-05-02 06:46:53,926 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 06:46:53,926 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 06:46:53,926 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> ist das <unk> Problem, weil es nicht die <unk> des <unk> zeigen.
2024-05-02 06:46:53,926 - INFO - joeynmt.training - Example #2
2024-05-02 06:46:53,926 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 06:46:53,926 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 06:46:53,926 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', 'einem', 'Sinn', ',', 'das', '<unk>', 'Herz', 'des', 'globalen', '<unk>', '.', '</s>']
2024-05-02 06:46:53,927 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 06:46:53,927 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 06:46:53,927 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in einem Sinn, das <unk> Herz des globalen <unk>.
2024-05-02 06:46:53,927 - INFO - joeynmt.training - Example #3
2024-05-02 06:46:53,927 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 06:46:53,927 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 06:46:53,927 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'im', 'Sommer', '.', '</s>']
2024-05-02 06:46:53,927 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 06:46:53,927 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 06:46:53,928 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> im Sommer.
2024-05-02 06:46:53,928 - INFO - joeynmt.training - Example #4
2024-05-02 06:46:53,928 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 06:46:53,928 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 06:46:53,928 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', ',', 'dass', 'ich', 'die', 'letzten', '25', 'Jahre', 'lang', 'eine', '<unk>', '<unk>', 'werden', '.', '</s>']
2024-05-02 06:46:53,928 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 06:46:53,928 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 06:46:53,928 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde, dass ich die letzten 25 Jahre lang eine <unk> <unk> werden.
2024-05-02 06:47:20,303 - INFO - joeynmt.training - Epoch   8, Step:    21100, Batch Loss:     1.197797, Batch Acc: 0.613780, Tokens per Sec:     2593, Lr: 0.000300
2024-05-02 06:47:46,802 - INFO - joeynmt.training - Epoch   8, Step:    21200, Batch Loss:     1.245264, Batch Acc: 0.615865, Tokens per Sec:     2565, Lr: 0.000300
2024-05-02 06:48:13,004 - INFO - joeynmt.training - Epoch   8, Step:    21300, Batch Loss:     1.218350, Batch Acc: 0.609948, Tokens per Sec:     2576, Lr: 0.000300
2024-05-02 06:48:39,514 - INFO - joeynmt.training - Epoch   8, Step:    21400, Batch Loss:     1.208171, Batch Acc: 0.611227, Tokens per Sec:     2552, Lr: 0.000300
2024-05-02 06:49:05,731 - INFO - joeynmt.training - Epoch   8, Step:    21500, Batch Loss:     1.272621, Batch Acc: 0.607822, Tokens per Sec:     2668, Lr: 0.000300
2024-05-02 06:49:05,731 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 06:49:05,731 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 06:49:38,646 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.98, acc:   0.56, generation: 32.8658[sec], evaluation: 0.0000[sec]
2024-05-02 06:49:38,850 - INFO - joeynmt.helpers - delete models/words/19000.ckpt
2024-05-02 06:49:38,853 - INFO - joeynmt.training - Example #0
2024-05-02 06:49:38,853 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 06:49:38,853 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 06:49:38,853 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', ',', 'das', 'ich', 'ihnen', 'gezeigt', 'gezeigt', 'habe', ',', 'dass', 'das', '<unk>', 'Eis', '<unk>', ',', 'das', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'der', 'Größe', 'der', '<unk>', '<unk>', '<unk>', ',', 'hat', 'von', '40', 'Prozent', '<unk>', '.', '</s>']
2024-05-02 06:49:38,853 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 06:49:38,853 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 06:49:38,853 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr, das ich ihnen gezeigt gezeigt habe, dass das <unk> Eis <unk>, das für die meisten drei Millionen Jahre der Größe der <unk> <unk> <unk>, hat von 40 Prozent <unk>.
2024-05-02 06:49:38,854 - INFO - joeynmt.training - Example #1
2024-05-02 06:49:38,854 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 06:49:38,854 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 06:49:38,854 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'des', '<unk>', '<unk>', '<unk>', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', 'nicht', 'zeigen', '.', '</s>']
2024-05-02 06:49:38,854 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 06:49:38,854 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 06:49:38,854 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> des <unk> <unk> <unk>, weil es nicht die <unk> des <unk> nicht zeigen.
2024-05-02 06:49:38,854 - INFO - joeynmt.training - Example #2
2024-05-02 06:49:38,854 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 06:49:38,854 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 06:49:38,854 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', '<unk>', 'ist', 'in', 'einem', 'Sinne', 'des', 'globalen', '<unk>', '.', '</s>']
2024-05-02 06:49:38,855 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 06:49:38,855 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 06:49:38,855 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis <unk> ist in einem Sinne des globalen <unk>.
2024-05-02 06:49:38,855 - INFO - joeynmt.training - Example #3
2024-05-02 06:49:38,855 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 06:49:38,855 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 06:49:38,855 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '.', '</s>']
2024-05-02 06:49:38,855 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 06:49:38,855 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 06:49:38,855 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk>.
2024-05-02 06:49:38,856 - INFO - joeynmt.training - Example #4
2024-05-02 06:49:38,856 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 06:49:38,856 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 06:49:38,856 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'dass', 'das', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 06:49:38,856 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 06:49:38,856 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 06:49:38,856 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, dass das, was in den letzten 25 Jahren passiert ist.
2024-05-02 06:50:05,264 - INFO - joeynmt.training - Epoch   8, Step:    21600, Batch Loss:     1.296433, Batch Acc: 0.610756, Tokens per Sec:     2544, Lr: 0.000300
2024-05-02 06:50:31,507 - INFO - joeynmt.training - Epoch   8, Step:    21700, Batch Loss:     1.080361, Batch Acc: 0.608682, Tokens per Sec:     2693, Lr: 0.000300
2024-05-02 06:50:58,038 - INFO - joeynmt.training - Epoch   8, Step:    21800, Batch Loss:     1.341254, Batch Acc: 0.607656, Tokens per Sec:     2534, Lr: 0.000300
2024-05-02 06:51:23,817 - INFO - joeynmt.training - Epoch   8, Step:    21900, Batch Loss:     1.203007, Batch Acc: 0.603311, Tokens per Sec:     2657, Lr: 0.000300
2024-05-02 06:51:50,329 - INFO - joeynmt.training - Epoch   8, Step:    22000, Batch Loss:     1.097153, Batch Acc: 0.606252, Tokens per Sec:     2494, Lr: 0.000300
2024-05-02 06:51:50,329 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 06:51:50,329 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 06:52:24,606 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.99, acc:   0.56, generation: 34.2272[sec], evaluation: 0.0000[sec]
2024-05-02 06:52:24,810 - INFO - joeynmt.helpers - delete models/words/20000.ckpt
2024-05-02 06:52:24,812 - INFO - joeynmt.training - Example #0
2024-05-02 06:52:24,813 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 06:52:24,813 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 06:52:24,813 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', ',', 'ich', 'habe', 'diese', 'zwei', '<unk>', 'so', '<unk>', ',', 'dass', 'das', '<unk>', 'Eis', '<unk>', ',', 'das', 'für', 'die', 'meisten', 'der', '<unk>', 'drei', 'Millionen', 'Jahre', 'war', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-02 06:52:24,813 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 06:52:24,813 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 06:52:24,813 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr, ich habe diese zwei <unk> so <unk>, dass das <unk> Eis <unk>, das für die meisten der <unk> drei Millionen Jahre war die Größe der <unk> <unk> <unk> <unk> <unk>.
2024-05-02 06:52:24,813 - INFO - joeynmt.training - Example #1
2024-05-02 06:52:24,813 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 06:52:24,813 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 06:52:24,813 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'des', '<unk>', '<unk>', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', 'nicht', '<unk>', '.', '</s>']
2024-05-02 06:52:24,814 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 06:52:24,814 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 06:52:24,814 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> des <unk> <unk>, weil es nicht die <unk> des <unk> nicht <unk>.
2024-05-02 06:52:24,814 - INFO - joeynmt.training - Example #2
2024-05-02 06:52:24,814 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 06:52:24,814 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 06:52:24,814 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', '<unk>', 'ist', ',', 'in', 'einem', 'Sinne', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 06:52:24,814 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 06:52:24,814 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 06:52:24,814 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis <unk> ist, in einem Sinne des <unk> <unk>.
2024-05-02 06:52:24,815 - INFO - joeynmt.training - Example #3
2024-05-02 06:52:24,815 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 06:52:24,815 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 06:52:24,815 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '.', '</s>']
2024-05-02 06:52:24,815 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 06:52:24,815 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 06:52:24,815 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk>.
2024-05-02 06:52:24,815 - INFO - joeynmt.training - Example #4
2024-05-02 06:52:24,815 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 06:52:24,815 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 06:52:24,815 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'dass', 'ich', 'Ihnen', 'eine', '<unk>', '<unk>', '<unk>', 'sein', 'wird', '.', '</s>']
2024-05-02 06:52:24,816 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 06:52:24,816 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 06:52:24,816 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, dass ich Ihnen eine <unk> <unk> <unk> sein wird.
2024-05-02 06:52:51,194 - INFO - joeynmt.training - Epoch   8, Step:    22100, Batch Loss:     1.152746, Batch Acc: 0.603043, Tokens per Sec:     2455, Lr: 0.000300
2024-05-02 06:53:17,179 - INFO - joeynmt.training - Epoch   8, Step:    22200, Batch Loss:     1.218647, Batch Acc: 0.605264, Tokens per Sec:     2646, Lr: 0.000300
2024-05-02 06:53:43,436 - INFO - joeynmt.training - Epoch   8, Step:    22300, Batch Loss:     1.212564, Batch Acc: 0.601089, Tokens per Sec:     2615, Lr: 0.000300
2024-05-02 06:54:10,408 - INFO - joeynmt.training - Epoch   8, Step:    22400, Batch Loss:     1.173571, Batch Acc: 0.605021, Tokens per Sec:     2469, Lr: 0.000300
2024-05-02 06:54:36,917 - INFO - joeynmt.training - Epoch   8, Step:    22500, Batch Loss:     1.217131, Batch Acc: 0.602033, Tokens per Sec:     2628, Lr: 0.000300
2024-05-02 06:54:36,918 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 06:54:36,918 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 06:55:09,925 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.00, acc:   0.56, generation: 32.9595[sec], evaluation: 0.0000[sec]
2024-05-02 06:55:09,927 - INFO - joeynmt.training - Example #0
2024-05-02 06:55:09,928 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 06:55:09,928 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 06:55:09,928 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', ',', 'dass', 'das', '<unk>', 'Eis', '<unk>', ',', 'das', 'für', 'die', 'meisten', '<unk>', '<unk>', ',', 'die', 'die', '<unk>', '<unk>', ',', 'die', 'von', '<unk>', '<unk>', '<unk>', ',', 'hat', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 06:55:09,928 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 06:55:09,929 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 06:55:09,929 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk>, dass das <unk> Eis <unk>, das für die meisten <unk> <unk>, die die <unk> <unk>, die von <unk> <unk> <unk>, hat von 40 Prozent.
2024-05-02 06:55:09,929 - INFO - joeynmt.training - Example #1
2024-05-02 06:55:09,929 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 06:55:09,929 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 06:55:09,929 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', '<unk>', 'dieses', '<unk>', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 06:55:09,929 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 06:55:09,929 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 06:55:09,929 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> <unk> dieses <unk> Problem, weil es nicht die <unk> des <unk> <unk>.
2024-05-02 06:55:09,929 - INFO - joeynmt.training - Example #2
2024-05-02 06:55:09,930 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 06:55:09,930 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 06:55:09,930 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', ',', 'in', 'einem', 'Sinne', ',', 'das', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 06:55:09,930 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 06:55:09,930 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 06:55:09,930 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist, in einem Sinne, das <unk> des <unk>.
2024-05-02 06:55:09,930 - INFO - joeynmt.training - Example #3
2024-05-02 06:55:09,930 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 06:55:09,930 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 06:55:09,930 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '.', '</s>']
2024-05-02 06:55:09,931 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 06:55:09,931 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 06:55:09,931 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk>.
2024-05-02 06:55:09,931 - INFO - joeynmt.training - Example #4
2024-05-02 06:55:09,931 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 06:55:09,931 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 06:55:09,931 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'dass', 'Sie', '<unk>', '<unk>', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 06:55:09,931 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 06:55:09,931 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 06:55:09,931 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, dass Sie <unk> <unk>, was in den letzten 25 Jahren passiert ist.
2024-05-02 06:55:37,315 - INFO - joeynmt.training - Epoch   8, Step:    22600, Batch Loss:     1.261942, Batch Acc: 0.608733, Tokens per Sec:     2513, Lr: 0.000300
2024-05-02 06:56:03,966 - INFO - joeynmt.training - Epoch   8, Step:    22700, Batch Loss:     1.250442, Batch Acc: 0.603316, Tokens per Sec:     2546, Lr: 0.000300
2024-05-02 06:56:30,495 - INFO - joeynmt.training - Epoch   8, Step:    22800, Batch Loss:     1.112925, Batch Acc: 0.605455, Tokens per Sec:     2569, Lr: 0.000300
2024-05-02 06:56:57,287 - INFO - joeynmt.training - Epoch   8, Step:    22900, Batch Loss:     1.259395, Batch Acc: 0.602033, Tokens per Sec:     2519, Lr: 0.000300
2024-05-02 06:57:23,634 - INFO - joeynmt.training - Epoch   8, Step:    23000, Batch Loss:     1.193891, Batch Acc: 0.599566, Tokens per Sec:     2608, Lr: 0.000300
2024-05-02 06:57:23,635 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 06:57:23,635 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 06:57:59,410 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.97, acc:   0.56, generation: 35.7270[sec], evaluation: 0.0000[sec]
2024-05-02 06:57:59,607 - INFO - joeynmt.helpers - delete models/words/22000.ckpt
2024-05-02 06:57:59,611 - INFO - joeynmt.helpers - delete /Users/Iris/test/mt-exercise-5/models/words/22000.ckpt
2024-05-02 06:57:59,611 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /Users/Iris/test/mt-exercise-5/models/words/22000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/Users/Iris/test/mt-exercise-5/models/words/22000.ckpt')
2024-05-02 06:57:59,611 - INFO - joeynmt.training - Example #0
2024-05-02 06:57:59,611 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 06:57:59,611 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 06:57:59,611 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'beiden', '<unk>', '<unk>', ',', 'die', 'die', '<unk>', '<unk>', ',', 'die', 'die', '<unk>', 'der', 'letzten', 'drei', 'Millionen', 'Jahren', '<unk>', '<unk>', 'hat', ',', 'hat', 'sich', 'von', '40', 'Prozent', '<unk>', '<unk>', '.', '</s>']
2024-05-02 06:57:59,612 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 06:57:59,612 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 06:57:59,612 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese beiden <unk> <unk>, die die <unk> <unk>, die die <unk> der letzten drei Millionen Jahren <unk> <unk> hat, hat sich von 40 Prozent <unk> <unk>.
2024-05-02 06:57:59,612 - INFO - joeynmt.training - Example #1
2024-05-02 06:57:59,612 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 06:57:59,612 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 06:57:59,612 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', '<unk>', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 06:57:59,612 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 06:57:59,612 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 06:57:59,613 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses <unk> Problem, weil es nicht die <unk> des <unk>.
2024-05-02 06:57:59,613 - INFO - joeynmt.training - Example #2
2024-05-02 06:57:59,613 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 06:57:59,613 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 06:57:59,613 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', '<unk>', 'ist', ',', 'in', 'einem', 'Sinne', 'des', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 06:57:59,613 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 06:57:59,613 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 06:57:59,613 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis <unk> ist, in einem Sinne des <unk> des <unk>.
2024-05-02 06:57:59,613 - INFO - joeynmt.training - Example #3
2024-05-02 06:57:59,613 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 06:57:59,613 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 06:57:59,613 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '<unk>', '.', '</s>']
2024-05-02 06:57:59,614 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 06:57:59,614 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 06:57:59,614 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> <unk>.
2024-05-02 06:57:59,614 - INFO - joeynmt.training - Example #4
2024-05-02 06:57:59,614 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 06:57:59,614 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 06:57:59,614 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'zeige', ',', 'werden', 'Sie', 'ein', '<unk>', '<unk>', 'sein', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 06:57:59,614 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 06:57:59,615 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 06:57:59,615 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich zeige, werden Sie ein <unk> <unk> sein, was in den letzten 25 Jahren Jahren passiert ist.
2024-05-02 06:58:26,850 - INFO - joeynmt.training - Epoch   8, Step:    23100, Batch Loss:     1.351262, Batch Acc: 0.605008, Tokens per Sec:     2432, Lr: 0.000300
2024-05-02 06:58:53,683 - INFO - joeynmt.training - Epoch   8, Step:    23200, Batch Loss:     1.347235, Batch Acc: 0.603002, Tokens per Sec:     2510, Lr: 0.000300
2024-05-02 06:59:20,502 - INFO - joeynmt.training - Epoch   8, Step:    23300, Batch Loss:     1.185573, Batch Acc: 0.599696, Tokens per Sec:     2523, Lr: 0.000300
2024-05-02 06:59:46,609 - INFO - joeynmt.training - Epoch   8, Step:    23400, Batch Loss:     1.174174, Batch Acc: 0.598383, Tokens per Sec:     2620, Lr: 0.000300
2024-05-02 07:00:12,676 - INFO - joeynmt.training - Epoch   8, Step:    23500, Batch Loss:     1.280432, Batch Acc: 0.598168, Tokens per Sec:     2647, Lr: 0.000300
2024-05-02 07:00:12,676 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 07:00:12,676 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 07:00:47,004 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.37, ppl:   3.93, acc:   0.56, generation: 34.2778[sec], evaluation: 0.0000[sec]
2024-05-02 07:00:47,005 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 07:00:47,210 - INFO - joeynmt.helpers - delete models/words/17500.ckpt
2024-05-02 07:00:47,213 - INFO - joeynmt.training - Example #0
2024-05-02 07:00:47,213 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 07:00:47,213 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 07:00:47,213 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', ',', 'ich', 'diese', 'beiden', '<unk>', ',', 'dass', 'das', '<unk>', 'Eis', '<unk>', ',', 'das', 'für', 'die', 'meisten', 'der', '<unk>', 'drei', 'Millionen', 'Jahre', '<unk>', 'hat', ',', 'hat', 'von', '40', 'Prozent', '<unk>', '<unk>', '.', '</s>']
2024-05-02 07:00:47,213 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 07:00:47,213 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 07:00:47,213 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr, ich diese beiden <unk>, dass das <unk> Eis <unk>, das für die meisten der <unk> drei Millionen Jahre <unk> hat, hat von 40 Prozent <unk> <unk>.
2024-05-02 07:00:47,213 - INFO - joeynmt.training - Example #1
2024-05-02 07:00:47,213 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 07:00:47,213 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 07:00:47,213 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', '<unk>', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 07:00:47,214 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 07:00:47,214 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 07:00:47,214 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses <unk> Problem, weil es nicht die <unk> des <unk> des <unk>.
2024-05-02 07:00:47,214 - INFO - joeynmt.training - Example #2
2024-05-02 07:00:47,214 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 07:00:47,214 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 07:00:47,214 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', ',', 'in', 'einem', 'Sinne', 'des', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 07:00:47,214 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 07:00:47,215 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 07:00:47,215 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist, in einem Sinne des <unk> des <unk> <unk>.
2024-05-02 07:00:47,215 - INFO - joeynmt.training - Example #3
2024-05-02 07:00:47,215 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 07:00:47,215 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 07:00:47,215 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '.', '</s>']
2024-05-02 07:00:47,215 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 07:00:47,215 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 07:00:47,215 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk>.
2024-05-02 07:00:47,215 - INFO - joeynmt.training - Example #4
2024-05-02 07:00:47,215 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 07:00:47,215 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 07:00:47,215 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'dass', 'ich', '<unk>', '<unk>', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 07:00:47,216 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 07:00:47,216 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 07:00:47,216 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, dass ich <unk> <unk> von dem, was in den letzten 25 Jahren passiert ist.
2024-05-02 07:01:09,662 - INFO - joeynmt.training - Epoch   8: total training loss 3589.78
2024-05-02 07:01:09,662 - INFO - joeynmt.training - EPOCH 9
2024-05-02 07:01:13,545 - INFO - joeynmt.training - Epoch   9, Step:    23600, Batch Loss:     1.137014, Batch Acc: 0.629612, Tokens per Sec:     2751, Lr: 0.000300
2024-05-02 07:01:39,910 - INFO - joeynmt.training - Epoch   9, Step:    23700, Batch Loss:     1.206650, Batch Acc: 0.628707, Tokens per Sec:     2603, Lr: 0.000300
2024-05-02 07:02:06,599 - INFO - joeynmt.training - Epoch   9, Step:    23800, Batch Loss:     1.207048, Batch Acc: 0.626985, Tokens per Sec:     2570, Lr: 0.000300
2024-05-02 07:02:33,118 - INFO - joeynmt.training - Epoch   9, Step:    23900, Batch Loss:     1.173034, Batch Acc: 0.622201, Tokens per Sec:     2551, Lr: 0.000300
2024-05-02 07:02:59,501 - INFO - joeynmt.training - Epoch   9, Step:    24000, Batch Loss:     1.253120, Batch Acc: 0.624334, Tokens per Sec:     2625, Lr: 0.000300
2024-05-02 07:02:59,501 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 07:02:59,501 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 07:03:34,502 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.98, acc:   0.56, generation: 34.9503[sec], evaluation: 0.0000[sec]
2024-05-02 07:03:34,708 - INFO - joeynmt.helpers - delete models/words/21000.ckpt
2024-05-02 07:03:34,711 - INFO - joeynmt.training - Example #0
2024-05-02 07:03:34,711 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 07:03:34,711 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 07:03:34,711 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', 'so', '<unk>', ',', 'dass', 'das', '<unk>', 'Eis', '<unk>', ',', 'das', 'für', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', '<unk>', 'hat', ',', 'hat', '<unk>', 'von', '40', 'Prozent', '<unk>', '<unk>', '.', '</s>']
2024-05-02 07:03:34,712 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 07:03:34,712 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 07:03:34,712 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> so <unk>, dass das <unk> Eis <unk>, das für die letzten drei Millionen Jahre lang <unk> hat, hat <unk> von 40 Prozent <unk> <unk>.
2024-05-02 07:03:34,712 - INFO - joeynmt.training - Example #1
2024-05-02 07:03:34,712 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 07:03:34,712 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 07:03:34,712 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', '<unk>', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 07:03:34,712 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 07:03:34,712 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 07:03:34,712 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> <unk> Problem, weil es nicht die <unk> des <unk> <unk>.
2024-05-02 07:03:34,712 - INFO - joeynmt.training - Example #2
2024-05-02 07:03:34,713 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 07:03:34,713 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 07:03:34,713 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', '<unk>', '<unk>', 'ist', ',', 'in', 'einem', 'Sinne', 'des', 'globalen', '<unk>', '.', '</s>']
2024-05-02 07:03:34,713 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 07:03:34,713 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 07:03:34,713 - INFO - joeynmt.training - 	Hypothesis: Die <unk> <unk> ist, in einem Sinne des globalen <unk>.
2024-05-02 07:03:34,713 - INFO - joeynmt.training - Example #3
2024-05-02 07:03:34,713 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 07:03:34,713 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 07:03:34,713 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '.', '</s>']
2024-05-02 07:03:34,714 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 07:03:34,714 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 07:03:34,714 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk>.
2024-05-02 07:03:34,714 - INFO - joeynmt.training - Example #4
2024-05-02 07:03:34,714 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 07:03:34,714 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 07:03:34,714 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'werden', '<unk>', '<unk>', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 07:03:34,714 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 07:03:34,714 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 07:03:34,714 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, werden <unk> <unk> von dem, was in den letzten 25 Jahren passiert ist.
2024-05-02 07:04:01,477 - INFO - joeynmt.training - Epoch   9, Step:    24100, Batch Loss:     1.118052, Batch Acc: 0.619920, Tokens per Sec:     2505, Lr: 0.000300
2024-05-02 07:04:27,493 - INFO - joeynmt.training - Epoch   9, Step:    24200, Batch Loss:     1.130477, Batch Acc: 0.619264, Tokens per Sec:     2537, Lr: 0.000300
2024-05-02 07:04:54,952 - INFO - joeynmt.training - Epoch   9, Step:    24300, Batch Loss:     1.299782, Batch Acc: 0.614059, Tokens per Sec:     2444, Lr: 0.000300
2024-05-02 07:05:21,802 - INFO - joeynmt.training - Epoch   9, Step:    24400, Batch Loss:     1.177984, Batch Acc: 0.615093, Tokens per Sec:     2505, Lr: 0.000300
2024-05-02 07:05:48,220 - INFO - joeynmt.training - Epoch   9, Step:    24500, Batch Loss:     1.134007, Batch Acc: 0.615162, Tokens per Sec:     2652, Lr: 0.000300
2024-05-02 07:05:48,220 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 07:05:48,220 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 07:06:18,633 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.98, acc:   0.56, generation: 30.3641[sec], evaluation: 0.0000[sec]
2024-05-02 07:06:18,838 - INFO - joeynmt.helpers - delete models/words/24000.ckpt
2024-05-02 07:06:18,840 - INFO - joeynmt.helpers - delete /Users/Iris/test/mt-exercise-5/models/words/24000.ckpt
2024-05-02 07:06:18,840 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /Users/Iris/test/mt-exercise-5/models/words/24000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/Users/Iris/test/mt-exercise-5/models/words/24000.ckpt')
2024-05-02 07:06:18,841 - INFO - joeynmt.training - Example #0
2024-05-02 07:06:18,841 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 07:06:18,841 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 07:06:18,841 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', ',', 'so', 'dass', 'ich', '<unk>', '<unk>', ',', 'dass', 'das', '<unk>', '<unk>', '<unk>', ',', 'die', 'für', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', 'hat', '.', '</s>']
2024-05-02 07:06:18,841 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 07:06:18,841 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 07:06:18,841 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk>, so dass ich <unk> <unk>, dass das <unk> <unk> <unk>, die für die letzten drei Millionen Jahre <unk> hat.
2024-05-02 07:06:18,841 - INFO - joeynmt.training - Example #1
2024-05-02 07:06:18,842 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 07:06:18,842 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 07:06:18,842 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', 'die', '<unk>', 'dieses', '<unk>', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 07:06:18,842 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 07:06:18,842 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 07:06:18,842 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> die <unk> dieses <unk> Problem, weil es nicht die <unk> des <unk> <unk>.
2024-05-02 07:06:18,842 - INFO - joeynmt.training - Example #2
2024-05-02 07:06:18,842 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 07:06:18,842 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 07:06:18,842 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', ',', 'in', 'einem', 'Sinne', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 07:06:18,843 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 07:06:18,843 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 07:06:18,843 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist, in einem Sinne des <unk> <unk>.
2024-05-02 07:06:18,843 - INFO - joeynmt.training - Example #3
2024-05-02 07:06:18,843 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 07:06:18,843 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 07:06:18,843 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '.', '</s>']
2024-05-02 07:06:18,843 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 07:06:18,843 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 07:06:18,843 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk>.
2024-05-02 07:06:18,844 - INFO - joeynmt.training - Example #4
2024-05-02 07:06:18,844 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 07:06:18,844 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 07:06:18,844 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'dass', 'ich', 'Ihnen', 'die', 'letzten', '25', 'Jahre', 'alt', 'ist', '.', '</s>']
2024-05-02 07:06:18,844 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 07:06:18,844 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 07:06:18,844 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, dass ich Ihnen die letzten 25 Jahre alt ist.
2024-05-02 07:06:46,036 - INFO - joeynmt.training - Epoch   9, Step:    24600, Batch Loss:     1.120324, Batch Acc: 0.616675, Tokens per Sec:     2448, Lr: 0.000300
2024-05-02 07:07:12,136 - INFO - joeynmt.training - Epoch   9, Step:    24700, Batch Loss:     1.146813, Batch Acc: 0.613576, Tokens per Sec:     2601, Lr: 0.000300
2024-05-02 07:07:38,471 - INFO - joeynmt.training - Epoch   9, Step:    24800, Batch Loss:     1.072186, Batch Acc: 0.616839, Tokens per Sec:     2591, Lr: 0.000300
2024-05-02 07:08:05,140 - INFO - joeynmt.training - Epoch   9, Step:    24900, Batch Loss:     1.142527, Batch Acc: 0.612243, Tokens per Sec:     2539, Lr: 0.000300
2024-05-02 07:08:32,103 - INFO - joeynmt.training - Epoch   9, Step:    25000, Batch Loss:     1.226321, Batch Acc: 0.612061, Tokens per Sec:     2530, Lr: 0.000300
2024-05-02 07:08:32,103 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 07:08:32,103 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 07:09:03,676 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.98, acc:   0.56, generation: 31.5225[sec], evaluation: 0.0000[sec]
2024-05-02 07:09:03,678 - INFO - joeynmt.training - Example #0
2024-05-02 07:09:03,679 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 07:09:03,679 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 07:09:03,679 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'beiden', '<unk>', '<unk>', ',', 'die', '<unk>', '<unk>', ',', 'die', 'die', '<unk>', '<unk>', ',', 'die', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '<unk>', 'hat', ',', 'hat', 'von', '40', 'Prozent', '<unk>', '.', '</s>']
2024-05-02 07:09:03,679 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 07:09:03,679 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 07:09:03,679 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese beiden <unk> <unk>, die <unk> <unk>, die die <unk> <unk>, die die meisten drei Millionen Jahre die Größe der <unk> <unk> <unk> <unk> hat, hat von 40 Prozent <unk>.
2024-05-02 07:09:03,680 - INFO - joeynmt.training - Example #1
2024-05-02 07:09:03,680 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 07:09:03,680 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 07:09:03,680 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', '<unk>', 'dieses', '<unk>', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', 'nicht', '<unk>', '.', '</s>']
2024-05-02 07:09:03,680 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 07:09:03,680 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 07:09:03,680 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> <unk> dieses <unk> Problem, weil es nicht die <unk> des <unk> nicht <unk>.
2024-05-02 07:09:03,680 - INFO - joeynmt.training - Example #2
2024-05-02 07:09:03,680 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 07:09:03,680 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 07:09:03,680 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', ',', 'in', 'einem', 'Sinne', ',', 'der', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 07:09:03,681 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 07:09:03,681 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 07:09:03,681 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist, in einem Sinne, der <unk> des <unk> <unk>.
2024-05-02 07:09:03,681 - INFO - joeynmt.training - Example #3
2024-05-02 07:09:03,681 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 07:09:03,681 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 07:09:03,681 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '.', '</s>']
2024-05-02 07:09:03,682 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 07:09:03,682 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 07:09:03,682 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk>.
2024-05-02 07:09:03,682 - INFO - joeynmt.training - Example #4
2024-05-02 07:09:03,682 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 07:09:03,682 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 07:09:03,682 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'wird', '<unk>', '<unk>', 'davon', 'sein', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 07:09:03,682 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 07:09:03,682 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 07:09:03,682 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, wird <unk> <unk> davon sein, was in den letzten 25 Jahren passiert ist.
2024-05-02 07:09:29,881 - INFO - joeynmt.training - Epoch   9, Step:    25100, Batch Loss:     1.156136, Batch Acc: 0.613762, Tokens per Sec:     2597, Lr: 0.000300
2024-05-02 07:09:56,152 - INFO - joeynmt.training - Epoch   9, Step:    25200, Batch Loss:     1.182964, Batch Acc: 0.614737, Tokens per Sec:     2641, Lr: 0.000300
2024-05-02 07:10:22,387 - INFO - joeynmt.training - Epoch   9, Step:    25300, Batch Loss:     1.220603, Batch Acc: 0.615187, Tokens per Sec:     2657, Lr: 0.000300
2024-05-02 07:10:48,779 - INFO - joeynmt.training - Epoch   9, Step:    25400, Batch Loss:     1.227197, Batch Acc: 0.613782, Tokens per Sec:     2531, Lr: 0.000300
2024-05-02 07:11:15,594 - INFO - joeynmt.training - Epoch   9, Step:    25500, Batch Loss:     1.117034, Batch Acc: 0.611797, Tokens per Sec:     2470, Lr: 0.000300
2024-05-02 07:11:15,594 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 07:11:15,594 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 07:11:54,001 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.98, acc:   0.56, generation: 38.3573[sec], evaluation: 0.0000[sec]
2024-05-02 07:11:54,203 - INFO - joeynmt.helpers - delete models/words/24500.ckpt
2024-05-02 07:11:54,205 - INFO - joeynmt.helpers - delete /Users/Iris/test/mt-exercise-5/models/words/24500.ckpt
2024-05-02 07:11:54,205 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /Users/Iris/test/mt-exercise-5/models/words/24500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/Users/Iris/test/mt-exercise-5/models/words/24500.ckpt')
2024-05-02 07:11:54,206 - INFO - joeynmt.training - Example #0
2024-05-02 07:11:54,206 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 07:11:54,206 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 07:11:54,206 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', ',', 'ich', 'habe', 'diese', 'zwei', '<unk>', '<unk>', ',', 'dass', 'das', '<unk>', '<unk>', ',', 'das', 'das', '<unk>', '<unk>', ',', 'das', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', ',', 'hat', 'sich', 'von', '40', 'Prozent', '<unk>', '.', '</s>']
2024-05-02 07:11:54,206 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 07:11:54,206 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 07:11:54,206 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr, ich habe diese zwei <unk> <unk>, dass das <unk> <unk>, das das <unk> <unk>, das für die meisten letzten drei Millionen Jahre <unk>, hat sich von 40 Prozent <unk>.
2024-05-02 07:11:54,206 - INFO - joeynmt.training - Example #1
2024-05-02 07:11:54,206 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 07:11:54,207 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 07:11:54,207 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'dieses', '<unk>', 'Problem', ',', 'weil', 'es', 'den', '<unk>', 'nicht', 'zeigen', ',', 'das', 'Eis', 'des', '<unk>', '.', '</s>']
2024-05-02 07:11:54,207 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 07:11:54,207 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 07:11:54,207 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> dieses <unk> Problem, weil es den <unk> nicht zeigen, das Eis des <unk>.
2024-05-02 07:11:54,207 - INFO - joeynmt.training - Example #2
2024-05-02 07:11:54,207 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 07:11:54,207 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 07:11:54,207 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', ',', 'in', 'einem', 'Sinne', ',', 'dass', 'das', '<unk>', 'Herz', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 07:11:54,208 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 07:11:54,208 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 07:11:54,208 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist, in einem Sinne, dass das <unk> Herz des <unk> <unk>.
2024-05-02 07:11:54,208 - INFO - joeynmt.training - Example #3
2024-05-02 07:11:54,208 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 07:11:54,208 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 07:11:54,208 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '.', '</s>']
2024-05-02 07:11:54,208 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 07:11:54,208 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 07:11:54,208 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk>.
2024-05-02 07:11:54,208 - INFO - joeynmt.training - Example #4
2024-05-02 07:11:54,208 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 07:11:54,209 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 07:11:54,209 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'dass', 'ich', 'Ihnen', 'das', '<unk>', '<unk>', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 07:11:54,209 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 07:11:54,209 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 07:11:54,209 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, dass ich Ihnen das <unk> <unk>, was in den letzten 25 Jahren passiert ist.
2024-05-02 07:12:20,854 - INFO - joeynmt.training - Epoch   9, Step:    25600, Batch Loss:     1.186340, Batch Acc: 0.611673, Tokens per Sec:     2506, Lr: 0.000300
2024-05-02 07:12:47,393 - INFO - joeynmt.training - Epoch   9, Step:    25700, Batch Loss:     1.173769, Batch Acc: 0.611694, Tokens per Sec:     2600, Lr: 0.000300
2024-05-02 07:13:14,311 - INFO - joeynmt.training - Epoch   9, Step:    25800, Batch Loss:     1.251256, Batch Acc: 0.609318, Tokens per Sec:     2534, Lr: 0.000300
2024-05-02 07:13:40,694 - INFO - joeynmt.training - Epoch   9, Step:    25900, Batch Loss:     1.097583, Batch Acc: 0.607498, Tokens per Sec:     2514, Lr: 0.000300
2024-05-02 07:14:08,295 - INFO - joeynmt.training - Epoch   9, Step:    26000, Batch Loss:     1.243598, Batch Acc: 0.608453, Tokens per Sec:     2592, Lr: 0.000300
2024-05-02 07:14:08,296 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 07:14:08,296 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 07:14:45,323 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.37, ppl:   3.92, acc:   0.56, generation: 36.9771[sec], evaluation: 0.0000[sec]
2024-05-02 07:14:45,325 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 07:14:45,520 - INFO - joeynmt.helpers - delete models/words/25500.ckpt
2024-05-02 07:14:45,523 - INFO - joeynmt.training - Example #0
2024-05-02 07:14:45,523 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 07:14:45,523 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 07:14:45,523 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'beiden', '<unk>', '<unk>', ',', 'dass', 'das', '<unk>', 'Eis', '<unk>', ',', 'das', 'für', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größe', '<unk>', '<unk>', '<unk>', ',', 'hat', '<unk>', 'von', '40', '%', '<unk>', '.', '</s>']
2024-05-02 07:14:45,523 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 07:14:45,523 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 07:14:45,524 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese beiden <unk> <unk>, dass das <unk> Eis <unk>, das für die meisten der letzten drei Millionen Jahre die Größe <unk> <unk> <unk>, hat <unk> von 40% <unk>.
2024-05-02 07:14:45,524 - INFO - joeynmt.training - Example #1
2024-05-02 07:14:45,524 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 07:14:45,524 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 07:14:45,524 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'des', '<unk>', '<unk>', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 07:14:45,524 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 07:14:45,524 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 07:14:45,524 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> des <unk> <unk>, weil es nicht die <unk> des <unk>.
2024-05-02 07:14:45,524 - INFO - joeynmt.training - Example #2
2024-05-02 07:14:45,524 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 07:14:45,524 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 07:14:45,524 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', ',', 'in', 'einem', 'Sinn', ',', 'der', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 07:14:45,525 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 07:14:45,525 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 07:14:45,525 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist, in einem Sinn, der <unk> des <unk> <unk>.
2024-05-02 07:14:45,525 - INFO - joeynmt.training - Example #3
2024-05-02 07:14:45,525 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 07:14:45,525 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 07:14:45,525 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'im', 'Sommer', '.', '</s>']
2024-05-02 07:14:45,525 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 07:14:45,526 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 07:14:45,526 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> im Sommer.
2024-05-02 07:14:45,526 - INFO - joeynmt.training - Example #4
2024-05-02 07:14:45,526 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 07:14:45,526 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 07:14:45,526 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', '<unk>', '<unk>', 'dessen', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 07:14:45,526 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 07:14:45,526 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 07:14:45,526 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige <unk> <unk> dessen, was in den letzten 25 Jahren passiert ist.
2024-05-02 07:15:12,271 - INFO - joeynmt.training - Epoch   9, Step:    26100, Batch Loss:     1.244178, Batch Acc: 0.608478, Tokens per Sec:     2489, Lr: 0.000300
2024-05-02 07:15:38,550 - INFO - joeynmt.training - Epoch   9, Step:    26200, Batch Loss:     1.113024, Batch Acc: 0.609461, Tokens per Sec:     2544, Lr: 0.000300
2024-05-02 07:16:05,700 - INFO - joeynmt.training - Epoch   9, Step:    26300, Batch Loss:     1.200106, Batch Acc: 0.608610, Tokens per Sec:     2462, Lr: 0.000300
2024-05-02 07:16:32,443 - INFO - joeynmt.training - Epoch   9, Step:    26400, Batch Loss:     1.171657, Batch Acc: 0.605819, Tokens per Sec:     2523, Lr: 0.000300
2024-05-02 07:16:58,940 - INFO - joeynmt.training - Epoch   9, Step:    26500, Batch Loss:     1.168586, Batch Acc: 0.606795, Tokens per Sec:     2576, Lr: 0.000300
2024-05-02 07:16:58,940 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 07:16:58,941 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 07:17:32,022 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.37, ppl:   3.94, acc:   0.56, generation: 33.0324[sec], evaluation: 0.0000[sec]
2024-05-02 07:17:32,226 - INFO - joeynmt.helpers - delete models/words/21500.ckpt
2024-05-02 07:17:32,228 - INFO - joeynmt.training - Example #0
2024-05-02 07:17:32,228 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 07:17:32,228 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 07:17:32,229 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', ',', 'ich', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', ',', 'dass', 'das', '<unk>', '<unk>', ',', 'das', 'für', 'die', 'meisten', 'der', '<unk>', 'drei', 'Millionen', 'Jahre', 'die', 'Größe', 'der', '<unk>', '<unk>', 'hat', ',', 'hat', '<unk>', 'von', '40', 'Prozent', '<unk>', '.', '</s>']
2024-05-02 07:17:32,229 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 07:17:32,229 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 07:17:32,229 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr, ich habe ich diese zwei <unk> <unk>, dass das <unk> <unk>, das für die meisten der <unk> drei Millionen Jahre die Größe der <unk> <unk> hat, hat <unk> von 40 Prozent <unk>.
2024-05-02 07:17:32,229 - INFO - joeynmt.training - Example #1
2024-05-02 07:17:32,229 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 07:17:32,229 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 07:17:32,229 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'dieses', '<unk>', '<unk>', ',', 'weil', 'es', 'den', '<unk>', 'nicht', 'das', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 07:17:32,230 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 07:17:32,230 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 07:17:32,230 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> dieses <unk> <unk>, weil es den <unk> nicht das <unk> des <unk>.
2024-05-02 07:17:32,230 - INFO - joeynmt.training - Example #2
2024-05-02 07:17:32,230 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 07:17:32,230 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 07:17:32,230 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', ',', 'in', 'einem', 'Sinne', ',', 'das', '<unk>', 'Herz', 'des', 'globalen', '<unk>', '.', '</s>']
2024-05-02 07:17:32,230 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 07:17:32,230 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 07:17:32,230 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist, in einem Sinne, das <unk> Herz des globalen <unk>.
2024-05-02 07:17:32,231 - INFO - joeynmt.training - Example #3
2024-05-02 07:17:32,231 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 07:17:32,231 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 07:17:32,231 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '<unk>', '.', '</s>']
2024-05-02 07:17:32,231 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 07:17:32,231 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 07:17:32,231 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> <unk>.
2024-05-02 07:17:32,231 - INFO - joeynmt.training - Example #4
2024-05-02 07:17:32,231 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 07:17:32,231 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 07:17:32,231 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'dass', 'ich', '<unk>', '<unk>', '<unk>', 'dessen', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 07:17:32,232 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 07:17:32,232 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 07:17:32,232 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, dass ich <unk> <unk> <unk> dessen, was in den letzten 25 Jahren passiert ist.
2024-05-02 07:17:41,576 - INFO - joeynmt.training - Epoch   9: total training loss 3522.57
2024-05-02 07:17:41,576 - INFO - joeynmt.training - EPOCH 10
2024-05-02 07:17:58,522 - INFO - joeynmt.training - Epoch  10, Step:    26600, Batch Loss:     1.191950, Batch Acc: 0.631482, Tokens per Sec:     2565, Lr: 0.000300
2024-05-02 07:18:25,922 - INFO - joeynmt.training - Epoch  10, Step:    26700, Batch Loss:     1.091779, Batch Acc: 0.632858, Tokens per Sec:     2446, Lr: 0.000300
2024-05-02 07:18:52,198 - INFO - joeynmt.training - Epoch  10, Step:    26800, Batch Loss:     1.112922, Batch Acc: 0.631517, Tokens per Sec:     2637, Lr: 0.000300
2024-05-02 07:19:19,105 - INFO - joeynmt.training - Epoch  10, Step:    26900, Batch Loss:     1.190212, Batch Acc: 0.629885, Tokens per Sec:     2512, Lr: 0.000300
2024-05-02 07:19:46,001 - INFO - joeynmt.training - Epoch  10, Step:    27000, Batch Loss:     1.093418, Batch Acc: 0.634102, Tokens per Sec:     2534, Lr: 0.000300
2024-05-02 07:19:46,001 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 07:19:46,001 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 07:20:16,817 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.00, acc:   0.56, generation: 30.7647[sec], evaluation: 0.0000[sec]
2024-05-02 07:20:16,819 - INFO - joeynmt.training - Example #0
2024-05-02 07:20:16,820 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 07:20:16,820 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 07:20:16,820 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'beiden', '<unk>', '<unk>', ',', 'dass', 'das', '<unk>', 'Eis', '<unk>', ',', 'das', 'für', 'die', 'meisten', 'der', '<unk>', 'drei', 'Millionen', 'Jahre', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', 'hat', ',', 'hat', 'von', '40', 'Prozent', '<unk>', '.', '</s>']
2024-05-02 07:20:16,820 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 07:20:16,820 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 07:20:16,821 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese beiden <unk> <unk>, dass das <unk> Eis <unk>, das für die meisten der <unk> drei Millionen Jahre die Größe der <unk> <unk> <unk> hat, hat von 40 Prozent <unk>.
2024-05-02 07:20:16,821 - INFO - joeynmt.training - Example #1
2024-05-02 07:20:16,821 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 07:20:16,821 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 07:20:16,821 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'dieses', '<unk>', 'Problem', ',', 'weil', 'es', 'das', '<unk>', 'nicht', 'das', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 07:20:16,821 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 07:20:16,821 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 07:20:16,821 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> dieses <unk> Problem, weil es das <unk> nicht das <unk> des <unk>.
2024-05-02 07:20:16,821 - INFO - joeynmt.training - Example #2
2024-05-02 07:20:16,821 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 07:20:16,822 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 07:20:16,822 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', ',', 'in', 'einem', 'Sinne', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 07:20:16,822 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 07:20:16,822 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 07:20:16,822 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist, in einem Sinne des <unk> <unk>.
2024-05-02 07:20:16,822 - INFO - joeynmt.training - Example #3
2024-05-02 07:20:16,822 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 07:20:16,822 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 07:20:16,822 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '.', '</s>']
2024-05-02 07:20:16,823 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 07:20:16,823 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 07:20:16,823 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk>.
2024-05-02 07:20:16,823 - INFO - joeynmt.training - Example #4
2024-05-02 07:20:16,823 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 07:20:16,823 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 07:20:16,823 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'dass', 'ich', 'Ihnen', 'die', 'letzten', '25', 'Jahre', 'lang', 'passiert', '.', '</s>']
2024-05-02 07:20:16,823 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 07:20:16,823 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 07:20:16,823 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, dass ich Ihnen die letzten 25 Jahre lang passiert.
2024-05-02 07:20:43,510 - INFO - joeynmt.training - Epoch  10, Step:    27100, Batch Loss:     1.235369, Batch Acc: 0.626292, Tokens per Sec:     2570, Lr: 0.000300
2024-05-02 07:21:09,667 - INFO - joeynmt.training - Epoch  10, Step:    27200, Batch Loss:     1.118337, Batch Acc: 0.627094, Tokens per Sec:     2585, Lr: 0.000300
2024-05-02 07:21:36,170 - INFO - joeynmt.training - Epoch  10, Step:    27300, Batch Loss:     1.238832, Batch Acc: 0.625104, Tokens per Sec:     2574, Lr: 0.000300
2024-05-02 07:22:02,464 - INFO - joeynmt.training - Epoch  10, Step:    27400, Batch Loss:     1.170981, Batch Acc: 0.627515, Tokens per Sec:     2566, Lr: 0.000300
2024-05-02 07:22:29,910 - INFO - joeynmt.training - Epoch  10, Step:    27500, Batch Loss:     1.101727, Batch Acc: 0.624239, Tokens per Sec:     2502, Lr: 0.000300
2024-05-02 07:22:29,910 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 07:22:29,911 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 07:23:06,344 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.97, acc:   0.56, generation: 36.3828[sec], evaluation: 0.0000[sec]
2024-05-02 07:23:06,346 - INFO - joeynmt.training - Example #0
2024-05-02 07:23:06,346 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 07:23:06,346 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 07:23:06,346 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', ',', 'die', 'das', '<unk>', 'Eis', '<unk>', ',', 'das', 'für', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', ',', 'hat', 'die', 'Größe', 'der', '<unk>', '<unk>', ',', 'hat', '<unk>', 'von', '40', 'Prozent', '<unk>', '.', '</s>']
2024-05-02 07:23:06,347 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 07:23:06,347 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 07:23:06,347 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk>, die das <unk> Eis <unk>, das für die letzten drei Millionen Jahre, hat die Größe der <unk> <unk>, hat <unk> von 40 Prozent <unk>.
2024-05-02 07:23:06,347 - INFO - joeynmt.training - Example #1
2024-05-02 07:23:06,347 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 07:23:06,347 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 07:23:06,347 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'des', '<unk>', '<unk>', ',', 'weil', 'es', 'den', '<unk>', 'nicht', 'das', 'Eis', 'zeigt', '.', '</s>']
2024-05-02 07:23:06,348 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 07:23:06,348 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 07:23:06,348 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> des <unk> <unk>, weil es den <unk> nicht das Eis zeigt.
2024-05-02 07:23:06,348 - INFO - joeynmt.training - Example #2
2024-05-02 07:23:06,348 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 07:23:06,348 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 07:23:06,348 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', ',', 'in', 'einem', 'Sinne', 'des', '<unk>', '<unk>', 'des', '<unk>', '.', '</s>']
2024-05-02 07:23:06,348 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 07:23:06,348 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 07:23:06,349 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist, in einem Sinne des <unk> <unk> des <unk>.
2024-05-02 07:23:06,349 - INFO - joeynmt.training - Example #3
2024-05-02 07:23:06,349 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 07:23:06,349 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 07:23:06,349 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'im', 'Sommer', '.', '</s>']
2024-05-02 07:23:06,349 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 07:23:06,349 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 07:23:06,349 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> im Sommer.
2024-05-02 07:23:06,349 - INFO - joeynmt.training - Example #4
2024-05-02 07:23:06,349 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 07:23:06,349 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 07:23:06,349 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'dass', 'ich', '<unk>', '<unk>', 'davon', 'sein', 'wird', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 07:23:06,350 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 07:23:06,350 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 07:23:06,350 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, dass ich <unk> <unk> davon sein wird, was in den letzten 25 Jahren passiert ist.
2024-05-02 07:23:33,578 - INFO - joeynmt.training - Epoch  10, Step:    27600, Batch Loss:     1.045226, Batch Acc: 0.621375, Tokens per Sec:     2487, Lr: 0.000300
2024-05-02 07:23:59,678 - INFO - joeynmt.training - Epoch  10, Step:    27700, Batch Loss:     1.085989, Batch Acc: 0.622761, Tokens per Sec:     2620, Lr: 0.000300
2024-05-02 07:24:26,242 - INFO - joeynmt.training - Epoch  10, Step:    27800, Batch Loss:     1.132305, Batch Acc: 0.619551, Tokens per Sec:     2522, Lr: 0.000300
2024-05-02 07:24:52,675 - INFO - joeynmt.training - Epoch  10, Step:    27900, Batch Loss:     1.195619, Batch Acc: 0.617560, Tokens per Sec:     2609, Lr: 0.000300
2024-05-02 07:25:19,556 - INFO - joeynmt.training - Epoch  10, Step:    28000, Batch Loss:     1.079620, Batch Acc: 0.622901, Tokens per Sec:     2527, Lr: 0.000300
2024-05-02 07:25:19,556 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 07:25:19,556 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 07:25:56,947 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.97, acc:   0.56, generation: 37.3418[sec], evaluation: 0.0000[sec]
2024-05-02 07:25:56,949 - INFO - joeynmt.training - Example #0
2024-05-02 07:25:56,949 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 07:25:56,949 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 07:25:56,949 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', 'so', '<unk>', ',', 'dass', 'das', '<unk>', '<unk>', ',', 'das', 'die', '<unk>', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', '.', '</s>']
2024-05-02 07:25:56,950 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 07:25:56,950 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 07:25:56,950 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> so <unk>, dass das <unk> <unk>, das die <unk> der letzten drei Millionen Jahre <unk>.
2024-05-02 07:25:56,950 - INFO - joeynmt.training - Example #1
2024-05-02 07:25:56,950 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 07:25:56,950 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 07:25:56,950 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'dieses', '<unk>', 'Problem', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', 'zeigen', '.', '</s>']
2024-05-02 07:25:56,950 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 07:25:56,951 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 07:25:56,951 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> dieses <unk> Problem, weil es nicht die <unk> des <unk> zeigen.
2024-05-02 07:25:56,951 - INFO - joeynmt.training - Example #2
2024-05-02 07:25:56,951 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 07:25:56,951 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 07:25:56,951 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', ',', 'in', 'einem', 'Sinn', ',', 'das', '<unk>', 'Herz', 'des', '<unk>', '.', '</s>']
2024-05-02 07:25:56,951 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 07:25:56,951 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 07:25:56,951 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist, in einem Sinn, das <unk> Herz des <unk>.
2024-05-02 07:25:56,951 - INFO - joeynmt.training - Example #3
2024-05-02 07:25:56,952 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 07:25:56,952 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 07:25:56,952 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '.', '</s>']
2024-05-02 07:25:56,952 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 07:25:56,952 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 07:25:56,952 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk>.
2024-05-02 07:25:56,952 - INFO - joeynmt.training - Example #4
2024-05-02 07:25:56,952 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 07:25:56,952 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 07:25:56,952 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'wird', 'eine', '<unk>', '<unk>', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 07:25:56,953 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 07:25:56,953 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 07:25:56,953 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, wird eine <unk> <unk> von dem, was in den letzten 25 Jahren passiert ist.
2024-05-02 07:26:23,390 - INFO - joeynmt.training - Epoch  10, Step:    28100, Batch Loss:     1.179481, Batch Acc: 0.621660, Tokens per Sec:     2609, Lr: 0.000300
2024-05-02 07:26:50,520 - INFO - joeynmt.training - Epoch  10, Step:    28200, Batch Loss:     1.191401, Batch Acc: 0.620408, Tokens per Sec:     2536, Lr: 0.000300
2024-05-02 07:27:16,780 - INFO - joeynmt.training - Epoch  10, Step:    28300, Batch Loss:     1.323966, Batch Acc: 0.617694, Tokens per Sec:     2553, Lr: 0.000300
2024-05-02 07:27:43,808 - INFO - joeynmt.training - Epoch  10, Step:    28400, Batch Loss:     1.014195, Batch Acc: 0.615506, Tokens per Sec:     2480, Lr: 0.000300
2024-05-02 07:28:10,757 - INFO - joeynmt.training - Epoch  10, Step:    28500, Batch Loss:     1.232589, Batch Acc: 0.618678, Tokens per Sec:     2536, Lr: 0.000300
2024-05-02 07:28:10,757 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 07:28:10,757 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 07:28:47,175 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.37, ppl:   3.93, acc:   0.56, generation: 36.3682[sec], evaluation: 0.0000[sec]
2024-05-02 07:28:47,381 - INFO - joeynmt.helpers - delete models/words/23000.ckpt
2024-05-02 07:28:47,384 - INFO - joeynmt.training - Example #0
2024-05-02 07:28:47,384 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 07:28:47,384 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 07:28:47,384 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', 'so', 'dass', '<unk>', 'Eis', '<unk>', ',', 'dass', 'das', '<unk>', 'Eis', '<unk>', ',', 'das', 'für', 'die', 'meisten', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 'hat', ',', 'hat', '<unk>', 'von', '40', 'Prozent', '<unk>', '.', '</s>']
2024-05-02 07:28:47,385 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 07:28:47,385 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 07:28:47,385 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> so dass <unk> Eis <unk>, dass das <unk> Eis <unk>, das für die meisten <unk> <unk> <unk> <unk> <unk> hat, hat <unk> von 40 Prozent <unk>.
2024-05-02 07:28:47,385 - INFO - joeynmt.training - Example #1
2024-05-02 07:28:47,385 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 07:28:47,385 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 07:28:47,385 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'des', '<unk>', '<unk>', '<unk>', ',', 'weil', 'es', 'nicht', 'das', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 07:28:47,385 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 07:28:47,385 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 07:28:47,386 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> des <unk> <unk> <unk>, weil es nicht das <unk> des <unk> <unk>.
2024-05-02 07:28:47,386 - INFO - joeynmt.training - Example #2
2024-05-02 07:28:47,386 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 07:28:47,386 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 07:28:47,386 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', 'einem', 'Sinn', ',', 'der', '<unk>', 'des', 'globalen', '<unk>', '.', '</s>']
2024-05-02 07:28:47,386 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 07:28:47,386 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 07:28:47,386 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in einem Sinn, der <unk> des globalen <unk>.
2024-05-02 07:28:47,386 - INFO - joeynmt.training - Example #3
2024-05-02 07:28:47,386 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 07:28:47,386 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 07:28:47,386 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '.', '</s>']
2024-05-02 07:28:47,387 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 07:28:47,387 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 07:28:47,387 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk>.
2024-05-02 07:28:47,387 - INFO - joeynmt.training - Example #4
2024-05-02 07:28:47,387 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 07:28:47,387 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 07:28:47,387 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'dass', 'das', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 07:28:47,387 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 07:28:47,388 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 07:28:47,388 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen, dass das, was in den letzten 25 Jahren passiert ist.
2024-05-02 07:29:14,575 - INFO - joeynmt.training - Epoch  10, Step:    28600, Batch Loss:     1.246997, Batch Acc: 0.616100, Tokens per Sec:     2480, Lr: 0.000300
2024-05-02 07:29:40,966 - INFO - joeynmt.training - Epoch  10, Step:    28700, Batch Loss:     1.202966, Batch Acc: 0.617431, Tokens per Sec:     2540, Lr: 0.000300
2024-05-02 07:30:07,772 - INFO - joeynmt.training - Epoch  10, Step:    28800, Batch Loss:     1.236534, Batch Acc: 0.620886, Tokens per Sec:     2522, Lr: 0.000300
2024-05-02 07:30:34,189 - INFO - joeynmt.training - Epoch  10, Step:    28900, Batch Loss:     1.061889, Batch Acc: 0.615206, Tokens per Sec:     2530, Lr: 0.000300
2024-05-02 07:31:01,104 - INFO - joeynmt.training - Epoch  10, Step:    29000, Batch Loss:     1.083232, Batch Acc: 0.617790, Tokens per Sec:     2567, Lr: 0.000300
2024-05-02 07:31:01,104 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 07:31:01,104 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 07:31:39,776 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.97, acc:   0.56, generation: 38.6182[sec], evaluation: 0.0000[sec]
2024-05-02 07:31:39,778 - INFO - joeynmt.training - Example #0
2024-05-02 07:31:39,779 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-02 07:31:39,779 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt', ',', 'um', 'zu', 'veranschaulichen', ',', 'dass', 'die', 'arktische', 'Eiskappe', ',', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'geschrumpft', 'ist', '.']
2024-05-02 07:31:39,779 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'beiden', '<unk>', '<unk>', ',', 'so', 'dass', 'das', '<unk>', 'Eis', '<unk>', ',', 'das', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größe', 'der', '<unk>', '<unk>', ',', 'hat', '<unk>', 'von', '40', 'Prozent', '<unk>', '.', '</s>']
2024-05-02 07:31:39,779 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-02 07:31:39,780 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt , um zu veranschaulichen , dass die arktische Eiskappe , die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte , um 40 Prozent geschrumpft ist .
2024-05-02 07:31:39,780 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese beiden <unk> <unk>, so dass das <unk> Eis <unk>, das für die meisten drei Millionen Jahre die Größe der <unk> <unk>, hat <unk> von 40 Prozent <unk>.
2024-05-02 07:31:39,780 - INFO - joeynmt.training - Example #1
2024-05-02 07:31:39,780 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-02 07:31:39,780 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus', ',', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt', '.']
2024-05-02 07:31:39,780 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'des', '<unk>', '<unk>', ',', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '.', '</s>']
2024-05-02 07:31:39,780 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-02 07:31:39,780 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus , da es nicht die Dicke des Eises zeigt .
2024-05-02 07:31:39,781 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> des <unk> <unk>, weil es nicht die <unk> des <unk> <unk>.
2024-05-02 07:31:39,781 - INFO - joeynmt.training - Example #2
2024-05-02 07:31:39,781 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-02 07:31:39,781 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems', '.']
2024-05-02 07:31:39,781 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', ',', 'das', '<unk>', 'Herz', 'des', '<unk>', 'des', 'globalen', '<unk>', '.', '</s>']
2024-05-02 07:31:39,781 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-02 07:31:39,781 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems .
2024-05-02 07:31:39,781 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist, das <unk> Herz des <unk> des globalen <unk>.
2024-05-02 07:31:39,781 - INFO - joeynmt.training - Example #3
2024-05-02 07:31:39,781 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-02 07:31:39,781 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer', '.']
2024-05-02 07:31:39,781 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'im', 'Sommer', '.', '</s>']
2024-05-02 07:31:39,782 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2024-05-02 07:31:39,782 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer .
2024-05-02 07:31:39,782 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> im Sommer.
2024-05-02 07:31:39,782 - INFO - joeynmt.training - Example #4
2024-05-02 07:31:39,782 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 07:31:39,782 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 07:31:39,782 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Folie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'eine', '<unk>', '<unk>', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 07:31:39,782 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-02 07:31:39,783 - INFO - joeynmt.training - 	Reference:  Die nächste Folie , die ich Ihnen zeige , ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist .
2024-05-02 07:31:39,783 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, eine <unk> <unk> von dem, was in den letzten 25 Jahren passiert ist.
2024-05-02 07:32:06,510 - INFO - joeynmt.training - Epoch  10, Step:    29100, Batch Loss:     1.245039, Batch Acc: 0.615371, Tokens per Sec:     2517, Lr: 0.000300
2024-05-02 07:32:33,639 - INFO - joeynmt.training - Epoch  10, Step:    29200, Batch Loss:     1.214340, Batch Acc: 0.618688, Tokens per Sec:     2508, Lr: 0.000300
2024-05-02 07:33:00,225 - INFO - joeynmt.training - Epoch  10, Step:    29300, Batch Loss:     1.109102, Batch Acc: 0.612717, Tokens per Sec:     2503, Lr: 0.000300
2024-05-02 07:33:27,426 - INFO - joeynmt.training - Epoch  10, Step:    29400, Batch Loss:     1.097556, Batch Acc: 0.612905, Tokens per Sec:     2549, Lr: 0.000300
2024-05-02 07:33:50,770 - INFO - joeynmt.training - Epoch  10: total training loss 3451.43
2024-05-02 07:33:50,770 - INFO - joeynmt.training - Training ended after  10 epochs.
2024-05-02 07:33:50,770 - INFO - joeynmt.training - Best validation result (greedy) at step    26000:   3.92 ppl.
2024-05-02 07:33:50,794 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-02 07:33:50,851 - INFO - joeynmt.model - Enc-dec model built.
2024-05-02 07:33:50,897 - INFO - joeynmt.helpers - Load model from /Users/Iris/test/mt-exercise-5/models/words/26000.ckpt.
2024-05-02 07:33:50,904 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	loss_function=None)
2024-05-02 07:33:50,905 - INFO - joeynmt.prediction - Decoding on dev set...
2024-05-02 07:33:50,905 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 07:33:50,905 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 07:34:37,284 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 46.3299[sec], evaluation: 0.0000[sec]
2024-05-02 07:34:37,286 - INFO - joeynmt.prediction - Translations saved to: /Users/Iris/test/mt-exercise-5/models/words/00026000.hyps.dev.
2024-05-02 07:34:37,286 - INFO - joeynmt.prediction - Decoding on test set...
2024-05-02 07:34:37,286 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 07:34:37,286 - INFO - joeynmt.prediction - Predicting 1568 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 07:35:42,742 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 65.3783[sec], evaluation: 0.0000[sec]
2024-05-02 07:35:42,744 - INFO - joeynmt.prediction - Translations saved to: /Users/Iris/test/mt-exercise-5/models/words/00026000.hyps.test.

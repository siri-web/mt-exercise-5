2024-05-02 07:35:46,933 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-02 07:35:46,933 - INFO - joeynmt.helpers -                           cfg.name : transformer_sample_config
2024-05-02 07:35:46,933 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2024-05-02 07:35:46,933 - INFO - joeynmt.helpers -                     cfg.data.train : data/train
2024-05-02 07:35:46,933 - INFO - joeynmt.helpers -                       cfg.data.dev : data/dev
2024-05-02 07:35:46,933 - INFO - joeynmt.helpers -                      cfg.data.test : data/test
2024-05-02 07:35:46,933 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2024-05-02 07:35:46,933 - INFO - joeynmt.helpers -                  cfg.data.src.lang : en
2024-05-02 07:35:46,933 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2024-05-02 07:35:46,933 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2024-05-02 07:35:46,934 - INFO - joeynmt.helpers -       cfg.data.src.max_sent_length : 100
2024-05-02 07:35:46,934 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : data/joint_vocab_4k.txt
2024-05-02 07:35:46,934 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2024-05-02 07:35:46,934 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : moses
2024-05-02 07:35:46,934 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.num_merges : 4000
2024-05-02 07:35:46,934 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : data/codes4000.bpe
2024-05-02 07:35:46,934 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : de
2024-05-02 07:35:46,934 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2024-05-02 07:35:46,934 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2024-05-02 07:35:46,934 - INFO - joeynmt.helpers -       cfg.data.trg.max_sent_length : 100
2024-05-02 07:35:46,934 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : data/joint_vocab_4k.txt
2024-05-02 07:35:46,934 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2024-05-02 07:35:46,934 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : moses
2024-05-02 07:35:46,934 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.num_merges : 2000
2024-05-02 07:35:46,934 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : data/codes4000.bpe
2024-05-02 07:35:46,934 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2024-05-02 07:35:46,934 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0
2024-05-02 07:35:46,935 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2024-05-02 07:35:46,935 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2024-05-02 07:35:46,935 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2024-05-02 07:35:46,935 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2024-05-02 07:35:46,935 - INFO - joeynmt.helpers -            cfg.training.batch_size : 2048
2024-05-02 07:35:46,935 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2024-05-02 07:35:46,935 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2024-05-02 07:35:46,935 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2024-05-02 07:35:46,935 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2024-05-02 07:35:46,935 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2024-05-02 07:35:46,935 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2024-05-02 07:35:46,935 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2024-05-02 07:35:46,935 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2024-05-02 07:35:46,935 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2024-05-02 07:35:46,935 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2024-05-02 07:35:46,935 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2024-05-02 07:35:46,935 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu
2024-05-02 07:35:46,935 - INFO - joeynmt.helpers -             cfg.training.model_dir : models/bpe_4k
2024-05-02 07:35:46,936 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2024-05-02 07:35:46,936 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2024-05-02 07:35:46,936 - INFO - joeynmt.helpers -              cfg.training.use_cuda : False
2024-05-02 07:35:46,936 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2024-05-02 07:35:46,936 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2024-05-02 07:35:46,936 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2024-05-02 07:35:46,936 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2024-05-02 07:35:46,936 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2024-05-02 07:35:46,936 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2024-05-02 07:35:46,936 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2024-05-02 07:35:46,936 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2024-05-02 07:35:46,936 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True
2024-05-02 07:35:46,936 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2024-05-02 07:35:46,936 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2024-05-02 07:35:46,936 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2024-05-02 07:35:46,936 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2024-05-02 07:35:46,936 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2024-05-02 07:35:46,936 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2024-05-02 07:35:46,936 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0
2024-05-02 07:35:46,937 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2024-05-02 07:35:46,937 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2024-05-02 07:35:46,937 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0
2024-05-02 07:35:46,937 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2024-05-02 07:35:46,937 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2024-05-02 07:35:46,937 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2024-05-02 07:35:46,937 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2024-05-02 07:35:46,937 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2024-05-02 07:35:46,937 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0
2024-05-02 07:35:46,937 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2024-05-02 07:35:46,937 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2024-05-02 07:35:46,937 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0
2024-05-02 07:35:46,943 - INFO - joeynmt.data - Building tokenizer...
2024-05-02 07:35:47,349 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-02 07:35:47,349 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-02 07:35:47,349 - INFO - joeynmt.data - Loading train set...
2024-05-02 07:36:07,923 - INFO - joeynmt.data - Building vocabulary...
2024-05-02 07:36:08,078 - INFO - joeynmt.data - Loading dev set...
2024-05-02 07:36:08,266 - INFO - joeynmt.data - Loading test set...
2024-05-02 07:36:08,577 - INFO - joeynmt.data - Data loaded.
2024-05-02 07:36:08,577 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=100000, src_lang=en, trg_lang=de, has_trg=True, random_subset=-1)
2024-05-02 07:36:08,577 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=888, src_lang=en, trg_lang=de, has_trg=True, random_subset=-1)
2024-05-02 07:36:08,577 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=1568, src_lang=en, trg_lang=de, has_trg=True, random_subset=-1)
2024-05-02 07:36:08,578 - INFO - joeynmt.data - First training example:
	[SRC] A@@ l G@@ ore : A@@ ver@@ ting the clim@@ ate c@@ ris@@ is
	[TRG] A@@ l G@@ ore : Die Ab@@ wen@@ dung der Kli@@ ma@@ k@@ at@@ ast@@ rop@@ he
2024-05-02 07:36:08,578 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) in (6) a (7) to (8) of (9) die
2024-05-02 07:36:08,578 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) in (6) a (7) to (8) of (9) die
2024-05-02 07:36:08,578 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 4000
2024-05-02 07:36:08,578 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 4000
2024-05-02 07:36:08,580 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-02 07:36:08,639 - INFO - joeynmt.model - Enc-dec model built.
2024-05-02 07:36:08,641 - INFO - joeynmt.model - Total params: 3923200
2024-05-02 07:36:08,641 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2024-05-02 07:36:08,642 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=4000),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=4000),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2024-05-02 07:36:08,642 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2024-05-02 07:36:08,642 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=8)
2024-05-02 07:36:08,643 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2048
	effective batch size (w. parallel & accumulation): 2048
2024-05-02 07:36:08,643 - INFO - joeynmt.training - EPOCH 1
2024-05-02 07:36:46,995 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     4.217216, Batch Acc: 0.057272, Tokens per Sec:     1945, Lr: 0.000300
2024-05-02 07:37:24,027 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     4.104640, Batch Acc: 0.092482, Tokens per Sec:     2013, Lr: 0.000300
2024-05-02 07:38:02,161 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     3.927828, Batch Acc: 0.101203, Tokens per Sec:     1979, Lr: 0.000300
2024-05-02 07:38:39,463 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     3.817652, Batch Acc: 0.121163, Tokens per Sec:     2006, Lr: 0.000300
2024-05-02 07:39:17,458 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     3.754706, Batch Acc: 0.135486, Tokens per Sec:     1933, Lr: 0.000300
2024-05-02 07:39:17,458 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 07:39:17,458 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 07:42:35,087 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.74, ppl:  41.98, acc:   0.12, generation: 197.3601[sec], evaluation: 0.0000[sec]
2024-05-02 07:42:35,089 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 07:42:35,286 - INFO - joeynmt.training - Example #0
2024-05-02 07:42:35,286 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 07:42:35,286 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 07:42:35,286 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', ',', 'dass', 'ich', ',', 'dass', 'die', ',', 'dass', 'die', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der']
2024-05-02 07:42:35,287 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 07:42:35,287 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 07:42:35,287 - INFO - joeynmt.training - 	Hypothesis: Ich ich ich ich ich ich ich ich ich ich ich ich, dass ich, dass die, dass die der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der
2024-05-02 07:42:35,287 - INFO - joeynmt.training - Example #1
2024-05-02 07:42:35,287 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 07:42:35,287 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 07:42:35,287 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'ist', 'ist', 'es', ',', 'dass', 'es', ',', 'dass', 'es', ',', 'dass', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die']
2024-05-02 07:42:35,287 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 07:42:35,287 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 07:42:35,288 - INFO - joeynmt.training - 	Hypothesis: Aber ist ist es, dass es, dass es, dass die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die
2024-05-02 07:42:35,288 - INFO - joeynmt.training - Example #2
2024-05-02 07:42:35,288 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 07:42:35,288 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 07:42:35,288 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'ist', ',', 'dass', 'die', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', ',']
2024-05-02 07:42:35,288 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 07:42:35,288 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 07:42:35,288 - INFO - joeynmt.training - 	Hypothesis: Das ist, dass die ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein,
2024-05-02 07:42:35,288 - INFO - joeynmt.training - Example #3
2024-05-02 07:42:35,288 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 07:42:35,289 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 07:42:35,289 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'die', 'ein', 'ein', 'ein', 'ein', ',', 'dass', 'die', 'ein', 'ein', ',', 'dass', 'die', 'ein', ',', 'dass', 'die', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'ein', 'k@@', 'en', '.', '</s>']
2024-05-02 07:42:35,289 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 07:42:35,289 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 07:42:35,289 - INFO - joeynmt.training - 	Hypothesis: Es ist die ein ein ein ein, dass die ein ein, dass die ein, dass die ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ein ken.
2024-05-02 07:42:35,289 - INFO - joeynmt.training - Example #4
2024-05-02 07:42:35,289 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 07:42:35,289 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 07:42:35,289 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich']
2024-05-02 07:42:35,290 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 07:42:35,290 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 07:42:35,290 - INFO - joeynmt.training - 	Hypothesis: Ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich
2024-05-02 07:43:13,319 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     3.569679, Batch Acc: 0.144303, Tokens per Sec:     2010, Lr: 0.000300
2024-05-02 07:43:49,822 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     3.627059, Batch Acc: 0.147108, Tokens per Sec:     2021, Lr: 0.000300
2024-05-02 07:44:27,848 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     3.571087, Batch Acc: 0.153733, Tokens per Sec:     1998, Lr: 0.000300
2024-05-02 07:45:04,430 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     3.494618, Batch Acc: 0.155151, Tokens per Sec:     1960, Lr: 0.000300
2024-05-02 07:45:40,984 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     3.273189, Batch Acc: 0.164297, Tokens per Sec:     2032, Lr: 0.000300
2024-05-02 07:45:40,984 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 07:45:40,984 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 07:48:51,543 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.51, ppl:  33.60, acc:   0.14, generation: 190.4222[sec], evaluation: 0.0000[sec]
2024-05-02 07:48:51,544 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 07:48:51,736 - INFO - joeynmt.training - Example #0
2024-05-02 07:48:51,736 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 07:48:51,737 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 07:48:51,737 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ich', 'habe', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'die', 'T@@', 'ü@@', 't@@', '-@@', 'T@@', 'ü@@', 't@@', '-@@', 'T@@', 'ü@@', 'en', ',', 'die', 'T@@', 'ü@@', 't@@', 'ü@@', 'en', ',', 'die', 'T@@', 'ü@@', 't@@', 'ü@@', 'en', ',', 'die', 'T@@', 'ü@@', 't@@', 'ü@@', 'en', ',', 'die', 'T@@', 'ü@@', 't@@', 'ü@@', 't@@', 'ü@@', 'en', ',', 'die', 'T@@', 'ü@@', 't@@', 'ü@@', 'en', ',', 'die', 'T@@', 'ü@@', 't@@', 'ü@@', 'en', '.', '</s>']
2024-05-02 07:48:51,737 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 07:48:51,737 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 07:48:51,737 - INFO - joeynmt.training - 	Hypothesis: Ich habe ich, dass ich, dass ich, dass ich, dass die Tüt-Tüt-Tüen, die Tütüen, die Tütüen, die Tütüen, die Tütütüen, die Tütüen, die Tütüen.
2024-05-02 07:48:51,737 - INFO - joeynmt.training - Example #1
2024-05-02 07:48:51,737 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 07:48:51,737 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 07:48:51,738 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'wenn', 'es', 'ist', 'die', 'Ver@@', 'u@@', '-@@', 'T@@', 'ü@@', 't', ',', 'dass', 'die', 'Ver@@', 'ei@@', 't', ',', 'dass', 'die', 'Ver@@', 'u@@', '-@@', 'T@@', 'ü@@', 'en', '.', '</s>']
2024-05-02 07:48:51,738 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 07:48:51,738 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 07:48:51,738 - INFO - joeynmt.training - 	Hypothesis: Aber wenn es ist die Veru-Tüt, dass die Vereit, dass die Veru-Tüen.
2024-05-02 07:48:51,738 - INFO - joeynmt.training - Example #2
2024-05-02 07:48:51,738 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 07:48:51,738 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 07:48:51,738 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'T@@', 'ü@@', 't', ',', 'dass', 'die', 'T@@', 'ü@@', 't', ',', 'die', 'T@@', 'ü@@', '-@@', 'T@@', 'ü@@', 'en', ',', 'die', 'T@@', 'ü@@', '-@@', 'T@@', 'ü@@', 't', ',', 'die', 'T@@', 'ü@@', '-@@', 'T@@', 'ü@@', 't', '.', '</s>']
2024-05-02 07:48:51,739 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 07:48:51,739 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 07:48:51,739 - INFO - joeynmt.training - 	Hypothesis: Die Tüt, dass die Tüt, die Tü-Tüen, die Tü-Tüt, die Tü-Tüt.
2024-05-02 07:48:51,739 - INFO - joeynmt.training - Example #3
2024-05-02 07:48:51,739 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 07:48:51,739 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 07:48:51,739 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'ein', 'T@@', 'ü@@', 't', ',', 'die', 'T@@', 'ü@@', 't', '.', '</s>']
2024-05-02 07:48:51,739 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 07:48:51,739 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 07:48:51,739 - INFO - joeynmt.training - 	Hypothesis: Es ist ein Tüt, die Tüt.
2024-05-02 07:48:51,740 - INFO - joeynmt.training - Example #4
2024-05-02 07:48:51,740 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 07:48:51,740 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 07:48:51,740 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'T@@', 'it@@', 'e', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', '.', '</s>']
2024-05-02 07:48:51,740 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 07:48:51,740 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 07:48:51,740 - INFO - joeynmt.training - 	Hypothesis: Die Tite, dass ich, dass ich, dass ich, dass ich, dass ich, dass ich, dass ich, dass ich, dass ich, dass ich, dass ich, dass ich, dass ich, dass ich, dass ich, dass ich, dass ich, dass ich, dass ich, dass ich.
2024-05-02 07:49:27,529 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     3.329073, Batch Acc: 0.167430, Tokens per Sec:     2081, Lr: 0.000300
2024-05-02 07:50:04,032 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     3.318813, Batch Acc: 0.170638, Tokens per Sec:     2065, Lr: 0.000300
2024-05-02 07:50:40,301 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     3.282660, Batch Acc: 0.175034, Tokens per Sec:     2059, Lr: 0.000300
2024-05-02 07:51:16,508 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     3.259555, Batch Acc: 0.181610, Tokens per Sec:     2087, Lr: 0.000300
2024-05-02 07:51:52,680 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     3.329810, Batch Acc: 0.192221, Tokens per Sec:     2041, Lr: 0.000300
2024-05-02 07:51:52,680 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 07:51:52,680 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 07:55:06,166 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.33, ppl:  27.80, acc:   0.17, generation: 193.3855[sec], evaluation: 0.0000[sec]
2024-05-02 07:55:06,167 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 07:55:06,364 - INFO - joeynmt.training - Example #0
2024-05-02 07:55:06,365 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 07:55:06,365 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 07:55:06,365 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Als', 'ich', 'in', 'dem', 'Jahren', ',', 'die', 'ich', 'in', 'der', 'T@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'ten', ',', 'die', 'die', 'die', 'K@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'ischen', 'B@@', 'ü@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 'ete', 'von', 'dem', 'Jahren', '.', '</s>']
2024-05-02 07:55:06,365 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 07:55:06,365 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 07:55:06,365 - INFO - joeynmt.training - 	Hypothesis: Als ich in dem Jahren, die ich in der Tiiiiiten, die die die Kiiiiiiiischen Büttttttttttttttttttttttttttttttttttttttttete von dem Jahren.
2024-05-02 07:55:06,366 - INFO - joeynmt.training - Example #1
2024-05-02 07:55:06,366 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 07:55:06,366 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 07:55:06,366 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'H@@', 'ü@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@']
2024-05-02 07:55:06,366 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 07:55:06,366 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 07:55:06,366 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die Hütttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt
2024-05-02 07:55:06,366 - INFO - joeynmt.training - Example #2
2024-05-02 07:55:06,367 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 07:55:06,367 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 07:55:06,367 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'a@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@']
2024-05-02 07:55:06,367 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 07:55:06,367 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 07:55:06,367 - INFO - joeynmt.training - 	Hypothesis: Die Kattttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt
2024-05-02 07:55:06,367 - INFO - joeynmt.training - Example #3
2024-05-02 07:55:06,367 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 07:55:06,367 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 07:55:06,367 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'war', 'in', 'einer', 'K@@', 'ü@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@', 't@@']
2024-05-02 07:55:06,368 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 07:55:06,368 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 07:55:06,368 - INFO - joeynmt.training - 	Hypothesis: Es war in einer Kütttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt
2024-05-02 07:55:06,368 - INFO - joeynmt.training - Example #4
2024-05-02 07:55:06,368 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 07:55:06,368 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 07:55:06,368 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Als', 'ich', 'Ihnen', 'Ihnen', ',', 'ich', 'habe', 'eine', 'paar', 'paar', 'paar', 'paar', 'paar', 'Jahren', ',', 'die', 'die', 'die', 'T@@', 'ü@@', 'le', '.', '</s>']
2024-05-02 07:55:06,369 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 07:55:06,369 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 07:55:06,369 - INFO - joeynmt.training - 	Hypothesis: Als ich Ihnen Ihnen, ich habe eine paar paar paar paar paar Jahren, die die die Tüle.
2024-05-02 07:55:42,624 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     3.166597, Batch Acc: 0.195429, Tokens per Sec:     2093, Lr: 0.000300
2024-05-02 07:56:19,478 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     3.302886, Batch Acc: 0.209529, Tokens per Sec:     2025, Lr: 0.000300
2024-05-02 07:56:56,385 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     3.116885, Batch Acc: 0.219896, Tokens per Sec:     1975, Lr: 0.000300
2024-05-02 07:57:33,547 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     2.849884, Batch Acc: 0.231042, Tokens per Sec:     1986, Lr: 0.000300
2024-05-02 07:58:11,084 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     2.975748, Batch Acc: 0.237923, Tokens per Sec:     1950, Lr: 0.000300
2024-05-02 07:58:11,084 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 07:58:11,084 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 08:01:18,787 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.05, ppl:  21.15, acc:   0.22, generation: 187.5806[sec], evaluation: 0.0000[sec]
2024-05-02 08:01:18,788 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 08:01:18,986 - INFO - joeynmt.training - Example #0
2024-05-02 08:01:18,986 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 08:01:18,986 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 08:01:18,986 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'a@@', 'b', 'ich', 'zwei', 'Jahre', 'Jahre', 'zwei', 'Jahre', 'zwei', 'Jahre', ',', 'die', 'die', 'die', 'T@@', 'on@@', 'on@@', 'a', ',', 'die', 'die', 'die', 'drei', 'Jahre', 'Jahre', 'Jahre', ',', 'die', 'zwei', 'Jahre', 'Jahre', ',', 'die', 'zwei', 'Jahre', 'Jahre', ',', 'die', 'zwei', 'Jahre', 'in', 'den', 'letzten', '2@@', '.000', 'Prozent', 'der', 'Prozent', 'der', 'Prozent', 'der', 'Prozent', 'der', 'Prozent', 'der', 'Prozent', 'der', 'Prozent', 'der', 'Prozent', 'der', 'Prozent', 'der', 'Prozent', 'der', 'Prozent', 'der', 'Prozent', 'der', 'Prozent', 'von', 'zwei', 'Prozent', 'der', 'Prozent', 'der', 'Prozent', 'der', 'Prozent', 'der', 'Prozent', 'der', 'Prozent', 'der', 'Prozent', 'der', 'Prozent', 'der', 'Prozent', 'der', 'Prozent', 'der', 'Prozent']
2024-05-02 08:01:18,987 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 08:01:18,987 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 08:01:18,987 - INFO - joeynmt.training - 	Hypothesis: Sab ich zwei Jahre Jahre zwei Jahre zwei Jahre, die die die Tonona, die die die drei Jahre Jahre Jahre, die zwei Jahre Jahre, die zwei Jahre Jahre, die zwei Jahre in den letzten 2.000 Prozent der Prozent der Prozent der Prozent der Prozent der Prozent der Prozent der Prozent der Prozent der Prozent der Prozent der Prozent der Prozent von zwei Prozent der Prozent der Prozent der Prozent der Prozent der Prozent der Prozent der Prozent der Prozent der Prozent der Prozent
2024-05-02 08:01:18,987 - INFO - joeynmt.training - Example #1
2024-05-02 08:01:18,987 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 08:01:18,987 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 08:01:18,987 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'S@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'um', '.', '</s>']
2024-05-02 08:01:18,988 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 08:01:18,988 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 08:01:18,988 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die Siiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiium.
2024-05-02 08:01:18,988 - INFO - joeynmt.training - Example #2
2024-05-02 08:01:18,988 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 08:01:18,988 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 08:01:18,988 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'T@@', 'on@@', 'on@@', 'on@@', 'on@@', 'on@@', 'on@@', 'ischen', 'K@@', 'a@@', 'a@@', 'b', ',', 'die', 'die', 'die', 'K@@', 'ap@@', 'a', ',', 'die', 'die', 'die', 'Er@@', 'st@@', 'ung', '.', '</s>']
2024-05-02 08:01:18,988 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 08:01:18,988 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 08:01:18,989 - INFO - joeynmt.training - 	Hypothesis: Die Tononononononischen Kaab, die die die Kapa, die die die Erstung.
2024-05-02 08:01:18,989 - INFO - joeynmt.training - Example #3
2024-05-02 08:01:18,989 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 08:01:18,989 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 08:01:18,989 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'in', 'der', 'T@@', 'ol@@', 'le', 'und', 'S@@', 'a@@', 'b', '.', '</s>']
2024-05-02 08:01:18,989 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 08:01:18,989 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 08:01:18,989 - INFO - joeynmt.training - 	Hypothesis: Es gibt in in der Tolle und Sab.
2024-05-02 08:01:18,989 - INFO - joeynmt.training - Example #4
2024-05-02 08:01:18,989 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 08:01:18,990 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 08:01:18,990 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'erste', 'Jahre', ',', 'ich', 'habe', 'Ihnen', 'Ihnen', 'Ihnen', ',', 'was', 'ich', 'habe', ',', 'was', 'ich', 'habe', ',', 'was', 'ich', 'habe', ',', 'was', 'ich', 'habe', ',', 'was', 'die', 'die', 'letzten', 'Jahren', '.', '</s>']
2024-05-02 08:01:18,990 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 08:01:18,990 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 08:01:18,990 - INFO - joeynmt.training - 	Hypothesis: Die erste Jahre, ich habe Ihnen Ihnen Ihnen, was ich habe, was ich habe, was ich habe, was ich habe, was die die letzten Jahren.
2024-05-02 08:01:55,799 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     3.012553, Batch Acc: 0.248240, Tokens per Sec:     2042, Lr: 0.000300
2024-05-02 08:02:32,816 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     2.926533, Batch Acc: 0.261091, Tokens per Sec:     2062, Lr: 0.000300
2024-05-02 08:03:10,164 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     2.773473, Batch Acc: 0.262159, Tokens per Sec:     1985, Lr: 0.000300
2024-05-02 08:03:47,315 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     2.724638, Batch Acc: 0.272389, Tokens per Sec:     1992, Lr: 0.000300
2024-05-02 08:04:24,079 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     2.845425, Batch Acc: 0.276540, Tokens per Sec:     2033, Lr: 0.000300
2024-05-02 08:04:24,079 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 08:04:24,079 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 08:07:07,016 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.83, ppl:  16.97, acc:   0.26, generation: 162.8214[sec], evaluation: 0.0000[sec]
2024-05-02 08:07:07,017 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 08:07:07,221 - INFO - joeynmt.training - Example #0
2024-05-02 08:07:07,221 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 08:07:07,222 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 08:07:07,222 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'icht', 'ich', 'diese', 'zwei', 'Jahr', 'zwei', 'zwei', 'zwei', 'zwei', 'Jahre', ',', 'dass', 'die', 'die', 'die', 'zwei', 'Millionen', 'Jahren', ',', 'die', 'die', 'die', 'meisten', 'zwei', 'Millionen', 'Jahren', ',', 'die', 'die', 'die', 'letzten', 'zwei', 'Millionen', 'Jahren', 'von', 'den', 'letzten', 'letzten', 'zwei', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen']
2024-05-02 08:07:07,222 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 08:07:07,222 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 08:07:07,222 - INFO - joeynmt.training - 	Hypothesis: Licht ich diese zwei Jahr zwei zwei zwei zwei Jahre, dass die die die zwei Millionen Jahren, die die die meisten zwei Millionen Jahren, die die die letzten zwei Millionen Jahren von den letzten letzten zwei Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen
2024-05-02 08:07:07,222 - INFO - joeynmt.training - Example #1
2024-05-02 08:07:07,223 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 08:07:07,223 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 08:07:07,223 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'das', 'T@@', 'ats@@', 'ache', ',', 'weil', 'es', 'ist', 'das', 'Problem', ',', 'weil', 'es', 'nicht', 'so', 'so', 'so', 'so', 'so', ',', 'weil', 'es', 'nicht', 'die', 'T@@', 'eile', 'der', 'T@@', 'ats@@', 'ache', '.', '</s>']
2024-05-02 08:07:07,223 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 08:07:07,223 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 08:07:07,223 - INFO - joeynmt.training - 	Hypothesis: Aber das ist das Tatsache, weil es ist das Problem, weil es nicht so so so so so, weil es nicht die Teile der Tatsache.
2024-05-02 08:07:07,223 - INFO - joeynmt.training - Example #2
2024-05-02 08:07:07,223 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 08:07:07,223 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 08:07:07,223 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'N@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@']
2024-05-02 08:07:07,224 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 08:07:07,224 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 08:07:07,224 - INFO - joeynmt.training - 	Hypothesis: Die Niiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii
2024-05-02 08:07:07,224 - INFO - joeynmt.training - Example #3
2024-05-02 08:07:07,224 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 08:07:07,224 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 08:07:07,224 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'den', 'Ver@@', 'füg@@', 'ung', 'und', 'Ver@@', 'einig@@', 'ten', '.', '</s>']
2024-05-02 08:07:07,225 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 08:07:07,225 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 08:07:07,225 - INFO - joeynmt.training - 	Hypothesis: Es gibt in den Verfügung und Vereinigten.
2024-05-02 08:07:07,225 - INFO - joeynmt.training - Example #4
2024-05-02 08:07:07,225 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 08:07:07,225 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 08:07:07,225 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'erste', 'Bild', ',', 'dass', 'ich', 'ein', 'paar', 'Jahren', ',', 'dass', 'man', 'ein', 'paar', 'Jahren', ',', 'was', 'ich', 'habe', ',', 'was', 'die', 'die', 'letzten', 'letzten', 'Jahren', '.', '</s>']
2024-05-02 08:07:07,225 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 08:07:07,225 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 08:07:07,225 - INFO - joeynmt.training - 	Hypothesis: Die erste Bild, dass ich ein paar Jahren, dass man ein paar Jahren, was ich habe, was die die letzten letzten Jahren.
2024-05-02 08:07:43,475 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     2.793743, Batch Acc: 0.277835, Tokens per Sec:     2021, Lr: 0.000300
2024-05-02 08:08:22,216 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     2.611405, Batch Acc: 0.281127, Tokens per Sec:     1910, Lr: 0.000300
2024-05-02 08:08:58,581 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     2.495117, Batch Acc: 0.291322, Tokens per Sec:     2040, Lr: 0.000300
2024-05-02 08:09:36,049 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     2.656291, Batch Acc: 0.293663, Tokens per Sec:     1958, Lr: 0.000300
2024-05-02 08:10:11,786 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     2.733928, Batch Acc: 0.303254, Tokens per Sec:     2071, Lr: 0.000300
2024-05-02 08:10:11,786 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 08:10:11,786 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 08:12:23,575 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.67, ppl:  14.48, acc:   0.28, generation: 131.6810[sec], evaluation: 0.0000[sec]
2024-05-02 08:12:23,577 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 08:12:23,773 - INFO - joeynmt.helpers - delete models/bpe_4k/500.ckpt
2024-05-02 08:12:23,776 - INFO - joeynmt.training - Example #0
2024-05-02 08:12:23,776 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 08:12:23,776 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 08:12:23,776 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'icht', 'ich', 'diese', 'zwei', 'Jahre', 'habe', 'diese', 'zwei', 'Millionen', 'von', 'drei', 'Jahren', ',', 'die', 'die', 'die', 'meisten', 'von', 'drei', 'Jahren', ',', 'die', 'die', 'meisten', 'von', 'drei', 'Jahren', ',', 'die', 'meisten', 'von', 'drei', 'Jahren', 'von', 'drei', 'Jahren', 'von', '1@@', ',@@', '5', 'Prozent', 'der', '1@@', ',@@', '5', 'Prozent', 'der', '1@@', ',@@', '5', 'Prozent', 'der', '1@@', ',@@', ',@@', '5', 'Prozent', 'der', '1@@', '1@@', ',@@', '5', 'Prozent', 'von', '1@@', ',@@', '5', 'Prozent', 'der', 'Milliarden', 'Prozent', 'der', 'der', 'letzten', 'drei', 'Prozent', 'der', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten']
2024-05-02 08:12:23,776 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 08:12:23,776 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 08:12:23,777 - INFO - joeynmt.training - 	Hypothesis: Licht ich diese zwei Jahre habe diese zwei Millionen von drei Jahren, die die die meisten von drei Jahren, die die meisten von drei Jahren, die meisten von drei Jahren von drei Jahren von 1,5 Prozent der 1,5 Prozent der 1,5 Prozent der 1,,5 Prozent der 11,5 Prozent von 1,5 Prozent der Milliarden Prozent der der letzten drei Prozent der letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten
2024-05-02 08:12:23,777 - INFO - joeynmt.training - Example #1
2024-05-02 08:12:23,777 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 08:12:23,777 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 08:12:23,777 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'S@@', 'itu@@', 'ation', 'des', 'Problem', ',', 'weil', 'es', 'nicht', 'nicht', 'die', 'Art', 'von', 'dieser', 'Art', 'der', 'Art', 'der', 'Art', 'der', 'H@@', 'aup@@', 't@@', 't@@', 't@@', 'at', '.', '</s>']
2024-05-02 08:12:23,777 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 08:12:23,777 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 08:12:23,777 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die Situation des Problem, weil es nicht nicht die Art von dieser Art der Art der Art der Hauptttat.
2024-05-02 08:12:23,777 - INFO - joeynmt.training - Example #2
2024-05-02 08:12:23,778 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 08:12:23,778 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 08:12:23,778 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'W@@', 'ie@@', 'der@@', 'der@@', 'der@@', 'der@@', 't', 'ist', 'ein', 'Kind', ',', 'die', 'K@@', 'rie@@', 'g', 'des', 'H@@', 'and', 'der', 'Welt', '.', '</s>']
2024-05-02 08:12:23,778 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 08:12:23,778 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 08:12:23,778 - INFO - joeynmt.training - 	Hypothesis: Die Wiederderderdert ist ein Kind, die Krieg des Hand der Welt.
2024-05-02 08:12:23,778 - INFO - joeynmt.training - Example #3
2024-05-02 08:12:23,778 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 08:12:23,779 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 08:12:23,779 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'hat', 'in', 'der', 'K@@', 'ü@@', 'he', 'und', 'ver@@', 'besser@@', 't', 'in', 'der', 'H@@', 'aup@@', 't@@', 't@@', 't@@', 'at', '.', '</s>']
2024-05-02 08:12:23,779 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 08:12:23,779 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 08:12:23,779 - INFO - joeynmt.training - 	Hypothesis: Es hat in der Kühe und verbessert in der Hauptttat.
2024-05-02 08:12:23,779 - INFO - joeynmt.training - Example #4
2024-05-02 08:12:23,779 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 08:12:23,779 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 08:12:23,779 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Bild', ',', 'ich', 'werde', 'Ihnen', 'ein', 'paar', 'Jahre', 'ein', 'F@@', 'rei@@', 's', ',', 'was', 'was', 'in', 'der', 'letzten', 'letzten', 'Jahre', '.', '</s>']
2024-05-02 08:12:23,780 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 08:12:23,780 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 08:12:23,780 - INFO - joeynmt.training - 	Hypothesis: Die nächste Bild, ich werde Ihnen ein paar Jahre ein Freis, was was in der letzten letzten Jahre.
2024-05-02 08:13:00,324 - INFO - joeynmt.training - Epoch   1, Step:     3100, Batch Loss:     2.512236, Batch Acc: 0.311637, Tokens per Sec:     2031, Lr: 0.000300
2024-05-02 08:13:37,932 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     2.725286, Batch Acc: 0.313692, Tokens per Sec:     1951, Lr: 0.000300
2024-05-02 08:14:14,770 - INFO - joeynmt.training - Epoch   1, Step:     3300, Batch Loss:     2.465370, Batch Acc: 0.318764, Tokens per Sec:     1991, Lr: 0.000300
2024-05-02 08:14:51,948 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     2.641305, Batch Acc: 0.326788, Tokens per Sec:     2018, Lr: 0.000300
2024-05-02 08:15:29,754 - INFO - joeynmt.training - Epoch   1, Step:     3500, Batch Loss:     2.384834, Batch Acc: 0.331759, Tokens per Sec:     1988, Lr: 0.000300
2024-05-02 08:15:29,754 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 08:15:29,755 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 08:17:58,321 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.55, ppl:  12.84, acc:   0.31, generation: 148.4543[sec], evaluation: 0.0000[sec]
2024-05-02 08:17:58,323 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 08:17:58,523 - INFO - joeynmt.helpers - delete models/bpe_4k/1000.ckpt
2024-05-02 08:17:58,526 - INFO - joeynmt.training - Example #0
2024-05-02 08:17:58,526 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 08:17:58,526 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 08:17:58,526 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'icht', 'diese', 'zwei', 'Jahr', ',', 'dass', 'diese', 'zwei', 'zwei', 'M@@', 'is@@', 'ik@@', 'ro@@', 'te', ',', 'die', 'die', 'die', 'T@@', 'ra@@', 'k@@', 'k@@', 'a', ',', 'die', 'meisten', 'der', 'meisten', 'Jahren', ',', 'die', 'meisten', 'der', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', '4@@', '5', 'Prozent', 'der', 'letzten', '4@@', '5', 'Prozent', 'der', 'F@@', 'ra@@', 'k@@', 'a', '.', '</s>']
2024-05-02 08:17:58,527 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 08:17:58,527 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 08:17:58,527 - INFO - joeynmt.training - 	Hypothesis: Licht diese zwei Jahr, dass diese zwei zwei Misikrote, die die die Trakka, die meisten der meisten Jahren, die meisten der letzten letzten letzten letzten letzten letzten 45 Prozent der letzten 45 Prozent der Fraka.
2024-05-02 08:17:58,527 - INFO - joeynmt.training - Example #1
2024-05-02 08:17:58,527 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 08:17:58,527 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 08:17:58,527 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'W@@', 'ä@@', 'hrend', 'die', 'die', 'S@@', 'ter@@', 'ne', 'dieser', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Problem', 'nicht', 'die', 'nicht', 'die', 'S@@', 'ter@@', 'ne', '.', '</s>']
2024-05-02 08:17:58,527 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 08:17:58,527 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 08:17:58,528 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die Während die die Sterne dieser Problem, weil es nicht die Problem nicht die nicht die Sterne.
2024-05-02 08:17:58,528 - INFO - joeynmt.training - Example #2
2024-05-02 08:17:58,528 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 08:17:58,528 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 08:17:58,528 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'D@@', 'or@@', 'o@@', 't@@', 'isch', 'ist', ',', 'in', 'einer', 'einer', 'Sin@@', 'n', ',', 'die', 'die', 'Ver@@', 'gan@@', 'gen@@', 'heit', 'des', 'Er@@', 'n@@', 'ungs@@', 'system', '.', '</s>']
2024-05-02 08:17:58,528 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 08:17:58,528 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 08:17:58,528 - INFO - joeynmt.training - 	Hypothesis: Die Dorotisch ist, in einer einer Sinn, die die Vergangenheit des Ernungssystem.
2024-05-02 08:17:58,528 - INFO - joeynmt.training - Example #3
2024-05-02 08:17:58,529 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 08:17:58,529 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 08:17:58,529 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'f@@', 'ör@@', 'ter', 'in', 'den', 'H@@', 'än@@', 'der', 'und', 'und', 'P@@', 'f@@', 'f@@', 'f@@', 'and', '.', '</s>']
2024-05-02 08:17:58,529 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 08:17:58,529 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 08:17:58,529 - INFO - joeynmt.training - 	Hypothesis: Es förter in den Händer und und Pfffand.
2024-05-02 08:17:58,529 - INFO - joeynmt.training - Example #4
2024-05-02 08:17:58,529 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 08:17:58,529 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 08:17:58,529 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Bild', ',', 'ich', 'kann', 'Ihnen', 'Ihnen', 'ein', 'K@@', 'ra@@', 'k@@', 'el', 'der', 'K@@', 'ra@@', 'k@@', 'el', 'der', 'letzten', 'Jahren', 'der', 'letzten', 'letzten', 'Jahren', 'ist', ',', 'was', 'das', 'ist', ',', 'was', 'das', 'letz@@', 'te', '.', '</s>']
2024-05-02 08:17:58,530 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 08:17:58,530 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 08:17:58,530 - INFO - joeynmt.training - 	Hypothesis: Die nächste Bild, ich kann Ihnen Ihnen ein Krakel der Krakel der letzten Jahren der letzten letzten Jahren ist, was das ist, was das letzte.
2024-05-02 08:18:35,886 - INFO - joeynmt.training - Epoch   1, Step:     3600, Batch Loss:     2.382274, Batch Acc: 0.337241, Tokens per Sec:     1980, Lr: 0.000300
2024-05-02 08:19:12,890 - INFO - joeynmt.training - Epoch   1, Step:     3700, Batch Loss:     2.474467, Batch Acc: 0.346632, Tokens per Sec:     1969, Lr: 0.000300
2024-05-02 08:19:49,626 - INFO - joeynmt.training - Epoch   1, Step:     3800, Batch Loss:     2.516752, Batch Acc: 0.346961, Tokens per Sec:     2040, Lr: 0.000300
2024-05-02 08:20:26,197 - INFO - joeynmt.training - Epoch   1, Step:     3900, Batch Loss:     2.642564, Batch Acc: 0.352183, Tokens per Sec:     2071, Lr: 0.000300
2024-05-02 08:21:03,508 - INFO - joeynmt.training - Epoch   1, Step:     4000, Batch Loss:     2.278668, Batch Acc: 0.360354, Tokens per Sec:     1983, Lr: 0.000300
2024-05-02 08:21:03,508 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 08:21:03,508 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 08:23:13,308 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.44, ppl:  11.42, acc:   0.34, generation: 129.6898[sec], evaluation: 0.0000[sec]
2024-05-02 08:23:13,309 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 08:23:13,511 - INFO - joeynmt.helpers - delete models/bpe_4k/1500.ckpt
2024-05-02 08:23:13,514 - INFO - joeynmt.training - Example #0
2024-05-02 08:23:13,514 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 08:23:13,514 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 08:23:13,514 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'e', 'ich', 'diese', 'zwei', 'zwei', 'Fo@@', 'to@@', 's', ',', 'die', 'die', 'die', 'A@@', 'tom@@', 'at@@', 'at@@', 'at@@', 'te', ',', 'die', 'die', 'meisten', 'von', 'drei', 'Jahren', ',', 'die', 'die', 'meisten', 'von', 'drei', 'Jahren', 'hat', 'die', 'letzten', '4@@', '8', 'Jahren', 'hat', 'die', 'F@@', 'ern@@', 'seh@@', 'seh@@', 'seh@@', 'e', ',', 'die', 'die', 'F@@', 'ern@@', 'seh@@', 'seh@@', 'e', 'von', '40', '%', '.', '</s>']
2024-05-02 08:23:13,515 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 08:23:13,515 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 08:23:13,515 - INFO - joeynmt.training - 	Hypothesis: Letze ich diese zwei zwei Fotos, die die die Atomatatatte, die die meisten von drei Jahren, die die meisten von drei Jahren hat die letzten 48 Jahren hat die Fernsehsehsehe, die die Fernsehsehe von 40%.
2024-05-02 08:23:13,515 - INFO - joeynmt.training - Example #1
2024-05-02 08:23:13,515 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 08:23:13,515 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 08:23:13,515 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'wird', 'die', 'F@@', 'eu@@', 'er', ',', 'das', 'das', 'Problem', 'des', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'W@@', 'erk@@', 'zeu@@', 'ge', 'der', 'G@@', 'ut', 'der', 'S@@', 'icht', 'der', 'G@@', 'ut', '.', '</s>']
2024-05-02 08:23:13,516 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 08:23:13,516 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 08:23:13,516 - INFO - joeynmt.training - 	Hypothesis: Aber das wird die Feuer, das das Problem des Problem, weil es nicht die Werkzeuge der Gut der Sicht der Gut.
2024-05-02 08:23:13,516 - INFO - joeynmt.training - Example #2
2024-05-02 08:23:13,516 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 08:23:13,516 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 08:23:13,516 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'B@@', 'ar@@', 'ar@@', 'ar@@', 'ch@@', 'a@@', 'b', 'ist', ',', 'in', 'der', 'Her@@', 'z', ',', 'die', 'Her@@', 'z', 'des', 'Her@@', 'z', 'des', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 08:23:13,516 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 08:23:13,517 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 08:23:13,517 - INFO - joeynmt.training - 	Hypothesis: Die Barararchab ist, in der Herz, die Herz des Herz des Klimawandel.
2024-05-02 08:23:13,517 - INFO - joeynmt.training - Example #3
2024-05-02 08:23:13,517 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 08:23:13,517 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 08:23:13,517 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'f@@', 'ör@@', 'dern', 'in', 'und', 'K@@', 'rie@@', 'g', 'und', 'ver@@', 'di@@', 'enen', 'in', 'den', 'B@@', 'ür@@', 'ger@@', 'en@@', 'zen', '.', '</s>']
2024-05-02 08:23:13,517 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 08:23:13,517 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 08:23:13,517 - INFO - joeynmt.training - 	Hypothesis: Es fördern in und Krieg und verdienen in den Bürgerenzen.
2024-05-02 08:23:13,517 - INFO - joeynmt.training - Example #4
2024-05-02 08:23:13,518 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 08:23:13,518 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 08:23:13,518 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fo@@', 'to', 'zeigen', ',', 'ich', 'Ihnen', 'Ihnen', 'Ihnen', 'Ihnen', 'Ihnen', 'Ihnen', 'Ihnen', 'ein', 'F@@', 'ern@@', 'seh@@', 'seh@@', 'e', ',', 'was', 'passiert', ',', 'was', 'das', 'letz@@', 'te', 'Jahrhunder@@', 't', '.', '</s>']
2024-05-02 08:23:13,518 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 08:23:13,518 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 08:23:13,518 - INFO - joeynmt.training - 	Hypothesis: Die nächste Foto zeigen, ich Ihnen Ihnen Ihnen Ihnen Ihnen Ihnen Ihnen ein Fernsehsehe, was passiert, was das letzte Jahrhundert.
2024-05-02 08:23:49,897 - INFO - joeynmt.training - Epoch   1, Step:     4100, Batch Loss:     2.288289, Batch Acc: 0.363873, Tokens per Sec:     1995, Lr: 0.000300
2024-05-02 08:24:26,327 - INFO - joeynmt.training - Epoch   1, Step:     4200, Batch Loss:     2.175377, Batch Acc: 0.370944, Tokens per Sec:     2066, Lr: 0.000300
2024-05-02 08:25:03,460 - INFO - joeynmt.training - Epoch   1, Step:     4300, Batch Loss:     2.232044, Batch Acc: 0.382346, Tokens per Sec:     2020, Lr: 0.000300
2024-05-02 08:25:11,137 - INFO - joeynmt.training - Epoch   1: total training loss 13001.57
2024-05-02 08:25:11,137 - INFO - joeynmt.training - EPOCH 2
2024-05-02 08:25:39,528 - INFO - joeynmt.training - Epoch   2, Step:     4400, Batch Loss:     2.118070, Batch Acc: 0.391671, Tokens per Sec:     1999, Lr: 0.000300
2024-05-02 08:26:15,390 - INFO - joeynmt.training - Epoch   2, Step:     4500, Batch Loss:     2.061304, Batch Acc: 0.386921, Tokens per Sec:     2090, Lr: 0.000300
2024-05-02 08:26:15,390 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 08:26:15,391 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 08:29:14,590 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.34, ppl:  10.39, acc:   0.36, generation: 179.0799[sec], evaluation: 0.0000[sec]
2024-05-02 08:29:14,591 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 08:29:14,791 - INFO - joeynmt.helpers - delete models/bpe_4k/2000.ckpt
2024-05-02 08:29:14,794 - INFO - joeynmt.training - Example #0
2024-05-02 08:29:14,794 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 08:29:14,794 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 08:29:14,794 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'e', 'ich', 'diese', 'zwei', 'D@@', 'or@@', 'gen', ',', 'dass', 'die', 'A@@', 'tom@@', '-@@', 'A@@', 'tom@@', '-@@', 'E@@', 'is@@', 'k@@', 'k@@', 'op@@', '-@@', 'K@@', 'raft', ',', 'die', 'für', 'die', 'letzten', 'drei', 'Millionen', 'Jahren', 'wurde', 'die', 'letzten', 'drei', 'Jahren', 'wurde', 'die', 'letzten', '4@@', '5', 'Millionen', 'von', '4@@', '3', 'Prozent', 'der', 'T@@', 'ro@@', 'ffen', ',', 'die', 'K@@', 'l@@', 'l@@', 'esen', ',', 'die', 'die', 'K@@', 'l@@', 'l@@', 'eh@@', 'r', ',', 'die', 'die', 'K@@', 'l@@', 'eh@@', 'ler', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 08:29:14,795 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 08:29:14,795 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 08:29:14,795 - INFO - joeynmt.training - 	Hypothesis: Letze ich diese zwei Dorgen, dass die Atom-Atom-Eiskkop-Kraft, die für die letzten drei Millionen Jahren wurde die letzten drei Jahren wurde die letzten 45 Millionen von 43 Prozent der Troffen, die Kllesen, die die Kllehr, die die Klehler von 40 Prozent.
2024-05-02 08:29:14,795 - INFO - joeynmt.training - Example #1
2024-05-02 08:29:14,795 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 08:29:14,795 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 08:29:14,795 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'L@@', 'än@@', 'der', 'der', 'S@@', 'or@@', 'n@@', 'ig@@', 'keit', 'des', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'S@@', 'tim@@', 'me', 'der', 'G@@', 'al@@', 't@@', 'eil', 'des', 'G@@', 'al@@', 'es', '.', '</s>']
2024-05-02 08:29:14,795 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 08:29:14,796 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 08:29:14,796 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die Länder der Sornigkeit des Problem, weil es nicht die Stimme der Galteil des Gales.
2024-05-02 08:29:14,796 - INFO - joeynmt.training - Example #2
2024-05-02 08:29:14,796 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 08:29:14,796 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 08:29:14,796 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ünst@@', 'ler', 'ist', ',', 'ist', ',', 'in', 'einem', 'Sin@@', 'n', ',', 'die', 'Her@@', 'zen', ',', 'die', 'B@@', 'ür@@', 'z@@', 'ung', 'des', 'glob@@', 'alen', 'glob@@', 'alen', 'System', '.', '</s>']
2024-05-02 08:29:14,796 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 08:29:14,796 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 08:29:14,796 - INFO - joeynmt.training - 	Hypothesis: Die Künstler ist, ist, in einem Sinn, die Herzen, die Bürzung des globalen globalen System.
2024-05-02 08:29:14,797 - INFO - joeynmt.training - Example #3
2024-05-02 08:29:14,797 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 08:29:14,797 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 08:29:14,797 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ver@@', 'r@@', 'än@@', 'den', 'und', 'Kon@@', 'z@@', 'ept', 'und', 'ver@@', 'ur@@', 's@@', 'acht', '.', '</s>']
2024-05-02 08:29:14,797 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 08:29:14,797 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 08:29:14,797 - INFO - joeynmt.training - 	Hypothesis: Es verränden und Konzept und verursacht.
2024-05-02 08:29:14,797 - INFO - joeynmt.training - Example #4
2024-05-02 08:29:14,797 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 08:29:14,798 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 08:29:14,798 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ge', ',', 'ich', 'werde', 'Ihnen', 'ein', 'F@@', 'el@@', 'des', 'R@@', 'ück@@', 's', ',', 'was', 'passiert', 'ist', ',', 'was', 'passiert', ',', 'was', 'passiert', 'ist', ',', 'was', 'passiert', 'ist', '.', '</s>']
2024-05-02 08:29:14,798 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 08:29:14,798 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 08:29:14,798 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folge, ich werde Ihnen ein Feldes Rücks, was passiert ist, was passiert, was passiert ist, was passiert ist.
2024-05-02 08:29:51,361 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     2.418954, Batch Acc: 0.394104, Tokens per Sec:     1982, Lr: 0.000300
2024-05-02 08:30:27,677 - INFO - joeynmt.training - Epoch   2, Step:     4700, Batch Loss:     2.125357, Batch Acc: 0.404968, Tokens per Sec:     2075, Lr: 0.000300
2024-05-02 08:31:05,386 - INFO - joeynmt.training - Epoch   2, Step:     4800, Batch Loss:     2.111776, Batch Acc: 0.406772, Tokens per Sec:     2035, Lr: 0.000300
2024-05-02 08:31:42,869 - INFO - joeynmt.training - Epoch   2, Step:     4900, Batch Loss:     2.115351, Batch Acc: 0.411595, Tokens per Sec:     2029, Lr: 0.000300
2024-05-02 08:32:19,799 - INFO - joeynmt.training - Epoch   2, Step:     5000, Batch Loss:     2.126243, Batch Acc: 0.415915, Tokens per Sec:     2079, Lr: 0.000300
2024-05-02 08:32:19,799 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 08:32:19,799 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 08:34:16,979 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.24, ppl:   9.40, acc:   0.39, generation: 117.0740[sec], evaluation: 0.0000[sec]
2024-05-02 08:34:16,980 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 08:34:17,182 - INFO - joeynmt.helpers - delete models/bpe_4k/2500.ckpt
2024-05-02 08:34:17,185 - INFO - joeynmt.training - Example #0
2024-05-02 08:34:17,185 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 08:34:17,185 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 08:34:17,185 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', ',', 'dass', 'die', 'zwei', 'Fo@@', 'to', 'so', ',', 'so', 'dass', 'die', 'die', 'Ar@@', 'z@@', 'ul@@', 'ische', 'E@@', 'is@@', 'is@@', 'z@@', 'z@@', 'ul@@', 'ul@@', 'ul@@', 'en', ',', 'die', 'die', 'meisten', 'drei', 'Millionen', 'Jahren', 'wurde', 'die', 'T@@', 'est', 'der', 'letzten', 'drei', 'Jahren', 'wurde', 'die', 'T@@', 'eile', 'der', 'K@@', 'rei@@', 'se', ',', 'hat', 'sich', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 08:34:17,186 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 08:34:17,186 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 08:34:17,186 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr, dass die zwei Foto so, so dass die die Arzulische Eisiszzulululen, die die meisten drei Millionen Jahren wurde die Test der letzten drei Jahren wurde die Teile der Kreise, hat sich von 40 Prozent.
2024-05-02 08:34:17,186 - INFO - joeynmt.training - Example #1
2024-05-02 08:34:17,186 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 08:34:17,186 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 08:34:17,186 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'Ver@@', 'br@@', 'eit@@', 'ung', 'der', 'E@@', 'vol@@', 'l', 'dieser', 'bestimm@@', 'te', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'S@@', 'tim@@', 'me', 'der', 'E@@', 'is@@', 'is', 'der', 'E@@', 'is@@', 'is', '.', '</s>']
2024-05-02 08:34:17,187 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 08:34:17,187 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 08:34:17,187 - INFO - joeynmt.training - 	Hypothesis: Aber das Verbreitung der Evoll dieser bestimmte Problem, weil es nicht die Stimme der Eisis der Eisis.
2024-05-02 08:34:17,187 - INFO - joeynmt.training - Example #2
2024-05-02 08:34:17,187 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 08:34:17,187 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 08:34:17,187 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'z@@', 'z@@', 'ul@@', 'ische', 'E@@', 'is', ',', 'in', 'einem', 'Sin@@', 'n', 'des', 'Ver@@', 'br@@', 'eit@@', 'ung', 'des', 'Kli@@', 'ma@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 08:34:17,188 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 08:34:17,188 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 08:34:17,188 - INFO - joeynmt.training - 	Hypothesis: Das Arzzulische Eis, in einem Sinn des Verbreitung des Klimamawandel.
2024-05-02 08:34:17,188 - INFO - joeynmt.training - Example #3
2024-05-02 08:34:17,188 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 08:34:17,188 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 08:34:17,188 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ver@@', 'w@@', 'ün@@', 'f@@', 'ter', 'und', 'Kon@@', 'tr@@', 'it@@', 'ten', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 08:34:17,188 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 08:34:17,188 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 08:34:17,188 - INFO - joeynmt.training - 	Hypothesis: Es verwünfter und Kontritten in Sommer.
2024-05-02 08:34:17,189 - INFO - joeynmt.training - Example #4
2024-05-02 08:34:17,189 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 08:34:17,189 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 08:34:17,189 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fo@@', 'to', 'zeigen', ',', 'dass', 'ich', 'Ihnen', 'ein', 'R@@', 'ück@@', 'en', 'der', 'letzten', '25', 'Jahren', 'im', 'letzten', '25', 'Jahren', 'ist', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', '.', '</s>']
2024-05-02 08:34:17,189 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 08:34:17,189 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 08:34:17,189 - INFO - joeynmt.training - 	Hypothesis: Die nächste Foto zeigen, dass ich Ihnen ein Rücken der letzten 25 Jahren im letzten 25 Jahren ist, was in den letzten 25 Jahren.
2024-05-02 08:34:54,304 - INFO - joeynmt.training - Epoch   2, Step:     5100, Batch Loss:     2.069850, Batch Acc: 0.413746, Tokens per Sec:     2020, Lr: 0.000300
2024-05-02 08:35:30,528 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     2.146544, Batch Acc: 0.423912, Tokens per Sec:     2090, Lr: 0.000300
2024-05-02 08:36:08,541 - INFO - joeynmt.training - Epoch   2, Step:     5300, Batch Loss:     2.324806, Batch Acc: 0.420735, Tokens per Sec:     1871, Lr: 0.000300
2024-05-02 08:36:44,966 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     2.126405, Batch Acc: 0.428493, Tokens per Sec:     2001, Lr: 0.000300
2024-05-02 08:37:22,212 - INFO - joeynmt.training - Epoch   2, Step:     5500, Batch Loss:     1.954801, Batch Acc: 0.426834, Tokens per Sec:     2047, Lr: 0.000300
2024-05-02 08:37:22,212 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 08:37:22,212 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 08:39:39,957 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.18, ppl:   8.83, acc:   0.41, generation: 137.6379[sec], evaluation: 0.0000[sec]
2024-05-02 08:39:39,959 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 08:39:40,161 - INFO - joeynmt.helpers - delete models/bpe_4k/3000.ckpt
2024-05-02 08:39:40,164 - INFO - joeynmt.training - Example #0
2024-05-02 08:39:40,164 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 08:39:40,164 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 08:39:40,164 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Im', 'Jahr', 'Jahr', 'ich', 'diese', 'zwei', 'Fo@@', 'to', ',', 'dass', 'die', 'K@@', 'ünst@@', 'ler', ',', 'dass', 'die', 'Ar@@', 'z@@', 'is', 'der', 'Ar@@', 'z@@', 'a@@', 'p', ',', 'die', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'wurde', ',', 'die', 'die', 'die', 'meisten', 'von', '40', 'Prozent', 'der', 'letzten', '4@@', '0@@', '-@@', '-@@', 'F@@', 'ahr@@', 'en', 'der', 'K@@', 'ap@@', 'ap@@', 'ap@@', 'ital', '.', '</s>']
2024-05-02 08:39:40,165 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 08:39:40,165 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 08:39:40,165 - INFO - joeynmt.training - 	Hypothesis: Im Jahr Jahr ich diese zwei Foto, dass die Künstler, dass die Arzis der Arzap, die die meisten drei Millionen Jahre wurde, die die die meisten von 40 Prozent der letzten 40--Fahren der Kapapapital.
2024-05-02 08:39:40,165 - INFO - joeynmt.training - Example #1
2024-05-02 08:39:40,165 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 08:39:40,165 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 08:39:40,165 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'Ver@@', 'füg@@', 'ung', 'der', 'N@@', 'i@@', 'i@@', 'ff@@', 'e', 'der', 'S@@', 'tim@@', 'me', 'des', 'Problem', 'nicht', 'die', 'T@@', 'is@@', 'n@@', 'ess', '.', '</s>']
2024-05-02 08:39:40,166 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 08:39:40,166 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 08:39:40,166 - INFO - joeynmt.training - 	Hypothesis: Aber das Verfügung der Niiffe der Stimme des Problem nicht die Tisness.
2024-05-02 08:39:40,166 - INFO - joeynmt.training - Example #2
2024-05-02 08:39:40,166 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 08:39:40,166 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 08:39:40,166 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ünst@@', 'ler', 'ist', ',', 'ist', 'in', 'einem', 'Sin@@', 'n', ',', 'der', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', '.', '</s>']
2024-05-02 08:39:40,167 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 08:39:40,167 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 08:39:40,167 - INFO - joeynmt.training - 	Hypothesis: Die Künstler ist, ist in einem Sinn, der Herz des globalen Klima.
2024-05-02 08:39:40,167 - INFO - joeynmt.training - Example #3
2024-05-02 08:39:40,167 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 08:39:40,167 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 08:39:40,167 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ver@@', 'di@@', 'enen', 'in', 'den', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'ta@@', 'k@@', 'ten', '.', '</s>']
2024-05-02 08:39:40,167 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 08:39:40,167 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 08:39:40,168 - INFO - joeynmt.training - 	Hypothesis: Es verdienen in den Sommer und Kontakten.
2024-05-02 08:39:40,168 - INFO - joeynmt.training - Example #4
2024-05-02 08:39:40,168 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 08:39:40,168 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 08:39:40,168 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zeigen', ',', 'ich', 'Ihnen', 'Ihnen', 'zeigen', ',', 'die', 'K@@', 'ra@@', 'k@@', 'te', 'von', 'dem', 'ich', 'Ihnen', 'zeigen', ',', 'was', 'passiert', 'ist', '.', '</s>']
2024-05-02 08:39:40,168 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 08:39:40,168 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 08:39:40,168 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeigen, ich Ihnen Ihnen zeigen, die Krakte von dem ich Ihnen zeigen, was passiert ist.
2024-05-02 08:40:17,768 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     2.123997, Batch Acc: 0.427416, Tokens per Sec:     1986, Lr: 0.000300
2024-05-02 08:40:55,171 - INFO - joeynmt.training - Epoch   2, Step:     5700, Batch Loss:     2.027452, Batch Acc: 0.430422, Tokens per Sec:     1992, Lr: 0.000300
2024-05-02 08:41:31,567 - INFO - joeynmt.training - Epoch   2, Step:     5800, Batch Loss:     1.902727, Batch Acc: 0.430298, Tokens per Sec:     2065, Lr: 0.000300
2024-05-02 08:42:09,338 - INFO - joeynmt.training - Epoch   2, Step:     5900, Batch Loss:     2.072257, Batch Acc: 0.436262, Tokens per Sec:     1969, Lr: 0.000300
2024-05-02 08:42:46,676 - INFO - joeynmt.training - Epoch   2, Step:     6000, Batch Loss:     1.863174, Batch Acc: 0.437080, Tokens per Sec:     2028, Lr: 0.000300
2024-05-02 08:42:46,676 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 08:42:46,676 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 08:44:52,967 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.12, ppl:   8.33, acc:   0.42, generation: 126.1833[sec], evaluation: 0.0000[sec]
2024-05-02 08:44:52,968 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 08:44:53,169 - INFO - joeynmt.helpers - delete models/bpe_4k/3500.ckpt
2024-05-02 08:44:53,172 - INFO - joeynmt.training - Example #0
2024-05-02 08:44:53,172 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 08:44:53,172 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 08:44:53,172 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'en@@', 'st@@', 'off', ',', 'dass', 'die', 'Ar@@', 'z@@', 'z@@', 'z@@', 'a@@', 'p', ',', 'dass', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'wurde', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'wurde', 'die', 'F@@', 'ahr@@', 'ung', 'von', '40', 'Prozent', 'der', 'P@@', 'f@@', 'ä@@', 'us@@', 'er', ',', 'die', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 08:44:53,173 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 08:44:53,173 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 08:44:53,173 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dienstoff, dass die Arzzzap, dass die meisten drei Millionen Jahre wurde, die für die meisten drei Millionen Jahre wurde die Fahrung von 40 Prozent der Pfäuser, die von 40 Prozent.
2024-05-02 08:44:53,173 - INFO - joeynmt.training - Example #1
2024-05-02 08:44:53,173 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 08:44:53,173 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 08:44:53,173 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'das', 'Er@@', 'n@@', 'ähr@@', 'ung', 'des', 'S@@', 'eh@@', 'r', ',', 'weil', 'es', 'nicht', 'die', 'E@@', 'is', 'des', 'E@@', 'is', 'des', 'E@@', 'is', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 08:44:53,174 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 08:44:53,174 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 08:44:53,174 - INFO - joeynmt.training - 	Hypothesis: Aber das ist das Ernährung des Sehr, weil es nicht die Eis des Eis des Eis des Eis.
2024-05-02 08:44:53,174 - INFO - joeynmt.training - Example #2
2024-05-02 08:44:53,174 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 08:44:53,174 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 08:44:53,174 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'z@@', 'a@@', 'h', 'ist', ',', 'ist', 'der', 'ein', 'Sin@@', 'n', ',', 'der', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'System', '.', '</s>']
2024-05-02 08:44:53,175 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 08:44:53,175 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 08:44:53,175 - INFO - joeynmt.training - 	Hypothesis: Die Arzah ist, ist der ein Sinn, der Herz des globalen Herz des globalen System.
2024-05-02 08:44:53,175 - INFO - joeynmt.training - Example #3
2024-05-02 08:44:53,175 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 08:44:53,175 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 08:44:53,175 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'von', 'H@@', 'um@@', 'mer', 'und', 'Kon@@', 'tin@@', 'ent', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 08:44:53,175 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 08:44:53,175 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 08:44:53,175 - INFO - joeynmt.training - 	Hypothesis: Es gibt von Hummer und Kontinent in Sommer.
2024-05-02 08:44:53,176 - INFO - joeynmt.training - Example #4
2024-05-02 08:44:53,176 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 08:44:53,176 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 08:44:53,176 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'ich', 'Ihnen', 'zeigen', ',', 'dass', 'ein', 'F@@', 'ast@@', 'er', 'von', 'dem', 'W@@', 'esen', ',', 'was', 'passiert', 'passiert', 'passiert', 'ist', '.', '</s>']
2024-05-02 08:44:53,176 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 08:44:53,176 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 08:44:53,176 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, ich Ihnen zeigen, dass ein Faster von dem Wesen, was passiert passiert passiert ist.
2024-05-02 08:45:30,139 - INFO - joeynmt.training - Epoch   2, Step:     6100, Batch Loss:     2.110378, Batch Acc: 0.443433, Tokens per Sec:     1977, Lr: 0.000300
2024-05-02 08:46:07,272 - INFO - joeynmt.training - Epoch   2, Step:     6200, Batch Loss:     1.958395, Batch Acc: 0.441907, Tokens per Sec:     2052, Lr: 0.000300
2024-05-02 08:46:43,540 - INFO - joeynmt.training - Epoch   2, Step:     6300, Batch Loss:     1.977081, Batch Acc: 0.442905, Tokens per Sec:     2045, Lr: 0.000300
2024-05-02 08:47:20,798 - INFO - joeynmt.training - Epoch   2, Step:     6400, Batch Loss:     1.952531, Batch Acc: 0.448060, Tokens per Sec:     1977, Lr: 0.000300
2024-05-02 08:47:57,080 - INFO - joeynmt.training - Epoch   2, Step:     6500, Batch Loss:     2.026366, Batch Acc: 0.451704, Tokens per Sec:     2104, Lr: 0.000300
2024-05-02 08:47:57,080 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 08:47:57,080 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 08:50:01,616 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.08, ppl:   8.01, acc:   0.43, generation: 124.4287[sec], evaluation: 0.0000[sec]
2024-05-02 08:50:01,618 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 08:50:01,820 - INFO - joeynmt.helpers - delete models/bpe_4k/4000.ckpt
2024-05-02 08:50:01,823 - INFO - joeynmt.training - Example #0
2024-05-02 08:50:01,824 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 08:50:01,824 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 08:50:01,824 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', ',', 'ich', 'habe', 'diese', 'zwei', 'Fol@@', 'gen@@', 'des', ',', 'so', 'dass', 'die', 'Ar@@', 'z@@', 'z@@', 'z@@', 'a@@', 'p', ',', 'die', 'für', 'die', 'meisten', 'meisten', 'von', 'den', 'letzten', 'drei', 'Millionen', 'Jahren', 'die', 'meisten', 'drei', 'Millionen', 'Jahren', 'die', 'T@@', 'ro@@', 'ten', 'der', 'T@@', 'ro@@', 't@@', 'z', 'der', 'größ@@', 'ten', '4@@', '8@@', '.', '</s>']
2024-05-02 08:50:01,824 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 08:50:01,824 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 08:50:01,824 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr, ich habe diese zwei Folgendes, so dass die Arzzzap, die für die meisten meisten von den letzten drei Millionen Jahren die meisten drei Millionen Jahren die Troten der Trotz der größten 48.
2024-05-02 08:50:01,824 - INFO - joeynmt.training - Example #1
2024-05-02 08:50:01,825 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 08:50:01,825 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 08:50:01,825 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'Er@@', 'ste', 'der', 'E@@', 'is@@', 's@@', 'th@@', 'u@@', 'ck@@', 'en', ',', 'weil', 'es', 'nicht', 'die', 'T@@', 'ro@@', 't@@', 'z', 'der', 'E@@', 'is', '.', '</s>']
2024-05-02 08:50:01,825 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 08:50:01,825 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 08:50:01,825 - INFO - joeynmt.training - 	Hypothesis: Aber das Erste der Eissthucken, weil es nicht die Trotz der Eis.
2024-05-02 08:50:01,825 - INFO - joeynmt.training - Example #2
2024-05-02 08:50:01,825 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 08:50:01,825 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 08:50:01,826 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'Her@@', 'z@@', '-@@', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'ma@@', 'k@@', 'k@@', 'lim@@', 'a', '.', '</s>']
2024-05-02 08:50:01,826 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 08:50:01,826 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 08:50:01,826 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eis ist, in einem Sinne, der Herz-Klima des globalen Klimamakklima.
2024-05-02 08:50:01,826 - INFO - joeynmt.training - Example #3
2024-05-02 08:50:01,826 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 08:50:01,826 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 08:50:01,826 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 08:50:01,827 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 08:50:01,827 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 08:50:01,827 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Sommer und Kontrakten in Sommer.
2024-05-02 08:50:01,827 - INFO - joeynmt.training - Example #4
2024-05-02 08:50:01,827 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 08:50:01,827 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 08:50:01,827 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'ich', 'Ihnen', 'wird', 'ein', 'R@@', 'i@@', 'es@@', 'o', ',', 'was', 'passiert', 'passiert', 'passiert', 'passiert', ',', 'was', 'passiert', 'passiert', 'passiert', 'passiert', 'passiert', 'passiert', 'passiert', 'passiert', '.', '</s>']
2024-05-02 08:50:01,827 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 08:50:01,828 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 08:50:01,828 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, ich Ihnen wird ein Rieso, was passiert passiert passiert passiert, was passiert passiert passiert passiert passiert passiert passiert passiert.
2024-05-02 08:50:39,038 - INFO - joeynmt.training - Epoch   2, Step:     6600, Batch Loss:     1.848717, Batch Acc: 0.444294, Tokens per Sec:     1997, Lr: 0.000300
2024-05-02 08:51:15,329 - INFO - joeynmt.training - Epoch   2, Step:     6700, Batch Loss:     2.008734, Batch Acc: 0.454529, Tokens per Sec:     2069, Lr: 0.000300
2024-05-02 08:51:51,988 - INFO - joeynmt.training - Epoch   2, Step:     6800, Batch Loss:     1.954024, Batch Acc: 0.446425, Tokens per Sec:     1950, Lr: 0.000300
2024-05-02 08:52:28,770 - INFO - joeynmt.training - Epoch   2, Step:     6900, Batch Loss:     2.056079, Batch Acc: 0.456912, Tokens per Sec:     2031, Lr: 0.000300
2024-05-02 08:53:05,320 - INFO - joeynmt.training - Epoch   2, Step:     7000, Batch Loss:     2.127585, Batch Acc: 0.459911, Tokens per Sec:     2062, Lr: 0.000300
2024-05-02 08:53:05,320 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 08:53:05,320 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 08:54:54,272 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.04, ppl:   7.69, acc:   0.44, generation: 108.8472[sec], evaluation: 0.0000[sec]
2024-05-02 08:54:54,273 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 08:54:54,477 - INFO - joeynmt.helpers - delete models/bpe_4k/4500.ckpt
2024-05-02 08:54:54,480 - INFO - joeynmt.training - Example #0
2024-05-02 08:54:54,481 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 08:54:54,481 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 08:54:54,481 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'en@@', 'st@@', 'off', ',', 'dass', 'die', 'Ar@@', 'kt@@', 'is', 'is', 'is', 'is', 'is', ',', 'die', 'für', 'die', 'meisten', 'von', 'den', 'letzten', 'drei', 'Millionen', 'Jahren', 'wurde', 'die', 'Größ@@', 'e', 'der', 'T@@', 'ra@@', 'k@@', 'ti@@', 'k', 'von', '40', '%', '.', '</s>']
2024-05-02 08:54:54,481 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 08:54:54,481 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 08:54:54,481 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dienstoff, dass die Arktis is is is is, die für die meisten von den letzten drei Millionen Jahren wurde die Größe der Traktik von 40%.
2024-05-02 08:54:54,481 - INFO - joeynmt.training - Example #1
2024-05-02 08:54:54,482 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 08:54:54,482 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 08:54:54,482 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'Ver@@', 'stand', 'dieser', 'Aus@@', 'ma@@', 'ß', 'dieser', 'bestimm@@', 'te', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'T@@', 'ü@@', 'r', 'der', 'T@@', 'u@@', 'f@@', 'el', 'des', 'E@@', 'is@@', 's', '.', '</s>']
2024-05-02 08:54:54,482 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 08:54:54,482 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 08:54:54,482 - INFO - joeynmt.training - 	Hypothesis: Aber das Verstand dieser Ausmaß dieser bestimmte Problem, weil es nicht die Tür der Tufel des Eiss.
2024-05-02 08:54:54,482 - INFO - joeynmt.training - Example #2
2024-05-02 08:54:54,482 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 08:54:54,482 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 08:54:54,483 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', '-@@', 'Z@@', 'a@@', 'p', 'ist', ',', 'ist', 'der', 'Her@@', 'z@@', '-@@', 'Her@@', 'z@@', '-@@', 'System', '.', '</s>']
2024-05-02 08:54:54,483 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 08:54:54,483 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 08:54:54,483 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eis-Zap ist, ist der Herz-Herz-System.
2024-05-02 08:54:54,483 - INFO - joeynmt.training - Example #3
2024-05-02 08:54:54,483 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 08:54:54,483 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 08:54:54,483 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'den', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 08:54:54,484 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 08:54:54,484 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 08:54:54,484 - INFO - joeynmt.training - 	Hypothesis: Es gibt in den Winter und Kontrakten in Sommer.
2024-05-02 08:54:54,484 - INFO - joeynmt.training - Example #4
2024-05-02 08:54:54,484 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 08:54:54,484 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 08:54:54,484 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'dass', 'ich', 'Ihnen', 'ein', 'R@@', 'i@@', 'ff@@', 'e', 'von', 'dem', ',', 'was', 'passiert', 'passiert', 'ist', '.', '</s>']
2024-05-02 08:54:54,484 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 08:54:54,485 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 08:54:54,485 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, dass ich Ihnen ein Riffe von dem, was passiert passiert ist.
2024-05-02 08:55:32,357 - INFO - joeynmt.training - Epoch   2, Step:     7100, Batch Loss:     2.098426, Batch Acc: 0.453190, Tokens per Sec:     2004, Lr: 0.000300
2024-05-02 08:56:08,947 - INFO - joeynmt.training - Epoch   2, Step:     7200, Batch Loss:     1.848507, Batch Acc: 0.460339, Tokens per Sec:     2025, Lr: 0.000300
2024-05-02 08:56:46,962 - INFO - joeynmt.training - Epoch   2, Step:     7300, Batch Loss:     1.831109, Batch Acc: 0.458275, Tokens per Sec:     2000, Lr: 0.000300
2024-05-02 08:57:23,965 - INFO - joeynmt.training - Epoch   2, Step:     7400, Batch Loss:     1.890246, Batch Acc: 0.459123, Tokens per Sec:     1991, Lr: 0.000300
2024-05-02 08:58:00,713 - INFO - joeynmt.training - Epoch   2, Step:     7500, Batch Loss:     2.035431, Batch Acc: 0.458372, Tokens per Sec:     1986, Lr: 0.000300
2024-05-02 08:58:00,714 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 08:58:00,714 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 09:00:07,987 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.01, ppl:   7.43, acc:   0.44, generation: 127.1643[sec], evaluation: 0.0000[sec]
2024-05-02 09:00:07,989 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 09:00:08,200 - INFO - joeynmt.helpers - delete models/bpe_4k/5000.ckpt
2024-05-02 09:00:08,203 - INFO - joeynmt.training - Example #0
2024-05-02 09:00:08,203 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 09:00:08,203 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 09:00:08,203 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'en@@', 'st@@', 'off', ',', 'so', 'dass', 'die', 'Ar@@', 'z@@', 't', ',', 'dass', 'die', 'Ar@@', 'z@@', 't', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'wurde', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'wurde', ',', 'die', 'in', '40', 'T@@', 'at@@', 'z', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 09:00:08,204 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 09:00:08,204 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 09:00:08,204 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dienstoff, so dass die Arzt, dass die Arzt, die für die meisten drei Millionen Jahre wurde, die für die meisten drei Millionen Jahre wurde, die in 40 Tatz von 40 Prozent.
2024-05-02 09:00:08,204 - INFO - joeynmt.training - Example #1
2024-05-02 09:00:08,204 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 09:00:08,204 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 09:00:08,204 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'Er@@', 'st@@', 'ens', 'des', 'ern@@', 's@@', 'th@@', 'th@@', 'af@@', 't', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'S@@', 'or@@', 'gen', 'des', 'E@@', 'is', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 09:00:08,205 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 09:00:08,205 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 09:00:08,205 - INFO - joeynmt.training - 	Hypothesis: Aber das Erstens des ernsththaft dieses Problem, weil es nicht die Sorgen des Eis des Eis.
2024-05-02 09:00:08,205 - INFO - joeynmt.training - Example #2
2024-05-02 09:00:08,205 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 09:00:08,205 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 09:00:08,205 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'z@@', 't', 'ist', 'ein', 'Ge@@', 'fühl', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'der', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wan@@', 'del', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 09:00:08,206 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 09:00:08,206 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 09:00:08,206 - INFO - joeynmt.training - 	Hypothesis: Die Arzt ist ein Gefühl, in einem Sinne, der der globalen Klimawandel des globalen Klimawandel.
2024-05-02 09:00:08,206 - INFO - joeynmt.training - Example #3
2024-05-02 09:00:08,206 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 09:00:08,206 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 09:00:08,206 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 09:00:08,207 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 09:00:08,207 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 09:00:08,207 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Kontrakten in Sommer.
2024-05-02 09:00:08,207 - INFO - joeynmt.training - Example #4
2024-05-02 09:00:08,207 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 09:00:08,207 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 09:00:08,207 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Di@@', 'en@@', 'st@@', 'al@@', 'te', ',', 'dass', 'Sie', 'ein', 'R@@', 'i@@', 'es@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'K@@', 'u@@', 'gel', '.', '</s>']
2024-05-02 09:00:08,207 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 09:00:08,207 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 09:00:08,208 - INFO - joeynmt.training - 	Hypothesis: Die nächste Dienstalte, dass Sie ein Ries-Fast-Fast-Fast-Fast-Fast-Kugel.
2024-05-02 09:00:45,643 - INFO - joeynmt.training - Epoch   2, Step:     7600, Batch Loss:     2.075643, Batch Acc: 0.459099, Tokens per Sec:     1959, Lr: 0.000300
2024-05-02 09:01:22,340 - INFO - joeynmt.training - Epoch   2, Step:     7700, Batch Loss:     2.152445, Batch Acc: 0.464301, Tokens per Sec:     2038, Lr: 0.000300
2024-05-02 09:01:59,893 - INFO - joeynmt.training - Epoch   2, Step:     7800, Batch Loss:     2.121184, Batch Acc: 0.467759, Tokens per Sec:     1989, Lr: 0.000300
2024-05-02 09:02:36,502 - INFO - joeynmt.training - Epoch   2, Step:     7900, Batch Loss:     1.993199, Batch Acc: 0.468371, Tokens per Sec:     2030, Lr: 0.000300
2024-05-02 09:03:13,986 - INFO - joeynmt.training - Epoch   2, Step:     8000, Batch Loss:     2.032059, Batch Acc: 0.472157, Tokens per Sec:     2044, Lr: 0.000300
2024-05-02 09:03:13,987 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 09:03:13,987 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 09:05:01,859 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.97, ppl:   7.16, acc:   0.45, generation: 107.7653[sec], evaluation: 0.0000[sec]
2024-05-02 09:05:01,860 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 09:05:02,067 - INFO - joeynmt.helpers - delete models/bpe_4k/5500.ckpt
2024-05-02 09:05:02,070 - INFO - joeynmt.training - Example #0
2024-05-02 09:05:02,070 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 09:05:02,071 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 09:05:02,071 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'ich', 'diese', 'zwei', 'Fol@@', 'gen@@', 'des', ',', 'die', 'T@@', 'at', ',', 'die', 'Ar@@', 'kt@@', 'ischen', 'E@@', '-@@', 'E@@', '-@@', 'E@@', '-@@', 'E@@', '-@@', 'F@@', 'el@@', 'te', ',', 'die', 'die', 'meisten', 'drei', 'Millionen', 'Jahren', 'der', 'L@@', 'age', 'der', 'letzten', 'drei', 'Millionen', 'Jahren', 'in', 'der', 'L@@', 'age', ',', 'die', 'S@@', 'ter@@', 'un@@', 'k', 'von', '40', '%', '.', '</s>']
2024-05-02 09:05:02,071 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 09:05:02,071 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 09:05:02,071 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr ich diese zwei Folgendes, die Tat, die Arktischen E-E-E-E-Felte, die die meisten drei Millionen Jahren der Lage der letzten drei Millionen Jahren in der Lage, die Sterunk von 40%.
2024-05-02 09:05:02,071 - INFO - joeynmt.training - Example #1
2024-05-02 09:05:02,072 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 09:05:02,072 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 09:05:02,072 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'Er@@', 'n@@', 'ähr@@', 'ungs@@', 'weise', 'der', 'Er@@', 'n@@', 'ähr@@', 'ung', 'dieser', 'spezi@@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'T@@', 'ick@@', 'n@@', 'ess', 'der', 'E@@', 'is', '.', '</s>']
2024-05-02 09:05:02,072 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 09:05:02,072 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 09:05:02,072 - INFO - joeynmt.training - 	Hypothesis: Aber das Ernährungsweise der Ernährung dieser spezielle Problem, weil es nicht die Tickness der Eis.
2024-05-02 09:05:02,072 - INFO - joeynmt.training - Example #2
2024-05-02 09:05:02,072 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 09:05:02,072 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 09:05:02,073 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'z@@', 'it@@', 'ische', 'E@@', 'is@@', '-@@', 'Z@@', 'a@@', 'p', 'ist', ',', 'in', 'einem', 'Sin@@', 'n', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 09:05:02,073 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 09:05:02,073 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 09:05:02,073 - INFO - joeynmt.training - 	Hypothesis: Die Arzitische Eis-Zap ist, in einem Sinn des globalen Klimawandel.
2024-05-02 09:05:02,073 - INFO - joeynmt.training - Example #3
2024-05-02 09:05:02,073 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 09:05:02,073 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 09:05:02,073 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'weiter@@', 't', 'in', 'den', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'ta@@', 'kt', '.', '</s>']
2024-05-02 09:05:02,074 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 09:05:02,074 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 09:05:02,074 - INFO - joeynmt.training - 	Hypothesis: Es erweitert in den Sommer und Kontakt.
2024-05-02 09:05:02,074 - INFO - joeynmt.training - Example #4
2024-05-02 09:05:02,074 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 09:05:02,074 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 09:05:02,074 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'ich', 'werde', 'Ihnen', 'ein', 'R@@', 'i@@', 'eb@@', 'en@@', 'st@@', '-@@', 'F@@', 'ast@@', '-@@', 'K@@', 'l@@', 'age', 'sein', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', '.', '</s>']
2024-05-02 09:05:02,075 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 09:05:02,075 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 09:05:02,075 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, ich werde Ihnen ein Riebenst-Fast-Klage sein, was in den letzten 25 Jahren passiert.
2024-05-02 09:05:38,988 - INFO - joeynmt.training - Epoch   2, Step:     8100, Batch Loss:     1.800792, Batch Acc: 0.474012, Tokens per Sec:     1996, Lr: 0.000300
2024-05-02 09:06:15,733 - INFO - joeynmt.training - Epoch   2, Step:     8200, Batch Loss:     1.896249, Batch Acc: 0.468685, Tokens per Sec:     2005, Lr: 0.000300
2024-05-02 09:06:52,466 - INFO - joeynmt.training - Epoch   2, Step:     8300, Batch Loss:     1.972541, Batch Acc: 0.473547, Tokens per Sec:     2011, Lr: 0.000300
2024-05-02 09:07:30,566 - INFO - joeynmt.training - Epoch   2, Step:     8400, Batch Loss:     1.850426, Batch Acc: 0.471860, Tokens per Sec:     1945, Lr: 0.000300
2024-05-02 09:08:07,447 - INFO - joeynmt.training - Epoch   2, Step:     8500, Batch Loss:     1.834928, Batch Acc: 0.475834, Tokens per Sec:     2066, Lr: 0.000300
2024-05-02 09:08:07,447 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 09:08:07,447 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 09:10:23,597 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.94, ppl:   6.98, acc:   0.46, generation: 136.0414[sec], evaluation: 0.0000[sec]
2024-05-02 09:10:23,599 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 09:10:23,802 - INFO - joeynmt.helpers - delete models/bpe_4k/6000.ckpt
2024-05-02 09:10:23,805 - INFO - joeynmt.training - Example #0
2024-05-02 09:10:23,806 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 09:10:23,806 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 09:10:23,806 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'en@@', 'st@@', 'off', ',', 'so', 'dass', 'die', 'Ar@@', 'z@@', 't', ',', 'die', 'Ar@@', 'z@@', 't', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'L@@', 'etz@@', 'e', 'von', '40', 'Prozent', 'der', 'R@@', 'ol@@', 'le', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 09:10:23,806 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 09:10:23,806 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 09:10:23,806 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dienstoff, so dass die Arzt, die Arzt, die für die meisten drei Millionen Jahre lang die Größe der letzten drei Millionen Jahre lang der Letze von 40 Prozent der Rolle von 40 Prozent.
2024-05-02 09:10:23,806 - INFO - joeynmt.training - Example #1
2024-05-02 09:10:23,807 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 09:10:23,807 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 09:10:23,807 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'ver@@', 'stan@@', 'den', 'das', 'ern@@', 'st', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'T@@', 'ü@@', 'r', 'nicht', 'die', 'T@@', 'ü@@', 'r', 'des', 'E@@', 'is', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 09:10:23,807 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 09:10:23,807 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 09:10:23,807 - INFO - joeynmt.training - 	Hypothesis: Aber diese verstanden das ernst dieses spezielle Problem, weil es nicht die Tür nicht die Tür des Eis des Eis.
2024-05-02 09:10:23,807 - INFO - joeynmt.training - Example #2
2024-05-02 09:10:23,807 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 09:10:23,807 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 09:10:23,807 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'z@@', 'z@@', 't', 'ist', ',', 'in', 'einem', 'Sin@@', 'n', ',', 'in', 'einem', 'Sin@@', 'n', ',', 'das', 'Her@@', 'z', 'des', 'glob@@', 'et@@', '-@@', 'Kli@@', 'ma@@', 'ma@@', '-@@', 'Kli@@', 'ma@@', 'ma@@', '-@@', 'System', '.', '</s>']
2024-05-02 09:10:23,808 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 09:10:23,808 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 09:10:23,808 - INFO - joeynmt.training - 	Hypothesis: Die Arzzt ist, in einem Sinn, in einem Sinn, das Herz des globet-Klimama-Klimama-System.
2024-05-02 09:10:23,808 - INFO - joeynmt.training - Example #3
2024-05-02 09:10:23,808 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 09:10:23,808 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 09:10:23,808 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'schein@@', 't', 'in', 'F@@', 'en@@', 'ter', 'und', 'ver@@', 'fol@@', 'gen', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 09:10:23,809 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 09:10:23,809 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 09:10:23,809 - INFO - joeynmt.training - 	Hypothesis: Es erscheint in Fenter und verfolgen in Sommer.
2024-05-02 09:10:23,809 - INFO - joeynmt.training - Example #4
2024-05-02 09:10:23,809 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 09:10:23,809 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 09:10:23,809 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'ich', 'werde', 'Ihnen', 'ein', 'R@@', 'i@@', 'eb@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@']
2024-05-02 09:10:23,809 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 09:10:23,810 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 09:10:23,810 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, ich werde Ihnen ein Rieb-Fast-Fast-Fast-Fast-Fast-Fast-Fast-Fast-Fast-Fast-Fast-Fast-Fast-Fast-Fast-Fast-Fast-Fast-Fast-Fast-Fast-Fast-Fast-Fast-Fast-Fast-Fast-
2024-05-02 09:11:01,020 - INFO - joeynmt.training - Epoch   2, Step:     8600, Batch Loss:     1.735270, Batch Acc: 0.476744, Tokens per Sec:     1970, Lr: 0.000300
2024-05-02 09:11:12,829 - INFO - joeynmt.training - Epoch   2: total training loss 8806.00
2024-05-02 09:11:12,829 - INFO - joeynmt.training - EPOCH 3
2024-05-02 09:11:38,390 - INFO - joeynmt.training - Epoch   3, Step:     8700, Batch Loss:     1.711168, Batch Acc: 0.494648, Tokens per Sec:     1948, Lr: 0.000300
2024-05-02 09:12:15,220 - INFO - joeynmt.training - Epoch   3, Step:     8800, Batch Loss:     1.861500, Batch Acc: 0.494112, Tokens per Sec:     2013, Lr: 0.000300
2024-05-02 09:12:52,764 - INFO - joeynmt.training - Epoch   3, Step:     8900, Batch Loss:     1.929208, Batch Acc: 0.488984, Tokens per Sec:     2009, Lr: 0.000300
2024-05-02 09:13:30,061 - INFO - joeynmt.training - Epoch   3, Step:     9000, Batch Loss:     1.903888, Batch Acc: 0.492775, Tokens per Sec:     1995, Lr: 0.000300
2024-05-02 09:13:30,061 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 09:13:30,061 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 09:15:20,984 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.92, ppl:   6.85, acc:   0.46, generation: 110.8132[sec], evaluation: 0.0000[sec]
2024-05-02 09:15:20,985 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 09:15:21,189 - INFO - joeynmt.helpers - delete models/bpe_4k/6500.ckpt
2024-05-02 09:15:21,192 - INFO - joeynmt.training - Example #0
2024-05-02 09:15:21,193 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 09:15:21,193 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 09:15:21,193 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'en@@', 'st@@', 'off', ',', 'die', 'die', 'Ar@@', 'kt@@', 'is', ',', 'die', 'Ar@@', 'kt@@', 'is', 'k@@', 'et@@', 'te', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', ',', 'die', 'die', '4@@', '8', 'Stat@@', 'en', ',', 'hat', 'sich', 'die', 'Größ@@', 'e', 'von', '40', '%', '.', '</s>']
2024-05-02 09:15:21,193 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 09:15:21,193 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 09:15:21,193 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dienstoff, die die Arktis, die Arktis kette, die für die meisten drei Millionen Jahre lang die Größe der letzten drei Millionen Jahre lang, die die 48 Staten, hat sich die Größe von 40%.
2024-05-02 09:15:21,193 - INFO - joeynmt.training - Example #1
2024-05-02 09:15:21,194 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 09:15:21,194 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 09:15:21,194 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'der', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'T@@', 'ü@@', 'r', 'der', 'E@@', 'is', '.', '</s>']
2024-05-02 09:15:21,194 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 09:15:21,194 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 09:15:21,194 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung der ernsthaft dieses Problem, weil es nicht die Tür der Eis.
2024-05-02 09:15:21,194 - INFO - joeynmt.training - Example #2
2024-05-02 09:15:21,195 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 09:15:21,195 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 09:15:21,195 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'is', 'ist', 'ist', ',', 'in', 'einem', 'Sin@@', 'n', ',', 'der', 'Her@@', 'z@@', '-@@', 'K@@', 'lim@@', 'a', ',', 'das', 'Her@@', 'z@@', '-@@', 'System', '.', '</s>']
2024-05-02 09:15:21,195 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 09:15:21,195 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 09:15:21,195 - INFO - joeynmt.training - 	Hypothesis: Die Arktis ist ist, in einem Sinn, der Herz-Klima, das Herz-System.
2024-05-02 09:15:21,195 - INFO - joeynmt.training - Example #3
2024-05-02 09:15:21,195 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 09:15:21,195 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 09:15:21,195 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'weiter@@', 't', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'ta@@', 'kt', 'in', 'S@@', 'omm@@', 'er', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 09:15:21,196 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 09:15:21,196 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 09:15:21,196 - INFO - joeynmt.training - 	Hypothesis: Es erweitert in Sommer und Kontakt in Sommer Sommer.
2024-05-02 09:15:21,196 - INFO - joeynmt.training - Example #4
2024-05-02 09:15:21,196 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 09:15:21,196 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 09:15:21,196 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'ich', 'werde', 'Ihnen', 'ein', 'R@@', 'i@@', 'es@@', 'o', 'zeigen', ',', 'was', 'im', 'letzten', '25', 'Jahre', 'lang', 'ist', '.', '</s>']
2024-05-02 09:15:21,197 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 09:15:21,197 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 09:15:21,197 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, ich werde Ihnen ein Rieso zeigen, was im letzten 25 Jahre lang ist.
2024-05-02 09:15:57,569 - INFO - joeynmt.training - Epoch   3, Step:     9100, Batch Loss:     1.828191, Batch Acc: 0.492042, Tokens per Sec:     2046, Lr: 0.000300
2024-05-02 09:16:34,692 - INFO - joeynmt.training - Epoch   3, Step:     9200, Batch Loss:     1.759520, Batch Acc: 0.493711, Tokens per Sec:     1990, Lr: 0.000300
2024-05-02 09:17:12,266 - INFO - joeynmt.training - Epoch   3, Step:     9300, Batch Loss:     2.131664, Batch Acc: 0.490655, Tokens per Sec:     1977, Lr: 0.000300
2024-05-02 09:17:50,505 - INFO - joeynmt.training - Epoch   3, Step:     9400, Batch Loss:     1.973914, Batch Acc: 0.492893, Tokens per Sec:     1915, Lr: 0.000300
2024-05-02 09:18:27,580 - INFO - joeynmt.training - Epoch   3, Step:     9500, Batch Loss:     1.699149, Batch Acc: 0.498998, Tokens per Sec:     2073, Lr: 0.000300
2024-05-02 09:18:27,580 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 09:18:27,580 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 09:20:22,781 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.90, ppl:   6.69, acc:   0.47, generation: 115.0913[sec], evaluation: 0.0000[sec]
2024-05-02 09:20:22,783 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 09:20:22,989 - INFO - joeynmt.helpers - delete models/bpe_4k/7000.ckpt
2024-05-02 09:20:22,992 - INFO - joeynmt.training - Example #0
2024-05-02 09:20:22,992 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 09:20:22,993 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 09:20:22,993 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'gen@@', 'des', ',', 'so', 'dass', 'die', 'Ar@@', 'z@@', 'z@@', 'z@@', 'a@@', 'p', 'ist', ',', 'die', 'die', 'Ar@@', 'z@@', 'z@@', 't', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'der', 'L@@', 'än@@', 'd@@', 'ungs@@', '-@@', 'Stat@@', 't', ',', 'hat', 'die', 'Größ@@', 'e', '4@@', '8', '%', '.', '</s>']
2024-05-02 09:20:22,993 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 09:20:22,993 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 09:20:22,993 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Folgendes, so dass die Arzzzap ist, die die Arzzt für die meisten drei Millionen Jahre der Ländungs-Statt, hat die Größe 48%.
2024-05-02 09:20:22,993 - INFO - joeynmt.training - Example #1
2024-05-02 09:20:22,994 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 09:20:22,994 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 09:20:22,994 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ver@@', 'st@@', 'eck@@', 'te', 'das', 'ern@@', 'st', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'T@@', 'ick@@', 'n@@', 'ess', 'der', 'E@@', 'is', 'des', 'E@@', 'is', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 09:20:22,994 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 09:20:22,994 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 09:20:22,994 - INFO - joeynmt.training - 	Hypothesis: Aber das versteckte das ernst dieses Problem, weil es nicht die Tickness der Eis des Eis des Eis.
2024-05-02 09:20:22,994 - INFO - joeynmt.training - Example #2
2024-05-02 09:20:22,994 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 09:20:22,994 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 09:20:22,995 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'is', 'ist', ',', 'ist', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'das', 'Her@@', 'z@@', 'system', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wan@@', 'del', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 09:20:22,995 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 09:20:22,995 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 09:20:22,995 - INFO - joeynmt.training - 	Hypothesis: Der Arktis ist, ist in einem Sinne, der das Herzsystem des globalen Klimawandel des globalen Klimawandel.
2024-05-02 09:20:22,995 - INFO - joeynmt.training - Example #3
2024-05-02 09:20:22,995 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 09:20:22,995 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 09:20:22,995 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'weiter@@', 't', 'in', 'S@@', 'omm@@', 'er', 'und', 'In@@', 'hal@@', 't', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 09:20:22,996 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 09:20:22,996 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 09:20:22,996 - INFO - joeynmt.training - 	Hypothesis: Es erweitert in Sommer und Inhalt in Sommer.
2024-05-02 09:20:22,996 - INFO - joeynmt.training - Example #4
2024-05-02 09:20:22,996 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 09:20:22,996 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 09:20:22,996 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'ich', 'zei@@', 'ge', 'Ihnen', 'zeigen', ',', 'dass', 'das', 'im', 'letzten', '25', 'Jahre', 'lang', 'lang', 'lang', 'lang', 'lang', 'lang', 'lang', 'lang', 'lang', 'ist', '.', '</s>']
2024-05-02 09:20:22,997 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 09:20:22,997 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 09:20:22,997 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, ich zeige Ihnen zeigen, dass das im letzten 25 Jahre lang lang lang lang lang lang lang lang lang ist.
2024-05-02 09:21:00,113 - INFO - joeynmt.training - Epoch   3, Step:     9600, Batch Loss:     1.627139, Batch Acc: 0.494326, Tokens per Sec:     2030, Lr: 0.000300
2024-05-02 09:21:37,749 - INFO - joeynmt.training - Epoch   3, Step:     9700, Batch Loss:     1.781117, Batch Acc: 0.502722, Tokens per Sec:     2045, Lr: 0.000300
2024-05-02 09:22:14,921 - INFO - joeynmt.training - Epoch   3, Step:     9800, Batch Loss:     1.693477, Batch Acc: 0.496530, Tokens per Sec:     2043, Lr: 0.000300
2024-05-02 09:22:53,250 - INFO - joeynmt.training - Epoch   3, Step:     9900, Batch Loss:     1.853860, Batch Acc: 0.495050, Tokens per Sec:     1958, Lr: 0.000300
2024-05-02 09:23:30,983 - INFO - joeynmt.training - Epoch   3, Step:    10000, Batch Loss:     1.861616, Batch Acc: 0.500873, Tokens per Sec:     1988, Lr: 0.000300
2024-05-02 09:23:30,984 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 09:23:30,984 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 09:25:25,061 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.90, ppl:   6.66, acc:   0.47, generation: 113.9691[sec], evaluation: 0.0000[sec]
2024-05-02 09:25:25,062 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 09:25:25,268 - INFO - joeynmt.helpers - delete models/bpe_4k/7500.ckpt
2024-05-02 09:25:25,271 - INFO - joeynmt.training - Example #0
2024-05-02 09:25:25,271 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 09:25:25,271 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 09:25:25,271 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'en@@', 'st@@', 'ad@@', 't', ',', 'so', 'dass', 'die', 'Ar@@', 'z@@', 'is', 'k@@', 'ti@@', 'e@@', 'f', ',', 'die', 'die', 'größ@@', 'te', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'L@@', 'ö@@', 'cher', ',', 'hat', 'die', '4@@', '8', 'Stat@@', 't', ',', 'hat', 'sich', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 09:25:25,271 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 09:25:25,272 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 09:25:25,272 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dienstadt, so dass die Arzis ktief, die die größte drei Millionen Jahren die Größe der letzten drei Millionen Jahre lang der Löcher, hat die 48 Statt, hat sich von 40 Prozent.
2024-05-02 09:25:25,272 - INFO - joeynmt.training - Example #1
2024-05-02 09:25:25,272 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 09:25:25,272 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 09:25:25,272 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ver@@', 'stan@@', 'den', 'das', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'spezi@@', 'es', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'T@@', 'ick@@', 'n@@', 'ess', 'der', 'E@@', 'is', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 09:25:25,272 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 09:25:25,272 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 09:25:25,273 - INFO - joeynmt.training - 	Hypothesis: Aber das verstanden das ernsthaft dieses spezies Problem, weil es nicht die Tickness der Eis des Eis.
2024-05-02 09:25:25,273 - INFO - joeynmt.training - Example #2
2024-05-02 09:25:25,273 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 09:25:25,273 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 09:25:25,273 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'z@@', 'is', 'ist', 'ein', 'ist', 'ein', 'Ge@@', 'fühl', ',', 'in', 'einem', 'Sin@@', 'n', ',', 'der', 'das', 'Her@@', 'z', 'des', 'Kli@@', 'ma@@', 'wandel@@', 's', 'des', 'Kli@@', 'ma@@', 'wandel@@', 's', '.', '</s>']
2024-05-02 09:25:25,273 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 09:25:25,273 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 09:25:25,273 - INFO - joeynmt.training - 	Hypothesis: Das Arzis ist ein ist ein Gefühl, in einem Sinn, der das Herz des Klimawandels des Klimawandels.
2024-05-02 09:25:25,273 - INFO - joeynmt.training - Example #3
2024-05-02 09:25:25,274 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 09:25:25,274 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 09:25:25,274 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'kann@@', 't', 'in', 'den', 'Win@@', 'ter', 'und', 'ver@@', 'fol@@', 'gen', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 09:25:25,274 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 09:25:25,274 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 09:25:25,274 - INFO - joeynmt.training - 	Hypothesis: Es erkannt in den Winter und verfolgen in Sommer.
2024-05-02 09:25:25,274 - INFO - joeynmt.training - Example #4
2024-05-02 09:25:25,274 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 09:25:25,274 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 09:25:25,275 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'nächste', 'Fol@@', 'ie', ',', 'ich', 'Ihnen', 'werde', 'Ihnen', 'ein', 'R@@', 'as@@', 'ik@@', 'en', ',', 'das', 'das', 'ist', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 09:25:25,275 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 09:25:25,275 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 09:25:25,275 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie, ich Ihnen werde Ihnen ein Rasiken, das das ist, was über die letzten 25 Jahren passiert ist.
2024-05-02 09:26:02,856 - INFO - joeynmt.training - Epoch   3, Step:    10100, Batch Loss:     1.815833, Batch Acc: 0.497804, Tokens per Sec:     1976, Lr: 0.000300
2024-05-02 09:26:40,784 - INFO - joeynmt.training - Epoch   3, Step:    10200, Batch Loss:     1.962936, Batch Acc: 0.499884, Tokens per Sec:     1937, Lr: 0.000300
2024-05-02 09:27:17,814 - INFO - joeynmt.training - Epoch   3, Step:    10300, Batch Loss:     1.802817, Batch Acc: 0.497323, Tokens per Sec:     1998, Lr: 0.000300
2024-05-02 09:27:55,982 - INFO - joeynmt.training - Epoch   3, Step:    10400, Batch Loss:     1.729477, Batch Acc: 0.496453, Tokens per Sec:     2002, Lr: 0.000300
2024-05-02 09:28:32,977 - INFO - joeynmt.training - Epoch   3, Step:    10500, Batch Loss:     1.941221, Batch Acc: 0.495674, Tokens per Sec:     2018, Lr: 0.000300
2024-05-02 09:28:32,977 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 09:28:32,977 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 09:30:40,420 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.87, ppl:   6.49, acc:   0.48, generation: 127.3373[sec], evaluation: 0.0000[sec]
2024-05-02 09:30:40,421 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 09:30:40,622 - INFO - joeynmt.helpers - delete models/bpe_4k/8000.ckpt
2024-05-02 09:30:40,625 - INFO - joeynmt.training - Example #0
2024-05-02 09:30:40,625 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 09:30:40,625 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 09:30:40,625 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'en@@', 'st@@', 'off', ',', 'die', 'die', 'Ar@@', 'kt@@', 'is', ',', 'die', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', ',', 'die', 'meisten', 'drei', 'Millionen', 'Jahren', ',', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahren', ',', 'die', 'Größ@@', 'e', '4@@', '8', 'Stat@@', 't', ',', 'hat', 'sich', 'in', '40', 'Prozent', '.', '</s>']
2024-05-02 09:30:40,626 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 09:30:40,626 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 09:30:40,626 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dienstoff, die die Arktis, die die meisten drei Millionen Jahre, die meisten drei Millionen Jahren, die Größe der letzten drei Millionen Jahren, die Größe 48 Statt, hat sich in 40 Prozent.
2024-05-02 09:30:40,626 - INFO - joeynmt.training - Example #1
2024-05-02 09:30:40,626 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 09:30:40,626 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 09:30:40,626 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'Ver@@', 'st@@', 'ei@@', 'gen', 'das', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'spezi@@', 'es', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'T@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 09:30:40,627 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 09:30:40,627 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 09:30:40,627 - INFO - joeynmt.training - 	Hypothesis: Aber das Versteigen das ernsthaft dieses spezies Problem, weil es nicht die Tickness des Eis.
2024-05-02 09:30:40,627 - INFO - joeynmt.training - Example #2
2024-05-02 09:30:40,627 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 09:30:40,627 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 09:30:40,627 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'c@@', 'a', ',', 'ist', 'ein', 'Ge@@', 'fühl', ',', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 09:30:40,627 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 09:30:40,627 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 09:30:40,628 - INFO - joeynmt.training - 	Hypothesis: Das Arktische Eisca, ist ein Gefühl, das Herz des globalen Klimawandel.
2024-05-02 09:30:40,628 - INFO - joeynmt.training - Example #3
2024-05-02 09:30:40,628 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 09:30:40,628 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 09:30:40,628 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'weiter@@', 't', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 09:30:40,628 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 09:30:40,628 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 09:30:40,628 - INFO - joeynmt.training - 	Hypothesis: Es erweitert in Winter und Kontrakten in Sommer.
2024-05-02 09:30:40,628 - INFO - joeynmt.training - Example #4
2024-05-02 09:30:40,629 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 09:30:40,629 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 09:30:40,629 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'ich', 'werde', 'Ihnen', 'ein', 'R@@', 'i@@', 'es@@', 'en@@', 'des', ',', 'was', 'passiert', 'passiert', ',', 'über', 'die', 'letzten', '25', 'Jahren', 'passier@@', 'te', '.', '</s>']
2024-05-02 09:30:40,629 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 09:30:40,629 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 09:30:40,629 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, ich werde Ihnen ein Riesendes, was passiert passiert, über die letzten 25 Jahren passierte.
2024-05-02 09:31:17,127 - INFO - joeynmt.training - Epoch   3, Step:    10600, Batch Loss:     1.660181, Batch Acc: 0.503815, Tokens per Sec:     1957, Lr: 0.000300
2024-05-02 09:31:54,431 - INFO - joeynmt.training - Epoch   3, Step:    10700, Batch Loss:     1.882561, Batch Acc: 0.500189, Tokens per Sec:     1986, Lr: 0.000300
2024-05-02 09:32:31,065 - INFO - joeynmt.training - Epoch   3, Step:    10800, Batch Loss:     1.888011, Batch Acc: 0.497832, Tokens per Sec:     2052, Lr: 0.000300
2024-05-02 09:33:07,777 - INFO - joeynmt.training - Epoch   3, Step:    10900, Batch Loss:     1.945673, Batch Acc: 0.498221, Tokens per Sec:     1990, Lr: 0.000300
2024-05-02 09:33:45,897 - INFO - joeynmt.training - Epoch   3, Step:    11000, Batch Loss:     1.836940, Batch Acc: 0.502422, Tokens per Sec:     1955, Lr: 0.000300
2024-05-02 09:33:45,898 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 09:33:45,898 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 09:35:39,156 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.37, acc:   0.48, generation: 113.1473[sec], evaluation: 0.0000[sec]
2024-05-02 09:35:39,158 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 09:35:39,359 - INFO - joeynmt.helpers - delete models/bpe_4k/8500.ckpt
2024-05-02 09:35:39,362 - INFO - joeynmt.training - Example #0
2024-05-02 09:35:39,362 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 09:35:39,363 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 09:35:39,363 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'en@@', 'st@@', 'lei@@', 'st@@', 'ung', ',', 'dass', 'die', 'Ar@@', 'z@@', 'z@@', 'z@@', 'a@@', 'p', ',', 'die', 'die', 'meisten', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', 'e', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', 'e', 'von', 'der', 'letzten', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', 'e', '4@@', '8', 'Stat@@', 't', ',', 'hat', 'sich', 's@@', 'ü@@', 'ge', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 09:35:39,363 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 09:35:39,363 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 09:35:39,363 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dienstleistung, dass die Arzzzap, die die meisten drei Millionen Jahren die Größe drei Millionen Jahren die Größe von der letzten drei Millionen Jahren die Größe 48 Statt, hat sich süge von 40 Prozent.
2024-05-02 09:35:39,363 - INFO - joeynmt.training - Example #1
2024-05-02 09:35:39,363 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 09:35:39,364 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 09:35:39,364 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'Unter@@', 'nehm@@', 'ung', 'des', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'spezi@@', 'es', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'T@@', 'ü@@', 'r', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 09:35:39,364 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 09:35:39,364 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 09:35:39,364 - INFO - joeynmt.training - 	Hypothesis: Aber das Unternehmung des ernsthaft dieses spezies Problem, weil es nicht die Tür des Eis.
2024-05-02 09:35:39,364 - INFO - joeynmt.training - Example #2
2024-05-02 09:35:39,364 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 09:35:39,364 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 09:35:39,364 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'is', 'ist', 'in', 'einem', 'Sin@@', 'ne', ',', 'in', 'einem', 'Sin@@', 'n', ',', 'der', 'Her@@', 'z@@', 'system', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', '-@@', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 09:35:39,365 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 09:35:39,365 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 09:35:39,365 - INFO - joeynmt.training - 	Hypothesis: Der Arktis ist in einem Sinne, in einem Sinn, der Herzsystem des globalen Klima-Klimawandel.
2024-05-02 09:35:39,365 - INFO - joeynmt.training - Example #3
2024-05-02 09:35:39,365 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 09:35:39,365 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 09:35:39,365 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'es', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 09:35:39,366 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 09:35:39,366 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 09:35:39,366 - INFO - joeynmt.training - 	Hypothesis: Es gibt es in Winter und Kontrakte in Sommer.
2024-05-02 09:35:39,366 - INFO - joeynmt.training - Example #4
2024-05-02 09:35:39,366 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 09:35:39,366 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 09:35:39,366 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'ich', 'werde', 'Ihnen', 'eine', 'Re@@', 'gel', 'f@@', 'le@@', 'gen', 'des', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'passiert', '.', '</s>']
2024-05-02 09:35:39,366 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 09:35:39,367 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 09:35:39,367 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, ich werde Ihnen eine Regel flegen des, was über die letzten 25 Jahren passiert.
2024-05-02 09:36:16,133 - INFO - joeynmt.training - Epoch   3, Step:    11100, Batch Loss:     1.842112, Batch Acc: 0.499314, Tokens per Sec:     1972, Lr: 0.000300
2024-05-02 09:36:53,571 - INFO - joeynmt.training - Epoch   3, Step:    11200, Batch Loss:     1.670463, Batch Acc: 0.508772, Tokens per Sec:     1972, Lr: 0.000300
2024-05-02 09:37:31,825 - INFO - joeynmt.training - Epoch   3, Step:    11300, Batch Loss:     1.662691, Batch Acc: 0.503635, Tokens per Sec:     1928, Lr: 0.000300
2024-05-02 09:38:09,563 - INFO - joeynmt.training - Epoch   3, Step:    11400, Batch Loss:     1.940905, Batch Acc: 0.495528, Tokens per Sec:     1955, Lr: 0.000300
2024-05-02 09:38:46,385 - INFO - joeynmt.training - Epoch   3, Step:    11500, Batch Loss:     1.869644, Batch Acc: 0.503575, Tokens per Sec:     2005, Lr: 0.000300
2024-05-02 09:38:46,385 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 09:38:46,385 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 09:40:55,871 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.84, ppl:   6.29, acc:   0.48, generation: 129.3790[sec], evaluation: 0.0000[sec]
2024-05-02 09:40:55,872 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 09:40:56,067 - INFO - joeynmt.helpers - delete models/bpe_4k/9000.ckpt
2024-05-02 09:40:56,070 - INFO - joeynmt.training - Example #0
2024-05-02 09:40:56,071 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 09:40:56,071 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 09:40:56,071 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Di@@', 'ä@@', 't', 'ge@@', 'zeigt', ',', 'dass', 'die', 'Ar@@', 'z@@', 't', 'ist', ',', 'die', 'die', 'm@@', 'ä@@', 'ß@@', 'ig@@', 'sten', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', 'e', 'von', 'den', 'letzten', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', 'e', '4@@', '8', 'Stat@@', 't', ',', 'hat', 'sich', 'die', 'Größ@@', 'e', '4@@', '8', 'Stat@@', 't', '.', '</s>']
2024-05-02 09:40:56,071 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 09:40:56,071 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 09:40:56,071 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Diät gezeigt, dass die Arzt ist, die die mäßigsten drei Millionen Jahren die Größe von den letzten drei Millionen Jahren die Größe 48 Statt, hat sich die Größe 48 Statt.
2024-05-02 09:40:56,071 - INFO - joeynmt.training - Example #1
2024-05-02 09:40:56,072 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 09:40:56,072 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 09:40:56,072 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ver@@', 'stan@@', 'den', 'den', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'T@@', 'ick@@', 'n@@', 'ess', 'der', 'E@@', 'is', '.', '</s>']
2024-05-02 09:40:56,072 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 09:40:56,072 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 09:40:56,072 - INFO - joeynmt.training - 	Hypothesis: Aber das verstanden den ernsthaft dieses Problem, weil es nicht die Tickness der Eis.
2024-05-02 09:40:56,072 - INFO - joeynmt.training - Example #2
2024-05-02 09:40:56,072 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 09:40:56,072 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 09:40:56,072 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'z@@', 'z@@', 'z@@', 'a@@', 'p', 'ist', ',', 'in', 'einem', 'Sin@@', 'n', ',', 'der', 'Her@@', 'z@@', 'system', 'des', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 09:40:56,073 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 09:40:56,073 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 09:40:56,073 - INFO - joeynmt.training - 	Hypothesis: Das Arzzzap ist, in einem Sinn, der Herzsystem des Klimawandel.
2024-05-02 09:40:56,073 - INFO - joeynmt.training - Example #3
2024-05-02 09:40:56,073 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 09:40:56,073 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 09:40:56,073 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'weiter@@', 'n', 'in', 'den', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ts', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 09:40:56,074 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 09:40:56,074 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 09:40:56,074 - INFO - joeynmt.training - 	Hypothesis: Es erweitern in den Winter und Kontrakts in Sommer.
2024-05-02 09:40:56,074 - INFO - joeynmt.training - Example #4
2024-05-02 09:40:56,074 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 09:40:56,074 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 09:40:56,074 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'ich', 'werde', 'Ihnen', 'zeigen', ',', 'dass', 'Sie', 'ein', 'R@@', 'as@@', 'ik@@', 'en', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', '.', '</s>']
2024-05-02 09:40:56,074 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 09:40:56,075 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 09:40:56,075 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, ich werde Ihnen zeigen, dass Sie ein Rasiken, was in den letzten 25 Jahren passiert.
2024-05-02 09:41:32,071 - INFO - joeynmt.training - Epoch   3, Step:    11600, Batch Loss:     1.711153, Batch Acc: 0.504037, Tokens per Sec:     2036, Lr: 0.000300
2024-05-02 09:42:09,887 - INFO - joeynmt.training - Epoch   3, Step:    11700, Batch Loss:     1.906499, Batch Acc: 0.506815, Tokens per Sec:     2008, Lr: 0.000300
2024-05-02 09:42:48,493 - INFO - joeynmt.training - Epoch   3, Step:    11800, Batch Loss:     1.720590, Batch Acc: 0.507302, Tokens per Sec:     1930, Lr: 0.000300
2024-05-02 09:43:26,147 - INFO - joeynmt.training - Epoch   3, Step:    11900, Batch Loss:     1.943681, Batch Acc: 0.505968, Tokens per Sec:     2007, Lr: 0.000300
2024-05-02 09:44:03,453 - INFO - joeynmt.training - Epoch   3, Step:    12000, Batch Loss:     2.042568, Batch Acc: 0.507123, Tokens per Sec:     2002, Lr: 0.000300
2024-05-02 09:44:03,454 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 09:44:03,454 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 09:46:21,459 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.83, ppl:   6.22, acc:   0.49, generation: 137.8942[sec], evaluation: 0.0000[sec]
2024-05-02 09:46:21,461 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 09:46:21,667 - INFO - joeynmt.helpers - delete models/bpe_4k/9500.ckpt
2024-05-02 09:46:21,670 - INFO - joeynmt.training - Example #0
2024-05-02 09:46:21,670 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 09:46:21,671 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 09:46:21,671 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Di@@', 'as', 'ge@@', 'zeigt', ',', 'so', 'dass', 'die', 'Ar@@', 'z@@', 't', 'ist', ',', 'dass', 'die', 'Ar@@', 'z@@', 't', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', 'e', 'der', 'L@@', 'är@@', 'ker', '4@@', '8', 'Stat@@', 't', 'hat', ',', 'hat', 'sich', 'die', 'Größ@@', 'en@@', 'ord@@', 'nung', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 09:46:21,671 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 09:46:21,671 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 09:46:21,671 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Dias gezeigt, so dass die Arzt ist, dass die Arzt, die für die meisten drei Millionen Jahren die Größe der Lärker 48 Statt hat, hat sich die Größenordnung von 40 Prozent.
2024-05-02 09:46:21,671 - INFO - joeynmt.training - Example #1
2024-05-02 09:46:21,671 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 09:46:21,672 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 09:46:21,672 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'Unter@@', 'stat@@', 't@@', 'es', 'das', 'S@@', 'tim@@', 'me', 'dieses', 'Problem', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'T@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is@@', 's@@', 'eines', 'E@@', 'is@@', 's', '.', '</s>']
2024-05-02 09:46:21,672 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 09:46:21,672 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 09:46:21,672 - INFO - joeynmt.training - 	Hypothesis: Aber das Unterstattes das Stimme dieses Problem dieses Problem, weil es nicht die Tickness des Eisseines Eiss.
2024-05-02 09:46:21,672 - INFO - joeynmt.training - Example #2
2024-05-02 09:46:21,672 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 09:46:21,672 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 09:46:21,672 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'c@@', 'a', 'ist', ',', 'in', 'einem', 'Sin@@', 'n', ',', 'der', 'das', 'Her@@', 'z@@', 'system', 'des', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 09:46:21,673 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 09:46:21,673 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 09:46:21,673 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eisca ist, in einem Sinn, der das Herzsystem des Klimawandel.
2024-05-02 09:46:21,673 - INFO - joeynmt.training - Example #3
2024-05-02 09:46:21,673 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 09:46:21,673 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 09:46:21,673 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 09:46:21,674 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 09:46:21,674 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 09:46:21,674 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Sommer und Kontrakten in Sommer.
2024-05-02 09:46:21,674 - INFO - joeynmt.training - Example #4
2024-05-02 09:46:21,674 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 09:46:21,674 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 09:46:21,674 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ge', ',', 'die', 'ich', 'Ihnen', 'eine', 'R@@', 'ra@@', 'pi@@', 'de', 'zeigen', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 09:46:21,675 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 09:46:21,675 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 09:46:21,675 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folge, die ich Ihnen eine Rrapide zeigen, was in den letzten 25 Jahren passiert ist.
2024-05-02 09:47:02,803 - INFO - joeynmt.training - Epoch   3, Step:    12100, Batch Loss:     1.801736, Batch Acc: 0.508908, Tokens per Sec:     1872, Lr: 0.000300
2024-05-02 09:47:40,746 - INFO - joeynmt.training - Epoch   3, Step:    12200, Batch Loss:     1.679112, Batch Acc: 0.506676, Tokens per Sec:     1984, Lr: 0.000300
2024-05-02 09:48:17,974 - INFO - joeynmt.training - Epoch   3, Step:    12300, Batch Loss:     1.690649, Batch Acc: 0.509776, Tokens per Sec:     1981, Lr: 0.000300
2024-05-02 09:48:56,671 - INFO - joeynmt.training - Epoch   3, Step:    12400, Batch Loss:     1.738581, Batch Acc: 0.509081, Tokens per Sec:     1969, Lr: 0.000300
2024-05-02 09:49:34,677 - INFO - joeynmt.training - Epoch   3, Step:    12500, Batch Loss:     1.651612, Batch Acc: 0.510326, Tokens per Sec:     1989, Lr: 0.000300
2024-05-02 09:49:34,677 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 09:49:34,677 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 09:51:28,323 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.15, acc:   0.49, generation: 113.5408[sec], evaluation: 0.0000[sec]
2024-05-02 09:51:28,325 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 09:51:28,530 - INFO - joeynmt.helpers - delete models/bpe_4k/10000.ckpt
2024-05-02 09:51:28,532 - INFO - joeynmt.training - Example #0
2024-05-02 09:51:28,533 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 09:51:28,533 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 09:51:28,533 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'gen', ',', 'so', 'dass', 'die', 'Ar@@', 'kt@@', 'is', 'k@@', 'et@@', 'te', 'K@@', 'a@@', 'p', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'war', ',', 'hat', 'sich', 'S@@', 'hr@@', 'un@@', 'k', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 09:51:28,533 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 09:51:28,533 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 09:51:28,533 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Folgen, so dass die Arktis kette Kap, die für die meisten drei Millionen Jahre die Größe der Größe der Größe der letzten drei Millionen Jahre war, hat sich Shrunk von 40 Prozent.
2024-05-02 09:51:28,534 - INFO - joeynmt.training - Example #1
2024-05-02 09:51:28,534 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 09:51:28,534 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 09:51:28,534 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ver@@', 'stan@@', 'den', 'die', 'Er@@', 'n@@', 'ähr@@', 'ung', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'T@@', 'ü@@', 'r', 'der', 'E@@', 'is@@', 'n@@', 'ess', 'des', 'E@@', 'is@@', 's@@', 'th@@', 'ü@@', 'll@@', 'en', '.', '</s>']
2024-05-02 09:51:28,534 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 09:51:28,534 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 09:51:28,534 - INFO - joeynmt.training - 	Hypothesis: Aber das verstanden die Ernährung dieses Problem, weil es nicht die Tür der Eisness des Eissthüllen.
2024-05-02 09:51:28,534 - INFO - joeynmt.training - Example #2
2024-05-02 09:51:28,535 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 09:51:28,535 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 09:51:28,535 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'kt@@', 'is', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'das', 'Her@@', 'z@@', 'sch@@', 'la@@', 'g', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'k@@', 'lim@@', 'a', '.', '</s>']
2024-05-02 09:51:28,535 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 09:51:28,535 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 09:51:28,535 - INFO - joeynmt.training - 	Hypothesis: Das Arktis ist, in einem Sinne, der das Herzschlag des globalen Klimaklima.
2024-05-02 09:51:28,535 - INFO - joeynmt.training - Example #3
2024-05-02 09:51:28,535 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 09:51:28,535 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 09:51:28,535 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'sich', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 09:51:28,536 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 09:51:28,536 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 09:51:28,536 - INFO - joeynmt.training - 	Hypothesis: Es gibt sich in Sommer und Kontrakten in Sommer.
2024-05-02 09:51:28,536 - INFO - joeynmt.training - Example #4
2024-05-02 09:51:28,536 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 09:51:28,536 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 09:51:28,536 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'das', 'ich', 'Ihnen', 'ein', 'R@@', 'ra@@', 'pi@@', 'd', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahre', 'gesch@@', 'a@@', 'h', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahre', '.', '</s>']
2024-05-02 09:51:28,537 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 09:51:28,537 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 09:51:28,537 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, das ich Ihnen ein Rrapid, was in den letzten 25 Jahre geschah, was über die letzten 25 Jahre.
2024-05-02 09:52:05,299 - INFO - joeynmt.training - Epoch   3, Step:    12600, Batch Loss:     1.828775, Batch Acc: 0.510198, Tokens per Sec:     2023, Lr: 0.000300
2024-05-02 09:52:42,959 - INFO - joeynmt.training - Epoch   3, Step:    12700, Batch Loss:     1.775828, Batch Acc: 0.509935, Tokens per Sec:     1973, Lr: 0.000300
2024-05-02 09:53:20,469 - INFO - joeynmt.training - Epoch   3, Step:    12800, Batch Loss:     1.804516, Batch Acc: 0.512604, Tokens per Sec:     1997, Lr: 0.000300
2024-05-02 09:53:58,082 - INFO - joeynmt.training - Epoch   3, Step:    12900, Batch Loss:     1.839625, Batch Acc: 0.507337, Tokens per Sec:     1979, Lr: 0.000300
2024-05-02 09:54:13,581 - INFO - joeynmt.training - Epoch   3: total training loss 7752.84
2024-05-02 09:54:13,581 - INFO - joeynmt.training - EPOCH 4
2024-05-02 09:54:35,075 - INFO - joeynmt.training - Epoch   4, Step:    13000, Batch Loss:     1.697134, Batch Acc: 0.530340, Tokens per Sec:     2003, Lr: 0.000300
2024-05-02 09:54:35,075 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 09:54:35,075 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 09:56:19,852 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.79, ppl:   6.01, acc:   0.50, generation: 104.6680[sec], evaluation: 0.0000[sec]
2024-05-02 09:56:19,853 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 09:56:20,067 - INFO - joeynmt.helpers - delete models/bpe_4k/10500.ckpt
2024-05-02 09:56:20,070 - INFO - joeynmt.training - Example #0
2024-05-02 09:56:20,070 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 09:56:20,070 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 09:56:20,070 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'en@@', 'st@@', 'lei@@', 'st@@', 'ung', ',', 'so', 'dass', 'die', 'Ar@@', 'z@@', 'a@@', 'p', 'E@@', 'is', 'k@@', 'et@@', 'te', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', 'e', 'der', '4@@', '8', 'Stat@@', 'en', ',', 'hat', 'sich', 'S@@', 'ü@@', 'ge', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 09:56:20,070 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 09:56:20,071 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 09:56:20,071 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dienstleistung, so dass die Arzap Eis kette, die für die meisten drei Millionen Jahre die Größe der letzten drei Millionen Jahren die Größe der 48 Staten, hat sich Süge von 40 Prozent.
2024-05-02 09:56:20,071 - INFO - joeynmt.training - Example #1
2024-05-02 09:56:20,071 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 09:56:20,071 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 09:56:20,071 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', ',', 'das', 'ern@@', 'st', 'das', 'Problem', 'des', 'E@@', 'is', 'des', 'E@@', 'is', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 09:56:20,071 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 09:56:20,071 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 09:56:20,071 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung, das ernst das Problem des Eis des Eis des Eis.
2024-05-02 09:56:20,072 - INFO - joeynmt.training - Example #2
2024-05-02 09:56:20,072 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 09:56:20,072 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 09:56:20,072 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'z@@', 't', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'im', 'Sin@@', 'ne', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 09:56:20,072 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 09:56:20,072 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 09:56:20,072 - INFO - joeynmt.training - 	Hypothesis: Das Arzt ist, in einem Sinne, im Sinne des globalen Klimawandel.
2024-05-02 09:56:20,072 - INFO - joeynmt.training - Example #3
2024-05-02 09:56:20,073 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 09:56:20,073 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 09:56:20,073 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'S@@', 'omm@@', 'er', 'und', 'Sch@@', 'w@@', 'einen', '.', '</s>']
2024-05-02 09:56:20,073 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 09:56:20,073 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 09:56:20,073 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Sommer und Schweinen.
2024-05-02 09:56:20,073 - INFO - joeynmt.training - Example #4
2024-05-02 09:56:20,073 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 09:56:20,073 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 09:56:20,073 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'zeigen', 'werde', ',', 'dass', 'Sie', 'ein', 'R@@', 'i@@', 'es@@', 'en@@', 'f@@', 'ör@@', 'm@@', 'lich', 'ist', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'gesch@@', 'a@@', 'h', '.', '</s>']
2024-05-02 09:56:20,074 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 09:56:20,074 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 09:56:20,074 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich zeigen werde, dass Sie ein Riesenförmlich ist, was über die letzten 25 Jahren geschah.
2024-05-02 09:56:57,851 - INFO - joeynmt.training - Epoch   4, Step:    13100, Batch Loss:     1.799930, Batch Acc: 0.530084, Tokens per Sec:     1950, Lr: 0.000300
2024-05-02 09:57:35,161 - INFO - joeynmt.training - Epoch   4, Step:    13200, Batch Loss:     1.610836, Batch Acc: 0.532083, Tokens per Sec:     1986, Lr: 0.000300
2024-05-02 09:58:12,433 - INFO - joeynmt.training - Epoch   4, Step:    13300, Batch Loss:     1.602305, Batch Acc: 0.524008, Tokens per Sec:     2030, Lr: 0.000300
2024-05-02 09:58:50,890 - INFO - joeynmt.training - Epoch   4, Step:    13400, Batch Loss:     1.666982, Batch Acc: 0.535733, Tokens per Sec:     1965, Lr: 0.000300
2024-05-02 09:59:28,663 - INFO - joeynmt.training - Epoch   4, Step:    13500, Batch Loss:     1.661397, Batch Acc: 0.526368, Tokens per Sec:     1974, Lr: 0.000300
2024-05-02 09:59:28,664 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 09:59:28,664 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 10:01:35,238 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.79, ppl:   6.00, acc:   0.50, generation: 126.4656[sec], evaluation: 0.0000[sec]
2024-05-02 10:01:35,240 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 10:01:35,446 - INFO - joeynmt.helpers - delete models/bpe_4k/11000.ckpt
2024-05-02 10:01:35,449 - INFO - joeynmt.training - Example #0
2024-05-02 10:01:35,449 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 10:01:35,449 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 10:01:35,449 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'en', 'ge@@', 'zeigt', ',', 'so', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is', 'C@@', 'a@@', 'p', ',', 'das', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'in', 'den', 'letzten', 'drei', 'Millionen', 'Jahre', 'in', 'der', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'aten', ',', 'hat', 'sich', 'die', '1@@', '8', 'Sta@@', 'aten', ',', 'die', 'Son@@', 'nen@@', 'st@@', 'ec@@', 'ken', ',', 'mit', '40', 'Prozent', '.', '</s>']
2024-05-02 10:01:35,450 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 10:01:35,450 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 10:01:35,450 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dien gezeigt, so dass die arktischen Eis Cap, das die meisten drei Millionen Jahre in den letzten drei Millionen Jahre in der Größe 48 Staaten, hat sich die 18 Staaten, die Sonnenstecken, mit 40 Prozent.
2024-05-02 10:01:35,450 - INFO - joeynmt.training - Example #1
2024-05-02 10:01:35,450 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 10:01:35,450 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 10:01:35,450 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'des', 'S@@', 'or@@', 'gen', 'dieses', 'Problem', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'T@@', 'ü@@', 'r', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 10:01:35,451 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 10:01:35,451 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 10:01:35,451 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung des Sorgen dieses Problem dieses Problem, weil es nicht die Tür des Eis.
2024-05-02 10:01:35,451 - INFO - joeynmt.training - Example #2
2024-05-02 10:01:35,451 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 10:01:35,451 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 10:01:35,451 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'is', 'ist', 'ein', ',', 'in', 'einem', 'Sin@@', 'n', ',', 'der', 'Her@@', 'z@@', 'sch@@', 'la@@', 'g', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 10:01:35,451 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 10:01:35,451 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 10:01:35,452 - INFO - joeynmt.training - 	Hypothesis: Der Arktis ist ein, in einem Sinn, der Herzschlag des globalen Klimawandel.
2024-05-02 10:01:35,452 - INFO - joeynmt.training - Example #3
2024-05-02 10:01:35,452 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 10:01:35,452 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 10:01:35,452 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'S@@', 'omm@@', 'er', 'und', 'Sch@@', 'w@@', 'einen', '.', '</s>']
2024-05-02 10:01:35,452 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 10:01:35,452 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 10:01:35,452 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Sommer und Schweinen.
2024-05-02 10:01:35,452 - INFO - joeynmt.training - Example #4
2024-05-02 10:01:35,453 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 10:01:35,453 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 10:01:35,453 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'ein', 'R@@', 'ra@@', 'h@@', 't', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', '.', '</s>']
2024-05-02 10:01:35,453 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 10:01:35,453 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 10:01:35,453 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen, ein Rraht, was in den letzten 25 Jahren passiert.
2024-05-02 10:02:13,191 - INFO - joeynmt.training - Epoch   4, Step:    13600, Batch Loss:     1.626020, Batch Acc: 0.531163, Tokens per Sec:     1977, Lr: 0.000300
2024-05-02 10:02:50,908 - INFO - joeynmt.training - Epoch   4, Step:    13700, Batch Loss:     1.652127, Batch Acc: 0.528431, Tokens per Sec:     1989, Lr: 0.000300
2024-05-02 10:03:29,372 - INFO - joeynmt.training - Epoch   4, Step:    13800, Batch Loss:     1.693360, Batch Acc: 0.529521, Tokens per Sec:     1965, Lr: 0.000300
2024-05-02 10:04:06,357 - INFO - joeynmt.training - Epoch   4, Step:    13900, Batch Loss:     1.720128, Batch Acc: 0.531897, Tokens per Sec:     2064, Lr: 0.000300
2024-05-02 10:04:42,717 - INFO - joeynmt.training - Epoch   4, Step:    14000, Batch Loss:     1.686374, Batch Acc: 0.530621, Tokens per Sec:     2062, Lr: 0.000300
2024-05-02 10:04:42,717 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 10:04:42,717 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 10:06:31,143 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.78, ppl:   5.96, acc:   0.50, generation: 108.3203[sec], evaluation: 0.0000[sec]
2024-05-02 10:06:31,144 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 10:06:31,357 - INFO - joeynmt.helpers - delete models/bpe_4k/11500.ckpt
2024-05-02 10:06:31,360 - INFO - joeynmt.training - Example #0
2024-05-02 10:06:31,360 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 10:06:31,360 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 10:06:31,360 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'en@@', 'st@@', 'ad@@', 't', ',', 'so', 'dass', 'der', 'Ar@@', 'z@@', 'is', 'C@@', 't@@', 'is', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahren', 'der', 'L@@', 'age', 'war', ',', 'die', 'Größ@@', 'e', 'der', 'N@@', 'ie@@', 'der@@', 'sch@@', 'r@@', 'än@@', 'k@@', 'ungen', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 10:06:31,361 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 10:06:31,361 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 10:06:31,361 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dienstadt, so dass der Arzis Ctis für die meisten letzten drei Millionen Jahren die Größe der letzten drei Millionen Jahren der Lage war, die Größe der Niederschränkungen von 40 Prozent.
2024-05-02 10:06:31,361 - INFO - joeynmt.training - Example #1
2024-05-02 10:06:31,361 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 10:06:31,361 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 10:06:31,361 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'dieses', 'spezi@@', 'elle', 'Problem', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'T@@', 'ü@@', 'r', 'der', 'E@@', 'is', '.', '</s>']
2024-05-02 10:06:31,362 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 10:06:31,362 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 10:06:31,362 - INFO - joeynmt.training - 	Hypothesis: Aber das Unterstützung dieses spezielle Problem dieses spezielle Problem, weil es nicht die Tür der Eis.
2024-05-02 10:06:31,362 - INFO - joeynmt.training - Example #2
2024-05-02 10:06:31,362 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 10:06:31,362 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 10:06:31,362 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'z@@', 'z@@', 'z@@', 'z@@', 'et@@', 'ische', 'E@@', 'is@@', 'en@@', 'system', 'ist', ',', 'der', 'Bi@@', 'en@@', 'system', '.', '</s>']
2024-05-02 10:06:31,363 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 10:06:31,363 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 10:06:31,363 - INFO - joeynmt.training - 	Hypothesis: Das Arzzzzetische Eisensystem ist, der Biensystem.
2024-05-02 10:06:31,363 - INFO - joeynmt.training - Example #3
2024-05-02 10:06:31,363 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 10:06:31,363 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 10:06:31,363 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 10:06:31,364 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 10:06:31,364 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 10:06:31,364 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Kontrakten in Sommer Sommer.
2024-05-02 10:06:31,364 - INFO - joeynmt.training - Example #4
2024-05-02 10:06:31,364 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 10:06:31,364 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 10:06:31,364 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'zeigen', ',', 'dass', 'Sie', 'ein', 'Re@@', 'f@@', 'ör@@', 'm@@', 'es', '.', '</s>']
2024-05-02 10:06:31,364 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 10:06:31,365 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 10:06:31,365 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie, die ich zeigen, dass Sie ein Reförmes.
2024-05-02 10:07:09,335 - INFO - joeynmt.training - Epoch   4, Step:    14100, Batch Loss:     1.513365, Batch Acc: 0.525792, Tokens per Sec:     1920, Lr: 0.000300
2024-05-02 10:07:47,254 - INFO - joeynmt.training - Epoch   4, Step:    14200, Batch Loss:     1.737343, Batch Acc: 0.529714, Tokens per Sec:     1986, Lr: 0.000300
2024-05-02 10:08:24,119 - INFO - joeynmt.training - Epoch   4, Step:    14300, Batch Loss:     1.804072, Batch Acc: 0.533187, Tokens per Sec:     1986, Lr: 0.000300
2024-05-02 10:09:01,865 - INFO - joeynmt.training - Epoch   4, Step:    14400, Batch Loss:     1.726491, Batch Acc: 0.526630, Tokens per Sec:     1960, Lr: 0.000300
2024-05-02 10:09:39,718 - INFO - joeynmt.training - Epoch   4, Step:    14500, Batch Loss:     1.519382, Batch Acc: 0.531396, Tokens per Sec:     1930, Lr: 0.000300
2024-05-02 10:09:39,718 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 10:09:39,718 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 10:11:31,753 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.77, ppl:   5.88, acc:   0.50, generation: 111.9276[sec], evaluation: 0.0000[sec]
2024-05-02 10:11:31,754 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 10:11:31,966 - INFO - joeynmt.helpers - delete models/bpe_4k/12000.ckpt
2024-05-02 10:11:31,969 - INFO - joeynmt.training - Example #0
2024-05-02 10:11:31,969 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 10:11:31,969 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 10:11:31,969 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'en@@', 'en', 'ge@@', 'zeigt', ',', 'dass', 'der', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', 'k@@', 'ti@@', 'eren', 'E@@', 'is@@', '-@@', 'Z@@', 'a@@', 'p', ',', 'der', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahren', 'der', 'L@@', 'ö@@', 'cher', 'von', '40', 'Stat@@', 't', 'hat', ',', 'hat', 'Son@@', 'nen@@', 'ne', '.', '</s>']
2024-05-02 10:11:31,970 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 10:11:31,970 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 10:11:31,970 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dienen gezeigt, dass der arktischen Eisktieren Eis-Zap, der für die meisten drei Millionen Jahren der Löcher von 40 Statt hat, hat Sonnenne.
2024-05-02 10:11:31,970 - INFO - joeynmt.training - Example #1
2024-05-02 10:11:31,970 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 10:11:31,970 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 10:11:31,970 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'der', 'Er@@', 'n@@', 'ähr@@', 'ung', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'T@@', 'as@@', 'che', 'zeigen', '.', '</s>']
2024-05-02 10:11:31,971 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 10:11:31,971 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 10:11:31,971 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung der Ernährung dieses Problem, weil es nicht die Tasche zeigen.
2024-05-02 10:11:31,971 - INFO - joeynmt.training - Example #2
2024-05-02 10:11:31,971 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 10:11:31,971 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 10:11:31,971 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'kt@@', 'is', 'ist', 'ein', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'Her@@', 'z@@', '-@@', 'B@@', 'ür@@', 'ger@@', 'e@@', 'det', '.', '</s>']
2024-05-02 10:11:31,972 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 10:11:31,972 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 10:11:31,972 - INFO - joeynmt.training - 	Hypothesis: Das Arktis ist ein, in einem Sinne, der Herz-Bürgeredet.
2024-05-02 10:11:31,972 - INFO - joeynmt.training - Example #3
2024-05-02 10:11:31,972 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 10:11:31,972 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 10:11:31,972 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 10:11:31,972 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 10:11:31,973 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 10:11:31,973 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Kontrakten in Sommer.
2024-05-02 10:11:31,973 - INFO - joeynmt.training - Example #4
2024-05-02 10:11:31,973 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 10:11:31,973 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 10:11:31,973 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'dass', 'Sie', 'ein', 'F@@', 'rei@@', 'wi@@', 'lli@@', 'g', 'sein', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'passiert', '.', '</s>']
2024-05-02 10:11:31,973 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 10:11:31,973 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 10:11:31,973 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, dass Sie ein Freiwillig sein, was über die letzten 25 Jahren passiert.
2024-05-02 10:12:10,890 - INFO - joeynmt.training - Epoch   4, Step:    14600, Batch Loss:     1.801317, Batch Acc: 0.532675, Tokens per Sec:     1923, Lr: 0.000300
2024-05-02 10:12:48,351 - INFO - joeynmt.training - Epoch   4, Step:    14700, Batch Loss:     1.613536, Batch Acc: 0.529320, Tokens per Sec:     1927, Lr: 0.000300
2024-05-02 10:13:25,910 - INFO - joeynmt.training - Epoch   4, Step:    14800, Batch Loss:     1.717615, Batch Acc: 0.529057, Tokens per Sec:     2018, Lr: 0.000300
2024-05-02 10:14:03,663 - INFO - joeynmt.training - Epoch   4, Step:    14900, Batch Loss:     1.581491, Batch Acc: 0.531219, Tokens per Sec:     2008, Lr: 0.000300
2024-05-02 10:14:42,314 - INFO - joeynmt.training - Epoch   4, Step:    15000, Batch Loss:     1.758076, Batch Acc: 0.529279, Tokens per Sec:     1926, Lr: 0.000300
2024-05-02 10:14:42,314 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 10:14:42,314 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 10:16:41,746 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.84, acc:   0.50, generation: 119.3204[sec], evaluation: 0.0000[sec]
2024-05-02 10:16:41,747 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 10:16:41,955 - INFO - joeynmt.helpers - delete models/bpe_4k/12500.ckpt
2024-05-02 10:16:41,958 - INFO - joeynmt.training - Example #0
2024-05-02 10:16:41,958 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 10:16:41,958 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 10:16:41,958 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'as', 'ge@@', 'zeigt', ',', 'dass', 'die', 'Ar@@', 'z@@', 't', 'k@@', 'ti@@', 'e@@', 'f', 'ist', ',', 'dass', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'größ@@', 'ten', '4@@', '8', 'Stat@@', 'en', 'war', ',', 'die', 'die', 'meisten', '4@@', '8', 'Stat@@', 'en', ',', 'hat', 'sich', 'Son@@', 'nen@@', 'st@@', 'ab@@', 't', ',', 'hat', 'sich', 'Son@@', 'nen@@', 'st@@', 'ab@@', 't', '.', '</s>']
2024-05-02 10:16:41,959 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 10:16:41,959 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 10:16:41,959 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dias gezeigt, dass die Arzt ktief ist, dass die meisten drei Millionen Jahre lang der größten 48 Staten war, die die meisten 48 Staten, hat sich Sonnenstabt, hat sich Sonnenstabt.
2024-05-02 10:16:41,959 - INFO - joeynmt.training - Example #1
2024-05-02 10:16:41,959 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 10:16:41,959 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 10:16:41,959 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'der', 'Er@@', 'n@@', 'ähr@@', 'ung', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Sch@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is', 'nicht', 'der', 'T@@', 'ick@@', 'n@@', 'ess', '.', '</s>']
2024-05-02 10:16:41,959 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 10:16:41,960 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 10:16:41,960 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung der Ernährung dieses spezielle Problem, weil es nicht die Schicksal des Eis nicht der Tickness.
2024-05-02 10:16:41,960 - INFO - joeynmt.training - Example #2
2024-05-02 10:16:41,960 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 10:16:41,960 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 10:16:41,960 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'z@@', 't', 'ist', 'ein', 'Ge@@', 'fühl', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'das', 'Her@@', 'z@@', '-@@', 'System', '.', '</s>']
2024-05-02 10:16:41,960 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 10:16:41,960 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 10:16:41,960 - INFO - joeynmt.training - 	Hypothesis: Das Arzt ist ein Gefühl, in einem Sinne, der das Herz-System.
2024-05-02 10:16:41,961 - INFO - joeynmt.training - Example #3
2024-05-02 10:16:41,961 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 10:16:41,961 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 10:16:41,961 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 10:16:41,961 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 10:16:41,961 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 10:16:41,961 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Kontrakten in Sommer.
2024-05-02 10:16:41,961 - INFO - joeynmt.training - Example #4
2024-05-02 10:16:41,961 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 10:16:41,962 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 10:16:41,962 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'dass', 'Sie', 'ein', 'R@@', 'ra@@', 'pi@@', 'de', 'schn@@', 'eller', 'sein', 'wird', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'gesch@@', 'a@@', 'h', 'ist', '.', '</s>']
2024-05-02 10:16:41,962 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 10:16:41,962 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 10:16:41,962 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, dass Sie ein Rrapide schneller sein wird, was über die letzten 25 Jahren geschah ist.
2024-05-02 10:17:20,065 - INFO - joeynmt.training - Epoch   4, Step:    15100, Batch Loss:     1.768440, Batch Acc: 0.530633, Tokens per Sec:     1943, Lr: 0.000300
2024-05-02 10:17:58,143 - INFO - joeynmt.training - Epoch   4, Step:    15200, Batch Loss:     1.620452, Batch Acc: 0.525897, Tokens per Sec:     1936, Lr: 0.000300
2024-05-02 10:18:36,602 - INFO - joeynmt.training - Epoch   4, Step:    15300, Batch Loss:     1.695434, Batch Acc: 0.530178, Tokens per Sec:     1923, Lr: 0.000300
2024-05-02 10:19:14,100 - INFO - joeynmt.training - Epoch   4, Step:    15400, Batch Loss:     1.804410, Batch Acc: 0.531571, Tokens per Sec:     1955, Lr: 0.000300
2024-05-02 10:19:50,790 - INFO - joeynmt.training - Epoch   4, Step:    15500, Batch Loss:     1.705807, Batch Acc: 0.526296, Tokens per Sec:     1979, Lr: 0.000300
2024-05-02 10:19:50,790 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 10:19:50,790 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 10:21:39,987 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.73, acc:   0.51, generation: 109.0889[sec], evaluation: 0.0000[sec]
2024-05-02 10:21:39,988 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 10:21:40,189 - INFO - joeynmt.helpers - delete models/bpe_4k/13000.ckpt
2024-05-02 10:21:40,192 - INFO - joeynmt.training - Example #0
2024-05-02 10:21:40,192 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 10:21:40,192 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 10:21:40,192 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Sch@@', 'li@@', 'den', 'so', ',', 'dass', 'die', 'Ar@@', 'kt@@', 'is', 'für', 'die', 'Ar@@', 'kt@@', 'is', ',', 'die', 'die', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', ',', 'die', 'Größ@@', 'e', 'der', 'ver@@', 'k@@', 'ün@@', 'dig@@', 'ste', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 10:21:40,193 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 10:21:40,193 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 10:21:40,193 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Schliden so, dass die Arktis für die Arktis, die die für die meisten letzten drei Millionen Jahre, die für die meisten drei Millionen Jahre, die Größe der verkündigste von 40 Prozent.
2024-05-02 10:21:40,193 - INFO - joeynmt.training - Example #1
2024-05-02 10:21:40,193 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 10:21:40,193 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 10:21:40,193 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', 'st@@', 'ütz@@', 't', 'das', 'Er@@', 'n@@', 'ähr@@', 'ung', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Sch@@', 'ä@@', 'del', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 10:21:40,194 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 10:21:40,194 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 10:21:40,194 - INFO - joeynmt.training - 	Hypothesis: Aber das unterstützt das Ernährung dieses Problem, weil es nicht die Schädel des Eis.
2024-05-02 10:21:40,194 - INFO - joeynmt.training - Example #2
2024-05-02 10:21:40,194 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 10:21:40,194 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 10:21:40,194 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'z@@', 't', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'das', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 10:21:40,195 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 10:21:40,195 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 10:21:40,195 - INFO - joeynmt.training - 	Hypothesis: Das Arzt ist, in einem Sinne, das das Herz des globalen Klimawandel.
2024-05-02 10:21:40,195 - INFO - joeynmt.training - Example #3
2024-05-02 10:21:40,195 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 10:21:40,195 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 10:21:40,195 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 10:21:40,195 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 10:21:40,195 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 10:21:40,195 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Kontrakten in Sommer.
2024-05-02 10:21:40,196 - INFO - joeynmt.training - Example #4
2024-05-02 10:21:40,196 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 10:21:40,196 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 10:21:40,196 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zeigen', ',', 'die', 'ich', 'Ihnen', 'Ihnen', 'ein', 'R@@', 'i@@', 'ff@@', 'en@@', 'st@@', 'off', 'von', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', '.', '</s>']
2024-05-02 10:21:40,196 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 10:21:40,196 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 10:21:40,196 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeigen, die ich Ihnen Ihnen ein Riffenstoff von, was in den letzten 25 Jahren passiert.
2024-05-02 10:22:17,842 - INFO - joeynmt.training - Epoch   4, Step:    15600, Batch Loss:     1.598310, Batch Acc: 0.525829, Tokens per Sec:     1987, Lr: 0.000300
2024-05-02 10:22:56,022 - INFO - joeynmt.training - Epoch   4, Step:    15700, Batch Loss:     1.776612, Batch Acc: 0.534250, Tokens per Sec:     1935, Lr: 0.000300
2024-05-02 10:23:33,105 - INFO - joeynmt.training - Epoch   4, Step:    15800, Batch Loss:     1.430933, Batch Acc: 0.531028, Tokens per Sec:     1971, Lr: 0.000300
2024-05-02 10:24:09,978 - INFO - joeynmt.training - Epoch   4, Step:    15900, Batch Loss:     1.650959, Batch Acc: 0.529650, Tokens per Sec:     2046, Lr: 0.000300
2024-05-02 10:24:47,748 - INFO - joeynmt.training - Epoch   4, Step:    16000, Batch Loss:     1.593397, Batch Acc: 0.534880, Tokens per Sec:     1972, Lr: 0.000300
2024-05-02 10:24:47,748 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 10:24:47,748 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 10:26:36,772 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.70, acc:   0.51, generation: 108.9165[sec], evaluation: 0.0000[sec]
2024-05-02 10:26:36,774 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 10:26:36,982 - INFO - joeynmt.helpers - delete models/bpe_4k/13500.ckpt
2024-05-02 10:26:36,985 - INFO - joeynmt.training - Example #0
2024-05-02 10:26:36,985 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 10:26:36,985 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 10:26:36,985 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'gen', 'ge@@', 'zeigt', 'habe', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', 'k@@', 'et@@', 'te', ',', 'die', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', ',', 'die', 'die', 'drei', 'Millionen', 'Jahre', 'der', 'n@@', 'ie@@', 'dri@@', 'g', 'von', '40', 'Jahren', ',', 'die', 'die', 'Größ@@', 'e', 'des', 'unter@@', 'en', '4@@', '8', 'Stat@@', 't', '.', '</s>']
2024-05-02 10:26:36,986 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 10:26:36,986 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 10:26:36,986 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folgen gezeigt habe, dass die arktischen Eiskette, die die meisten drei Millionen Jahre, die die drei Millionen Jahre der niedrig von 40 Jahren, die die Größe des unteren 48 Statt.
2024-05-02 10:26:36,986 - INFO - joeynmt.training - Example #1
2024-05-02 10:26:36,986 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 10:26:36,986 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 10:26:36,986 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ver@@', 'fol@@', 'gt', 'das', 'Er@@', 'n@@', 'heit', 'dieses', 'Problem', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'T@@', 'ü@@', 'r', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 10:26:36,986 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 10:26:36,987 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 10:26:36,987 - INFO - joeynmt.training - 	Hypothesis: Aber das verfolgt das Ernheit dieses Problem dieses Problem, weil es nicht die Tür des Eis.
2024-05-02 10:26:36,987 - INFO - joeynmt.training - Example #2
2024-05-02 10:26:36,987 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 10:26:36,987 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 10:26:36,987 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'ist', 'kl@@', 'ar', 'E@@', 'is@@', 'z@@', '-@@', 'K@@', 'a@@', 'p', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'das', 'Her@@', 'z@@', '-@@', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 10:26:36,987 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 10:26:36,987 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 10:26:36,987 - INFO - joeynmt.training - 	Hypothesis: Das ist klar Eisz-Kap ist, in einem Sinne, der das Herz-Klimawandel.
2024-05-02 10:26:36,988 - INFO - joeynmt.training - Example #3
2024-05-02 10:26:36,988 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 10:26:36,988 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 10:26:36,988 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'wird', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 10:26:36,988 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 10:26:36,988 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 10:26:36,988 - INFO - joeynmt.training - 	Hypothesis: Es wird in Sommer und Kontrakten in Sommer.
2024-05-02 10:26:36,988 - INFO - joeynmt.training - Example #4
2024-05-02 10:26:36,988 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 10:26:36,989 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 10:26:36,989 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'ich', 'Ihnen', 'werde', 'ein', 'R@@', 'oh@@', 'le', 'vor@@', 'wär@@', 'ts', 'sein', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahre', 'lang', 'ist', '.', '</s>']
2024-05-02 10:26:36,989 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 10:26:36,989 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 10:26:36,989 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, ich Ihnen werde ein Rohle vorwärts sein, was über die letzten 25 Jahre lang ist.
2024-05-02 10:27:14,349 - INFO - joeynmt.training - Epoch   4, Step:    16100, Batch Loss:     1.665388, Batch Acc: 0.528335, Tokens per Sec:     2047, Lr: 0.000300
2024-05-02 10:27:52,490 - INFO - joeynmt.training - Epoch   4, Step:    16200, Batch Loss:     1.791583, Batch Acc: 0.527630, Tokens per Sec:     1997, Lr: 0.000300
2024-05-02 10:28:30,542 - INFO - joeynmt.training - Epoch   4, Step:    16300, Batch Loss:     1.678662, Batch Acc: 0.536639, Tokens per Sec:     1999, Lr: 0.000300
2024-05-02 10:29:08,865 - INFO - joeynmt.training - Epoch   4, Step:    16400, Batch Loss:     1.554668, Batch Acc: 0.529473, Tokens per Sec:     1993, Lr: 0.000300
2024-05-02 10:29:46,354 - INFO - joeynmt.training - Epoch   4, Step:    16500, Batch Loss:     1.602860, Batch Acc: 0.530608, Tokens per Sec:     2010, Lr: 0.000300
2024-05-02 10:29:46,354 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 10:29:46,354 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 10:31:48,685 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.67, acc:   0.51, generation: 122.2208[sec], evaluation: 0.0000[sec]
2024-05-02 10:31:48,687 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 10:31:48,892 - INFO - joeynmt.helpers - delete models/bpe_4k/14000.ckpt
2024-05-02 10:31:48,895 - INFO - joeynmt.training - Example #0
2024-05-02 10:31:48,895 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 10:31:48,895 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 10:31:48,895 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'gen@@', 'des', ',', 'die', 'D@@', 'mon@@', 'stri@@', 'eren', 'Z@@', 'a@@', 'p', ',', 'die', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', 'k@@', 'a@@', 'p', ',', 'die', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', ',', 'hat', 'die', 'Größ@@', 'e', 'des', 'lo@@', 'wer', '4@@', '8', '.', '</s>']
2024-05-02 10:31:48,896 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 10:31:48,896 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 10:31:48,896 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folgendes, die Dmonstrieren Zap, die die arktischen Eiskap, die die meisten drei Millionen Jahre der Größe der letzten drei Millionen Jahre, hat die Größe des lower 48.
2024-05-02 10:31:48,896 - INFO - joeynmt.training - Example #1
2024-05-02 10:31:48,896 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 10:31:48,896 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 10:31:48,896 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', ',', 'das', 'ern@@', 'st', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is@@', 's@@', 'th@@', 'ü@@', 'll@@', 't', '.', '</s>']
2024-05-02 10:31:48,897 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 10:31:48,897 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 10:31:48,897 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung, das ernst dieses spezielle Problem, weil es nicht die Dickness des Eissthüllt.
2024-05-02 10:31:48,897 - INFO - joeynmt.training - Example #2
2024-05-02 10:31:48,897 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 10:31:48,897 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 10:31:48,897 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'is', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'im', 'Sin@@', 'ne', ',', 'das', 'Her@@', 'z@@', 'sch@@', 'r@@', 'z@@', 'en@@', 'des', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 10:31:48,898 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 10:31:48,898 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 10:31:48,898 - INFO - joeynmt.training - 	Hypothesis: Der Arktis ist, in einem Sinne, im Sinne, das Herzschrzendes Klimawandel.
2024-05-02 10:31:48,898 - INFO - joeynmt.training - Example #3
2024-05-02 10:31:48,898 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 10:31:48,898 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 10:31:48,898 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'wird', 'in', 'den', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 10:31:48,898 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 10:31:48,899 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 10:31:48,899 - INFO - joeynmt.training - 	Hypothesis: Es wird in den Winter und Kontrakte in Sommer.
2024-05-02 10:31:48,899 - INFO - joeynmt.training - Example #4
2024-05-02 10:31:48,899 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 10:31:48,899 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 10:31:48,899 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', ',', 'dass', 'man', 'einen', 'R@@', 'a@@', 'di@@', 'en', 'des', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahre', 'gesch@@', 'a@@', 'h', '.', '</s>']
2024-05-02 10:31:48,899 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 10:31:48,899 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 10:31:48,899 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde, dass man einen Radien des, was über die letzten 25 Jahre geschah.
2024-05-02 10:32:27,516 - INFO - joeynmt.training - Epoch   4, Step:    16600, Batch Loss:     1.542785, Batch Acc: 0.533926, Tokens per Sec:     1977, Lr: 0.000300
2024-05-02 10:33:05,656 - INFO - joeynmt.training - Epoch   4, Step:    16700, Batch Loss:     1.646146, Batch Acc: 0.539141, Tokens per Sec:     1954, Lr: 0.000300
2024-05-02 10:33:43,695 - INFO - joeynmt.training - Epoch   4, Step:    16800, Batch Loss:     1.635249, Batch Acc: 0.534578, Tokens per Sec:     1943, Lr: 0.000300
2024-05-02 10:34:21,818 - INFO - joeynmt.training - Epoch   4, Step:    16900, Batch Loss:     1.645605, Batch Acc: 0.536205, Tokens per Sec:     1963, Lr: 0.000300
2024-05-02 10:34:59,325 - INFO - joeynmt.training - Epoch   4, Step:    17000, Batch Loss:     1.877907, Batch Acc: 0.535596, Tokens per Sec:     2005, Lr: 0.000300
2024-05-02 10:34:59,325 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 10:34:59,326 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 10:36:49,543 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.59, acc:   0.52, generation: 110.1100[sec], evaluation: 0.0000[sec]
2024-05-02 10:36:49,545 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 10:36:49,744 - INFO - joeynmt.helpers - delete models/bpe_4k/14500.ckpt
2024-05-02 10:36:49,747 - INFO - joeynmt.training - Example #0
2024-05-02 10:36:49,747 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 10:36:49,747 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 10:36:49,747 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'des', 'ge@@', 'zeigt', ',', 'dass', 'die', 'Ar@@', 'kt@@', 'is', 'die', 'Ar@@', 'kt@@', 'is', 'k@@', 'ti@@', 'k', 'war', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'des', 'ver@@', 'letz@@', 'te', '4@@', '8', 'Stat@@', 'es', ',', 'die', 'für', 'die', 'meisten', '4@@', '8', 'Stat@@', 't', '.', '</s>']
2024-05-02 10:36:49,748 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 10:36:49,748 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 10:36:49,748 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Foldes gezeigt, dass die Arktis die Arktis ktik war, die für die meisten drei Millionen Jahre der Größe des verletzte 48 States, die für die meisten 48 Statt.
2024-05-02 10:36:49,748 - INFO - joeynmt.training - Example #1
2024-05-02 10:36:49,748 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 10:36:49,748 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 10:36:49,748 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'unter@@', 'st@@', 'ütz@@', 't', 'das', 'ern@@', 'st', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'denn', 'es', 'ist', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 10:36:49,749 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 10:36:49,749 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 10:36:49,749 - INFO - joeynmt.training - 	Hypothesis: Aber diese unterstützt das ernst dieses spezielle Problem, denn es ist nicht die Dickness des Eis.
2024-05-02 10:36:49,749 - INFO - joeynmt.training - Example #2
2024-05-02 10:36:49,749 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 10:36:49,749 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 10:36:49,749 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'et@@', 'ti@@', 'v', 'ist', ',', 'in', 'einem', 'Sin@@', 'n', ',', 'der', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', '.', '</s>']
2024-05-02 10:36:49,749 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 10:36:49,749 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 10:36:49,750 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eiskettiv ist, in einem Sinn, der Herz des globalen Klima.
2024-05-02 10:36:49,750 - INFO - joeynmt.training - Example #3
2024-05-02 10:36:49,750 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 10:36:49,750 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 10:36:49,750 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'weiter@@', 'n', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 10:36:49,750 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 10:36:49,750 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 10:36:49,750 - INFO - joeynmt.training - 	Hypothesis: Es erweitern und Kontrakte in Sommer.
2024-05-02 10:36:49,750 - INFO - joeynmt.training - Example #4
2024-05-02 10:36:49,751 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 10:36:49,751 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 10:36:49,751 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', ',', 'dass', 'ein', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'Vor@@', 'stellung', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'gesch@@', 'ehen', 'ist', '.', '</s>']
2024-05-02 10:36:49,751 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 10:36:49,751 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 10:36:49,751 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde, dass ein Fast-Fast-Vorstellung, was über die letzten 25 Jahren geschehen ist.
2024-05-02 10:37:27,130 - INFO - joeynmt.training - Epoch   4, Step:    17100, Batch Loss:     1.772296, Batch Acc: 0.529364, Tokens per Sec:     1945, Lr: 0.000300
2024-05-02 10:38:04,992 - INFO - joeynmt.training - Epoch   4, Step:    17200, Batch Loss:     1.588185, Batch Acc: 0.533120, Tokens per Sec:     1953, Lr: 0.000300
2024-05-02 10:38:24,681 - INFO - joeynmt.training - Epoch   4: total training loss 7236.03
2024-05-02 10:38:24,681 - INFO - joeynmt.training - EPOCH 5
2024-05-02 10:38:43,155 - INFO - joeynmt.training - Epoch   5, Step:    17300, Batch Loss:     1.773428, Batch Acc: 0.556253, Tokens per Sec:     1984, Lr: 0.000300
2024-05-02 10:39:20,942 - INFO - joeynmt.training - Epoch   5, Step:    17400, Batch Loss:     1.555369, Batch Acc: 0.556240, Tokens per Sec:     1981, Lr: 0.000300
2024-05-02 10:39:59,191 - INFO - joeynmt.training - Epoch   5, Step:    17500, Batch Loss:     1.660885, Batch Acc: 0.554676, Tokens per Sec:     1941, Lr: 0.000300
2024-05-02 10:39:59,192 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 10:39:59,192 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 10:41:43,784 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.58, acc:   0.52, generation: 104.4880[sec], evaluation: 0.0000[sec]
2024-05-02 10:41:43,785 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 10:41:43,987 - INFO - joeynmt.helpers - delete models/bpe_4k/15000.ckpt
2024-05-02 10:41:43,990 - INFO - joeynmt.training - Example #0
2024-05-02 10:41:43,990 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 10:41:43,990 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 10:41:43,990 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'des', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', 'sch@@', 'ul@@', 'dig@@', 'sten', 'K@@', 'et@@', 'te', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', '4@@', '8', 'Stat@@', 't', ',', 'die', 'Son@@', 'nen@@', 'lich@@', 't', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 10:41:43,991 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 10:41:43,991 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 10:41:43,991 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Foldes gezeigt, dass die arktischen Eisschuldigsten Kette, die für die meisten drei Millionen Jahre der Größe 48 Statt, die Sonnenlicht von 40 Prozent.
2024-05-02 10:41:43,991 - INFO - joeynmt.training - Example #1
2024-05-02 10:41:43,991 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 10:41:43,991 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 10:41:43,991 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'des', 'ern@@', 'ern@@', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ec@@', 'k', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 10:41:43,991 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 10:41:43,992 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 10:41:43,992 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung des ernernernsthaft dieses Problem, weil es nicht die Deck des Eis.
2024-05-02 10:41:43,992 - INFO - joeynmt.training - Example #2
2024-05-02 10:41:43,992 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 10:41:43,992 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 10:41:43,992 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'is', 'ist', 'K@@', 'raft', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 10:41:43,992 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 10:41:43,992 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 10:41:43,992 - INFO - joeynmt.training - 	Hypothesis: Der Arktis ist Kraft, in einem Sinne, der Herz des globalen Klima des globalen Klimawandel.
2024-05-02 10:41:43,993 - INFO - joeynmt.training - Example #3
2024-05-02 10:41:43,993 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 10:41:43,993 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 10:41:43,993 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'exp@@', 'an@@', 'an@@', 'ende', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ti@@', 'ert', '.', '</s>']
2024-05-02 10:41:43,993 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 10:41:43,993 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 10:41:43,993 - INFO - joeynmt.training - 	Hypothesis: Es expananende in Winter und Kontraktiert.
2024-05-02 10:41:43,993 - INFO - joeynmt.training - Example #4
2024-05-02 10:41:43,993 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 10:41:43,993 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 10:41:43,994 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', ',', 'dass', 'ein', 'R@@', 'oh@@', 'l@@', 'en@@', 'der', 'schn@@', 'eller', 'vor@@', 'l@@', 'ag@@', 'er@@', 'stellen', 'wird', '.', '</s>']
2024-05-02 10:41:43,994 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 10:41:43,994 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 10:41:43,994 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde, dass ein Rohlender schneller vorlagerstellen wird.
2024-05-02 10:42:21,006 - INFO - joeynmt.training - Epoch   5, Step:    17600, Batch Loss:     1.660521, Batch Acc: 0.554499, Tokens per Sec:     1995, Lr: 0.000300
2024-05-02 10:42:59,324 - INFO - joeynmt.training - Epoch   5, Step:    17700, Batch Loss:     1.778791, Batch Acc: 0.553694, Tokens per Sec:     1978, Lr: 0.000300
2024-05-02 10:43:37,098 - INFO - joeynmt.training - Epoch   5, Step:    17800, Batch Loss:     1.604149, Batch Acc: 0.554337, Tokens per Sec:     1939, Lr: 0.000300
2024-05-02 10:44:15,454 - INFO - joeynmt.training - Epoch   5, Step:    17900, Batch Loss:     1.511955, Batch Acc: 0.547599, Tokens per Sec:     1973, Lr: 0.000300
2024-05-02 10:44:53,737 - INFO - joeynmt.training - Epoch   5, Step:    18000, Batch Loss:     1.488550, Batch Acc: 0.550859, Tokens per Sec:     1906, Lr: 0.000300
2024-05-02 10:44:53,737 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 10:44:53,737 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 10:47:13,423 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.71, ppl:   5.55, acc:   0.52, generation: 139.5775[sec], evaluation: 0.0000[sec]
2024-05-02 10:47:13,424 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 10:47:13,628 - INFO - joeynmt.helpers - delete models/bpe_4k/15500.ckpt
2024-05-02 10:47:13,631 - INFO - joeynmt.training - Example #0
2024-05-02 10:47:13,631 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 10:47:13,631 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 10:47:13,631 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'des', 'des', 'des', 'des', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', '-@@', 'E@@', 'is@@', '-@@', 'E@@', 'is@@', '-@@', 'E@@', 'is@@', '-@@', 'Z@@', 'a@@', 'p', ',', 'die', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', '4@@', '8', 'Stat@@', 'en', ',', 'die', 'die', 'vier@@', 'te', '4@@', '8', 'Stat@@', 't', ',', 'die', 'Son@@', 'nen@@', 'lich@@', 't', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 10:47:13,632 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 10:47:13,632 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 10:47:13,632 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Foldes des des des arktischen Eis-Eis-Eis-Eis-Zap, die die meisten drei Millionen Jahre die Größe 48 Staten, die die vierte 48 Statt, die Sonnenlicht von 40 Prozent.
2024-05-02 10:47:13,632 - INFO - joeynmt.training - Example #1
2024-05-02 10:47:13,632 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 10:47:13,632 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 10:47:13,632 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'des', 'Er@@', 'n@@', 'ähr@@', 'ens', 'des', 'Problem@@', 's', ',', 'denn', 'es', 'gibt', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 10:47:13,633 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 10:47:13,633 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 10:47:13,633 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung des Ernährens des Problems, denn es gibt nicht die Dickness des Eis.
2024-05-02 10:47:13,633 - INFO - joeynmt.training - Example #2
2024-05-02 10:47:13,633 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 10:47:13,633 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 10:47:13,633 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'kt@@', 'is', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'lim@@', 'a', '.', '</s>']
2024-05-02 10:47:13,633 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 10:47:13,634 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 10:47:13,634 - INFO - joeynmt.training - 	Hypothesis: Das Arktis ist, in einem Sinne, der Herz des globalen Klimalima des globalen Klimalima.
2024-05-02 10:47:13,634 - INFO - joeynmt.training - Example #3
2024-05-02 10:47:13,634 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 10:47:13,634 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 10:47:13,634 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'den', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', 'und', '.', '</s>']
2024-05-02 10:47:13,634 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 10:47:13,634 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 10:47:13,634 - INFO - joeynmt.training - 	Hypothesis: Es gibt in den Winter und Kontrakten in Sommer und.
2024-05-02 10:47:13,635 - INFO - joeynmt.training - Example #4
2024-05-02 10:47:13,635 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 10:47:13,635 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 10:47:13,635 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'wird', 'es', 'von', 'der', 'letzten', '25', 'Jahren', 'auf', 'die', 'letzten', '25', 'Jahren', 'passiert', '.', '</s>']
2024-05-02 10:47:13,635 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 10:47:13,635 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 10:47:13,635 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, wird es von der letzten 25 Jahren auf die letzten 25 Jahren passiert.
2024-05-02 10:47:51,347 - INFO - joeynmt.training - Epoch   5, Step:    18100, Batch Loss:     1.828967, Batch Acc: 0.546511, Tokens per Sec:     1903, Lr: 0.000300
2024-05-02 10:48:29,327 - INFO - joeynmt.training - Epoch   5, Step:    18200, Batch Loss:     1.709452, Batch Acc: 0.547511, Tokens per Sec:     1973, Lr: 0.000300
2024-05-02 10:49:07,317 - INFO - joeynmt.training - Epoch   5, Step:    18300, Batch Loss:     1.598197, Batch Acc: 0.545976, Tokens per Sec:     2004, Lr: 0.000300
2024-05-02 10:49:44,786 - INFO - joeynmt.training - Epoch   5, Step:    18400, Batch Loss:     1.761868, Batch Acc: 0.546502, Tokens per Sec:     1932, Lr: 0.000300
2024-05-02 10:50:22,809 - INFO - joeynmt.training - Epoch   5, Step:    18500, Batch Loss:     1.540789, Batch Acc: 0.547248, Tokens per Sec:     1974, Lr: 0.000300
2024-05-02 10:50:22,810 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 10:50:22,810 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 10:52:20,458 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.71, ppl:   5.51, acc:   0.52, generation: 117.5425[sec], evaluation: 0.0000[sec]
2024-05-02 10:52:20,460 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 10:52:20,669 - INFO - joeynmt.helpers - delete models/bpe_4k/16000.ckpt
2024-05-02 10:52:20,672 - INFO - joeynmt.training - Example #0
2024-05-02 10:52:20,673 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 10:52:20,673 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 10:52:20,673 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Di@@', 'en', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', '-@@', 'E@@', 'is@@', '-@@', 'C@@', 'a@@', 'p', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', '.', '</s>']
2024-05-02 10:52:20,673 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 10:52:20,673 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 10:52:20,673 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Dien gezeigt, dass die arktischen Eis-Eis-Cap, die für die meisten drei Millionen Jahre der Größe der letzten drei Millionen Jahre der Größe der letzten drei Millionen Jahre der Größe der letzten drei Millionen Jahre.
2024-05-02 10:52:20,673 - INFO - joeynmt.training - Example #1
2024-05-02 10:52:20,674 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 10:52:20,674 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 10:52:20,674 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'dieses', 'Problem', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is@@', 'en@@', 'des', 'E@@', 'is@@', 's', 'nicht', '.', '</s>']
2024-05-02 10:52:20,674 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 10:52:20,674 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 10:52:20,674 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung dieses Problem dieses spezielle Problem, weil es nicht die Dickness des Eisendes Eiss nicht.
2024-05-02 10:52:20,674 - INFO - joeynmt.training - Example #2
2024-05-02 10:52:20,674 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 10:52:20,675 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 10:52:20,675 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'kt@@', 'is', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'Her@@', 'z@@', 'sch@@', 'la@@', 'g', ',', 'der', 'Her@@', 'z@@', 'sch@@', 'r@@', 'z@@', 'system', '.', '</s>']
2024-05-02 10:52:20,675 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 10:52:20,675 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 10:52:20,675 - INFO - joeynmt.training - 	Hypothesis: Das Arktis ist, in einem Sinne, der Herzschlag, der Herzschrzsystem.
2024-05-02 10:52:20,675 - INFO - joeynmt.training - Example #3
2024-05-02 10:52:20,675 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 10:52:20,675 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 10:52:20,675 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'exp@@', 'an@@', 'di@@', 'eren', 'und', 'Kon@@', 'tra@@', 'k@@', 'ti@@', 'eren', 'in', 'S@@', 'omm@@', 'er@@', 'ei@@', 'ert', '.', '</s>']
2024-05-02 10:52:20,676 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 10:52:20,676 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 10:52:20,676 - INFO - joeynmt.training - 	Hypothesis: Es expandieren und Kontraktieren in Sommereiert.
2024-05-02 10:52:20,676 - INFO - joeynmt.training - Example #4
2024-05-02 10:52:20,676 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 10:52:20,676 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 10:52:20,676 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'zeigen', 'werde', ',', 'dass', 'ein', 'R@@', 'a@@', 'di@@', 'en@@', 'st@@', 'ück', 'sein', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'gesch@@', 'a@@', 'h', '.', '</s>']
2024-05-02 10:52:20,677 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 10:52:20,677 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 10:52:20,677 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich zeigen werde, dass ein Radienstück sein, was in den letzten 25 Jahren geschah.
2024-05-02 10:52:58,903 - INFO - joeynmt.training - Epoch   5, Step:    18600, Batch Loss:     1.653590, Batch Acc: 0.553236, Tokens per Sec:     1962, Lr: 0.000300
2024-05-02 10:53:36,227 - INFO - joeynmt.training - Epoch   5, Step:    18700, Batch Loss:     1.468855, Batch Acc: 0.553595, Tokens per Sec:     1918, Lr: 0.000300
2024-05-02 10:54:13,206 - INFO - joeynmt.training - Epoch   5, Step:    18800, Batch Loss:     1.709612, Batch Acc: 0.549386, Tokens per Sec:     2004, Lr: 0.000300
2024-05-02 10:54:51,501 - INFO - joeynmt.training - Epoch   5, Step:    18900, Batch Loss:     1.520453, Batch Acc: 0.552287, Tokens per Sec:     1920, Lr: 0.000300
2024-05-02 10:55:29,397 - INFO - joeynmt.training - Epoch   5, Step:    19000, Batch Loss:     1.588953, Batch Acc: 0.550305, Tokens per Sec:     1993, Lr: 0.000300
2024-05-02 10:55:29,397 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 10:55:29,397 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 10:57:21,865 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.70, ppl:   5.48, acc:   0.53, generation: 112.3613[sec], evaluation: 0.0000[sec]
2024-05-02 10:57:21,866 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 10:57:22,075 - INFO - joeynmt.helpers - delete models/bpe_4k/16500.ckpt
2024-05-02 10:57:22,077 - INFO - joeynmt.training - Example #0
2024-05-02 10:57:22,078 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 10:57:22,078 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 10:57:22,078 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'des', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', '-@@', 'C@@', 'a@@', 'p', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'der', 'unter@@', 'en', '4@@', '8', 'Stat@@', 't', 'hat', '.', '</s>']
2024-05-02 10:57:22,078 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 10:57:22,078 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 10:57:22,079 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Foldes gezeigt, dass die arktischen Eis-Cap, die für die meisten drei Millionen Jahre die Größe der letzten drei Millionen Jahre der Größe der unteren 48 Statt hat.
2024-05-02 10:57:22,079 - INFO - joeynmt.training - Example #1
2024-05-02 10:57:22,079 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 10:57:22,079 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 10:57:22,079 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'des', 'S@@', 'or@@', 'es', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Sch@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 's', 'nicht', 'die', 'd@@', 'ün@@', 'ne', '.', '</s>']
2024-05-02 10:57:22,079 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 10:57:22,079 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 10:57:22,079 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung des Sores dieses spezielle Problem, weil es nicht die Schicksal des Eiss nicht die dünne.
2024-05-02 10:57:22,080 - INFO - joeynmt.training - Example #2
2024-05-02 10:57:22,080 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 10:57:22,080 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 10:57:22,080 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'et@@', 'ische', 'E@@', 'is@@', '-@@', 'Z@@', 'ah@@', 'n@@', 'system', '.', '</s>']
2024-05-02 10:57:22,080 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 10:57:22,080 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 10:57:22,080 - INFO - joeynmt.training - 	Hypothesis: Das Arktische Eisketische Eis-Zahnsystem.
2024-05-02 10:57:22,080 - INFO - joeynmt.training - Example #3
2024-05-02 10:57:22,080 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 10:57:22,081 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 10:57:22,081 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'exp@@', 'an@@', 'di@@', 'eren', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 10:57:22,081 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 10:57:22,081 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 10:57:22,081 - INFO - joeynmt.training - 	Hypothesis: Es expandieren und Kontrakten in Sommer.
2024-05-02 10:57:22,081 - INFO - joeynmt.training - Example #4
2024-05-02 10:57:22,081 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 10:57:22,081 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 10:57:22,081 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', ',', 'dass', 'Sie', 'eine', 'Re@@', 'gen@@', 'w@@', 'and@@', 'te', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahre', 'passiert', 'ist', '.', '</s>']
2024-05-02 10:57:22,082 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 10:57:22,082 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 10:57:22,082 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde, dass Sie eine Regenwandte, was über die letzten 25 Jahre passiert ist.
2024-05-02 10:57:58,796 - INFO - joeynmt.training - Epoch   5, Step:    19100, Batch Loss:     1.507592, Batch Acc: 0.551194, Tokens per Sec:     1990, Lr: 0.000300
2024-05-02 10:58:36,476 - INFO - joeynmt.training - Epoch   5, Step:    19200, Batch Loss:     1.693285, Batch Acc: 0.549521, Tokens per Sec:     1935, Lr: 0.000300
2024-05-02 10:59:15,138 - INFO - joeynmt.training - Epoch   5, Step:    19300, Batch Loss:     1.482755, Batch Acc: 0.553494, Tokens per Sec:     1928, Lr: 0.000300
2024-05-02 10:59:52,677 - INFO - joeynmt.training - Epoch   5, Step:    19400, Batch Loss:     1.700010, Batch Acc: 0.555433, Tokens per Sec:     2079, Lr: 0.000300
2024-05-02 11:00:31,654 - INFO - joeynmt.training - Epoch   5, Step:    19500, Batch Loss:     1.590854, Batch Acc: 0.549112, Tokens per Sec:     1892, Lr: 0.000300
2024-05-02 11:00:31,655 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 11:00:31,655 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 11:02:24,724 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.70, ppl:   5.47, acc:   0.52, generation: 112.9618[sec], evaluation: 0.0000[sec]
2024-05-02 11:02:24,726 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 11:02:24,939 - INFO - joeynmt.helpers - delete models/bpe_4k/17000.ckpt
2024-05-02 11:02:24,941 - INFO - joeynmt.training - Example #0
2024-05-02 11:02:24,942 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 11:02:24,942 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 11:02:24,942 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'gen', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', '-@@', 'E@@', 'is@@', '-@@', 'C@@', 'p', ',', 'das', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'des', 'unter@@', 'en', '4@@', '8', 'Stat@@', 't', ',', 'die', 'Son@@', 'nen@@', 'st@@', 'ad@@', 't', 'wurde', '.', '</s>']
2024-05-02 11:02:24,942 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 11:02:24,942 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 11:02:24,943 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folgen gezeigt, dass die arktischen Eis-Eis-Cp, das für die meisten drei Millionen Jahre die Größe des unteren 48 Statt, die Sonnenstadt wurde.
2024-05-02 11:02:24,943 - INFO - joeynmt.training - Example #1
2024-05-02 11:02:24,943 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 11:02:24,943 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 11:02:24,943 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'unter@@', 'st@@', 'üt@@', 'zen', 'das', 'Er@@', 'n@@', 'ähr@@', 'ung', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'denn', 'es', 'zeigt', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is@@', 'er@@', 'es', '.', '</s>']
2024-05-02 11:02:24,943 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 11:02:24,943 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 11:02:24,943 - INFO - joeynmt.training - 	Hypothesis: Aber diese unterstützen das Ernährung dieses spezielle Problem, denn es zeigt es nicht die Dickness des Eiseres.
2024-05-02 11:02:24,944 - INFO - joeynmt.training - Example #2
2024-05-02 11:02:24,944 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 11:02:24,944 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 11:02:24,944 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'kt@@', 'is', 'ist', ',', 'in', 'einem', 'Sin@@', 'n', ',', 'der', 'der', 'der', 'Bi@@', 'en@@', 'en@@', 'en@@', 'system', '.', '</s>']
2024-05-02 11:02:24,944 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 11:02:24,944 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 11:02:24,944 - INFO - joeynmt.training - 	Hypothesis: Das Arktis ist, in einem Sinn, der der der Bienenensystem.
2024-05-02 11:02:24,944 - INFO - joeynmt.training - Example #3
2024-05-02 11:02:24,944 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 11:02:24,945 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 11:02:24,945 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'den', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 11:02:24,945 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 11:02:24,945 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 11:02:24,945 - INFO - joeynmt.training - 	Hypothesis: Es gibt in den Winter und Kontrakte in Sommer.
2024-05-02 11:02:24,945 - INFO - joeynmt.training - Example #4
2024-05-02 11:02:24,945 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 11:02:24,945 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 11:02:24,945 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', ',', 'dass', 'ein', 'R@@', 'oh@@', 'stoff@@', '-@@', 'F@@', 'el@@', 'd', 'passiert', '.', '</s>']
2024-05-02 11:02:24,946 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 11:02:24,946 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 11:02:24,946 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde, dass ein Rohstoff-Feld passiert.
2024-05-02 11:03:01,846 - INFO - joeynmt.training - Epoch   5, Step:    19600, Batch Loss:     1.587901, Batch Acc: 0.550207, Tokens per Sec:     2040, Lr: 0.000300
2024-05-02 11:03:40,234 - INFO - joeynmt.training - Epoch   5, Step:    19700, Batch Loss:     1.787219, Batch Acc: 0.550869, Tokens per Sec:     1878, Lr: 0.000300
2024-05-02 11:04:17,318 - INFO - joeynmt.training - Epoch   5, Step:    19800, Batch Loss:     1.688573, Batch Acc: 0.549078, Tokens per Sec:     1937, Lr: 0.000300
2024-05-02 11:04:55,108 - INFO - joeynmt.training - Epoch   5, Step:    19900, Batch Loss:     1.575222, Batch Acc: 0.545726, Tokens per Sec:     1978, Lr: 0.000300
2024-05-02 11:05:32,956 - INFO - joeynmt.training - Epoch   5, Step:    20000, Batch Loss:     1.628182, Batch Acc: 0.549234, Tokens per Sec:     1943, Lr: 0.000300
2024-05-02 11:05:32,957 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 11:05:32,957 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 11:07:16,012 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.70, ppl:   5.45, acc:   0.52, generation: 102.9477[sec], evaluation: 0.0000[sec]
2024-05-02 11:07:16,013 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 11:07:16,223 - INFO - joeynmt.helpers - delete models/bpe_4k/17500.ckpt
2024-05-02 11:07:16,226 - INFO - joeynmt.training - Example #0
2024-05-02 11:07:16,226 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 11:07:16,226 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 11:07:16,226 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'gen', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is', 'ge@@', 'zeigt', ',', 'dass', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', '4@@', '8', 'Stat@@', 'en', ',', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'aten', ',', 'hat', 'sich', 'Son@@', 'nen@@', 'sch@@', 'hr@@', 't', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 11:07:16,227 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 11:07:16,227 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 11:07:16,227 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folgen gezeigt, dass die arktischen Eis gezeigt, dass die meisten drei Millionen Jahre der Größe 48 Staten, die meisten drei Millionen Jahre der Größe 48 Staaten, hat sich Sonnenschhrt von 40 Prozent.
2024-05-02 11:07:16,227 - INFO - joeynmt.training - Example #1
2024-05-02 11:07:16,227 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 11:07:16,227 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 11:07:16,227 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ungen', 'das', 'Er@@', 'n@@', 'ähr@@', 'ung', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'der', 'E@@', 'is', '.', '</s>']
2024-05-02 11:07:16,228 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 11:07:16,228 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 11:07:16,228 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützungen das Ernährung dieses spezielle Problem, weil es nicht die Dickness der Eis.
2024-05-02 11:07:16,228 - INFO - joeynmt.training - Example #2
2024-05-02 11:07:16,228 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 11:07:16,228 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 11:07:16,228 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'is', 'ist', 'in', 'einem', 'Sin@@', 'ne', ',', 'das', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', '.', '</s>']
2024-05-02 11:07:16,229 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 11:07:16,229 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 11:07:16,229 - INFO - joeynmt.training - 	Hypothesis: Der Arktis ist in einem Sinne, das das Herz des globalen Klima des globalen Klima.
2024-05-02 11:07:16,229 - INFO - joeynmt.training - Example #3
2024-05-02 11:07:16,229 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 11:07:16,229 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 11:07:16,229 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'weiter@@', 't', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 11:07:16,229 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 11:07:16,230 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 11:07:16,230 - INFO - joeynmt.training - 	Hypothesis: Es erweitert in Winter und Kontrakten in Sommer.
2024-05-02 11:07:16,230 - INFO - joeynmt.training - Example #4
2024-05-02 11:07:16,230 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 11:07:16,230 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 11:07:16,230 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'eine', 'R@@', 'ra@@', 'pi@@', 'de', 'schn@@', 'ei@@', 'den', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahre', 'gesch@@', 'a@@', 'h', 'ist', '.', '</s>']
2024-05-02 11:07:16,230 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 11:07:16,230 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 11:07:16,230 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen eine Rrapide schneiden, was über die letzten 25 Jahre geschah ist.
2024-05-02 11:07:54,494 - INFO - joeynmt.training - Epoch   5, Step:    20100, Batch Loss:     1.555842, Batch Acc: 0.553549, Tokens per Sec:     1917, Lr: 0.000300
2024-05-02 11:08:33,512 - INFO - joeynmt.training - Epoch   5, Step:    20200, Batch Loss:     1.633799, Batch Acc: 0.547916, Tokens per Sec:     1920, Lr: 0.000300
2024-05-02 11:09:11,624 - INFO - joeynmt.training - Epoch   5, Step:    20300, Batch Loss:     1.437191, Batch Acc: 0.549669, Tokens per Sec:     1927, Lr: 0.000300
2024-05-02 11:09:50,020 - INFO - joeynmt.training - Epoch   5, Step:    20400, Batch Loss:     1.474226, Batch Acc: 0.549116, Tokens per Sec:     1908, Lr: 0.000300
2024-05-02 11:10:27,366 - INFO - joeynmt.training - Epoch   5, Step:    20500, Batch Loss:     1.613828, Batch Acc: 0.551140, Tokens per Sec:     2019, Lr: 0.000300
2024-05-02 11:10:27,366 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 11:10:27,366 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 11:12:18,606 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.69, ppl:   5.41, acc:   0.53, generation: 111.1281[sec], evaluation: 0.0000[sec]
2024-05-02 11:12:18,607 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 11:12:18,816 - INFO - joeynmt.helpers - delete models/bpe_4k/18000.ckpt
2024-05-02 11:12:18,819 - INFO - joeynmt.training - Example #0
2024-05-02 11:12:18,820 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 11:12:18,820 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 11:12:18,820 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'gen', 'ge@@', 'zeigt', ',', 'dass', 'die', 'Ar@@', 'kt@@', 'is', ',', 'dass', 'die', 'Ar@@', 'kt@@', 'is', ',', 'die', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', '4@@', '8', 'Stat@@', 'en', ',', 'hat', 'Son@@', 'nen@@', 'st@@', 'ü@@', 'cke', 'von', '40', '%', '.', '</s>']
2024-05-02 11:12:18,820 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 11:12:18,820 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 11:12:18,820 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Folgen gezeigt, dass die Arktis, dass die Arktis, die die meisten drei Millionen Jahre der Größe der letzten drei Millionen Jahre der Größe 48 Staten, hat Sonnenstücke von 40%.
2024-05-02 11:12:18,821 - INFO - joeynmt.training - Example #1
2024-05-02 11:12:18,821 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 11:12:18,821 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 11:12:18,821 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Aus@@', 'ma@@', 'ß', 'des', 'S@@', 'eri@@', 'ums', 'des', 'Problem@@', 's', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'der', 'E@@', 'is', '.', '</s>']
2024-05-02 11:12:18,821 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 11:12:18,821 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 11:12:18,821 - INFO - joeynmt.training - 	Hypothesis: Aber diese Ausmaß des Seriums des Problems, weil es nicht die Dickness der Eis.
2024-05-02 11:12:18,821 - INFO - joeynmt.training - Example #2
2024-05-02 11:12:18,822 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 11:12:18,822 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 11:12:18,822 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'das', 'das', 'Er@@', 'geb@@', 'nis', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wandel@@', 's', '.', '</s>']
2024-05-02 11:12:18,822 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 11:12:18,822 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 11:12:18,822 - INFO - joeynmt.training - 	Hypothesis: Das Arktische Eis ist, in einem Sinne, das das Ergebnis des globalen Klimawandels.
2024-05-02 11:12:18,822 - INFO - joeynmt.training - Example #3
2024-05-02 11:12:18,822 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 11:12:18,823 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 11:12:18,823 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 11:12:18,823 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 11:12:18,823 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 11:12:18,823 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Kontrakte in Sommer.
2024-05-02 11:12:18,823 - INFO - joeynmt.training - Example #4
2024-05-02 11:12:18,823 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 11:12:18,823 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 11:12:18,823 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'eine', 'R@@', 'oh@@', 'nung', 'von', 'dem', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahre', 'lang', 'gesch@@', 'a@@', 'h', '.', '</s>']
2024-05-02 11:12:18,824 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 11:12:18,824 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 11:12:18,824 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen eine Rohnung von dem, was über die letzten 25 Jahre lang geschah.
2024-05-02 11:12:56,719 - INFO - joeynmt.training - Epoch   5, Step:    20600, Batch Loss:     1.603554, Batch Acc: 0.546263, Tokens per Sec:     1975, Lr: 0.000300
2024-05-02 11:13:35,146 - INFO - joeynmt.training - Epoch   5, Step:    20700, Batch Loss:     1.546299, Batch Acc: 0.552844, Tokens per Sec:     1976, Lr: 0.000300
2024-05-02 11:14:12,317 - INFO - joeynmt.training - Epoch   5, Step:    20800, Batch Loss:     1.548749, Batch Acc: 0.548908, Tokens per Sec:     1954, Lr: 0.000300
2024-05-02 11:14:51,106 - INFO - joeynmt.training - Epoch   5, Step:    20900, Batch Loss:     1.633564, Batch Acc: 0.544767, Tokens per Sec:     1935, Lr: 0.000300
2024-05-02 11:15:32,979 - INFO - joeynmt.training - Epoch   5, Step:    21000, Batch Loss:     1.677472, Batch Acc: 0.551298, Tokens per Sec:     1795, Lr: 0.000300
2024-05-02 11:15:32,980 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 11:15:32,980 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 11:17:35,393 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.67, ppl:   5.33, acc:   0.53, generation: 122.2955[sec], evaluation: 0.0000[sec]
2024-05-02 11:17:35,395 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 11:17:35,623 - INFO - joeynmt.helpers - delete models/bpe_4k/18500.ckpt
2024-05-02 11:17:35,626 - INFO - joeynmt.training - Example #0
2024-05-02 11:17:35,626 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 11:17:35,626 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 11:17:35,627 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'gen', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', '-@@', 'E@@', 'is@@', '-@@', 'Z@@', 'a@@', 'p', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'von', '40', 'Jahren', 'der', 'Größ@@', 'e', 'von', '40', '%', '.', '</s>']
2024-05-02 11:17:35,627 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 11:17:35,627 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 11:17:35,627 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folgen gezeigt, dass die arktische Eis-Eis-Zap, die für die meisten drei Millionen Jahre die Größe von 40 Jahren der Größe von 40%.
2024-05-02 11:17:35,627 - INFO - joeynmt.training - Example #1
2024-05-02 11:17:35,627 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 11:17:35,628 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 11:17:35,628 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Ver@@', 'l@@', 'auf', 'dieses', 'Problem', 'dieses', 'Problem', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is@@', 's', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is@@', 's', '.', '</s>']
2024-05-02 11:17:35,628 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 11:17:35,628 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 11:17:35,628 - INFO - joeynmt.training - 	Hypothesis: Aber diese Verlauf dieses Problem dieses Problem dieses Problem, weil es nicht die Dickness des Eiss nicht die Dickness des Eiss.
2024-05-02 11:17:35,628 - INFO - joeynmt.training - Example #2
2024-05-02 11:17:35,628 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 11:17:35,628 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 11:17:35,629 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'z@@', 't', 'ist', 'ein', 'E@@', 'is@@', '-@@', 'E@@', 'is@@', '-@@', 'E@@', 'is@@', '-@@', 'System', '.', '</s>']
2024-05-02 11:17:35,629 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 11:17:35,629 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 11:17:35,629 - INFO - joeynmt.training - 	Hypothesis: Das Arzt ist ein Eis-Eis-Eis-System.
2024-05-02 11:17:35,629 - INFO - joeynmt.training - Example #3
2024-05-02 11:17:35,629 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 11:17:35,629 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 11:17:35,629 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'sich', 'in', 'den', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'g', 'in', 'S@@', 'omm@@', 'er', 'und', 'ver@@', 'fol@@', 'gen', '.', '</s>']
2024-05-02 11:17:35,630 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 11:17:35,630 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 11:17:35,630 - INFO - joeynmt.training - 	Hypothesis: Es gibt sich in den Winter und Kontrag in Sommer und verfolgen.
2024-05-02 11:17:35,630 - INFO - joeynmt.training - Example #4
2024-05-02 11:17:35,630 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 11:17:35,630 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 11:17:35,630 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', ',', 'dass', 'Sie', 'ein', 'R@@', 'oh@@', 'l@@', 'en@@', 'der', 'vor@@', 'wär@@', 'ts', 'von', 'dem', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 11:17:35,631 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 11:17:35,631 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 11:17:35,631 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde, dass Sie ein Rohlender vorwärts von dem, was über die letzten 25 Jahren passiert ist.
2024-05-02 11:18:18,289 - INFO - joeynmt.training - Epoch   5, Step:    21100, Batch Loss:     1.464439, Batch Acc: 0.554119, Tokens per Sec:     1784, Lr: 0.000300
2024-05-02 11:18:59,775 - INFO - joeynmt.training - Epoch   5, Step:    21200, Batch Loss:     1.631523, Batch Acc: 0.552801, Tokens per Sec:     1799, Lr: 0.000300
2024-05-02 11:19:40,669 - INFO - joeynmt.training - Epoch   5, Step:    21300, Batch Loss:     1.571542, Batch Acc: 0.555982, Tokens per Sec:     1842, Lr: 0.000300
2024-05-02 11:20:22,149 - INFO - joeynmt.training - Epoch   5, Step:    21400, Batch Loss:     1.666483, Batch Acc: 0.546608, Tokens per Sec:     1757, Lr: 0.000300
2024-05-02 11:21:04,094 - INFO - joeynmt.training - Epoch   5, Step:    21500, Batch Loss:     1.602365, Batch Acc: 0.547547, Tokens per Sec:     1789, Lr: 0.000300
2024-05-02 11:21:04,094 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 11:21:04,094 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 11:23:00,925 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.68, ppl:   5.35, acc:   0.53, generation: 116.7136[sec], evaluation: 0.0000[sec]
2024-05-02 11:23:01,154 - INFO - joeynmt.helpers - delete models/bpe_4k/19000.ckpt
2024-05-02 11:23:01,157 - INFO - joeynmt.training - Example #0
2024-05-02 11:23:01,157 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 11:23:01,158 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 11:23:01,158 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'gen', 'ge@@', 'zeigt', ',', 'dass', 'die', 'Ar@@', 'kt@@', 'is', 'der', 'Ar@@', 'kt@@', 'is', ',', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', ',', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', ',', 'die', 'Größ@@', 'e', '4@@', '8', 'Stat@@', 't', ',', 'die', 'Son@@', 'nen@@', 'st@@', 'ad@@', 't', ',', 'hat', 'Son@@', 'nen@@', 'st@@', 'ad@@', 't', ',', 'hat', 'Son@@', 'nen@@', 'ne', '.', '</s>']
2024-05-02 11:23:01,158 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 11:23:01,158 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 11:23:01,158 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folgen gezeigt, dass die Arktis der Arktis, die meisten drei Millionen Jahre, die meisten drei Millionen Jahre, die Größe 48 Statt, die Sonnenstadt, hat Sonnenstadt, hat Sonnenne.
2024-05-02 11:23:01,158 - INFO - joeynmt.training - Example #1
2024-05-02 11:23:01,159 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 11:23:01,159 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 11:23:01,159 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'das', 'Er@@', 'n@@', 's@@', 'th@@', 'ü@@', 'll@@', 'ung', 'dieses', 'Problem', ',', 'denn', 'es', 'gibt', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 11:23:01,159 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 11:23:01,159 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 11:23:01,159 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung das Ernsthüllung dieses Problem, denn es gibt nicht die Dickness des Eis.
2024-05-02 11:23:01,159 - INFO - joeynmt.training - Example #2
2024-05-02 11:23:01,160 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 11:23:01,160 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 11:23:01,160 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'kt@@', 'is', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'das', 'Her@@', 'z@@', '-@@', 'Her@@', 'z@@', '-@@', 'System', '.', '</s>']
2024-05-02 11:23:01,160 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 11:23:01,160 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 11:23:01,160 - INFO - joeynmt.training - 	Hypothesis: Das Arktis ist, in einem Sinne, das Herz-Herz-System.
2024-05-02 11:23:01,160 - INFO - joeynmt.training - Example #3
2024-05-02 11:23:01,160 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 11:23:01,160 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 11:23:01,161 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'exp@@', 'an@@', 'di@@', 'er@@', 'te', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ti@@', 'k', '.', '</s>']
2024-05-02 11:23:01,161 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 11:23:01,161 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 11:23:01,161 - INFO - joeynmt.training - 	Hypothesis: Es expandierte in Winter und Kontraktik.
2024-05-02 11:23:01,161 - INFO - joeynmt.training - Example #4
2024-05-02 11:23:01,161 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 11:23:01,161 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 11:23:01,161 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'dass', 'Sie', 'eine', 'R@@', 'ra@@', 'pi@@', 'de', 'schn@@', 'elle', 'sein', 'wird', '.', '</s>']
2024-05-02 11:23:01,162 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 11:23:01,162 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 11:23:01,162 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen, dass Sie eine Rrapide schnelle sein wird.
2024-05-02 11:23:34,711 - INFO - joeynmt.training - Epoch   5: total training loss 6926.88
2024-05-02 11:23:34,711 - INFO - joeynmt.training - EPOCH 6
2024-05-02 11:23:42,389 - INFO - joeynmt.training - Epoch   6, Step:    21600, Batch Loss:     1.426284, Batch Acc: 0.561093, Tokens per Sec:     1725, Lr: 0.000300
2024-05-02 11:24:24,040 - INFO - joeynmt.training - Epoch   6, Step:    21700, Batch Loss:     1.507278, Batch Acc: 0.566753, Tokens per Sec:     1836, Lr: 0.000300
2024-05-02 11:25:05,539 - INFO - joeynmt.training - Epoch   6, Step:    21800, Batch Loss:     1.542726, Batch Acc: 0.571450, Tokens per Sec:     1757, Lr: 0.000300
2024-05-02 11:25:47,467 - INFO - joeynmt.training - Epoch   6, Step:    21900, Batch Loss:     1.385219, Batch Acc: 0.571761, Tokens per Sec:     1771, Lr: 0.000300
2024-05-02 11:26:30,404 - INFO - joeynmt.training - Epoch   6, Step:    22000, Batch Loss:     1.556868, Batch Acc: 0.571037, Tokens per Sec:     1759, Lr: 0.000300
2024-05-02 11:26:30,404 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 11:26:30,405 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 11:28:32,595 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.67, ppl:   5.31, acc:   0.53, generation: 122.0711[sec], evaluation: 0.0000[sec]
2024-05-02 11:28:32,596 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 11:28:32,828 - INFO - joeynmt.helpers - delete models/bpe_4k/19500.ckpt
2024-05-02 11:28:32,831 - INFO - joeynmt.training - Example #0
2024-05-02 11:28:32,832 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 11:28:32,832 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 11:28:32,832 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'des', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', '-@@', 'Z@@', 'a@@', 'p', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', '4@@', '8', 'Stat@@', 'en', 'der', 'n@@', 'ie@@', 'dri@@', 'g', '4@@', '8', 'Stat@@', 'en', ',', 'die', 'Son@@', 'nen@@', 'k@@', 'eh@@', 'r', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 11:28:32,832 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 11:28:32,832 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 11:28:32,832 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Foldes gezeigt, dass die arktischen Eis-Zap, die für die meisten drei Millionen Jahre die Größe 48 Staten der niedrig 48 Staten, die Sonnenkehr von 40 Prozent.
2024-05-02 11:28:32,833 - INFO - joeynmt.training - Example #1
2024-05-02 11:28:32,833 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 11:28:32,833 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 11:28:32,833 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'unter@@', 'st@@', 'ütz@@', 't', 'das', 'Er@@', 'n@@', 's@@', 'th@@', 'ü@@', 'll@@', 'ung', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is@@', 'o@@', 'li@@', 'eren', '.', '</s>']
2024-05-02 11:28:32,833 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 11:28:32,833 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 11:28:32,834 - INFO - joeynmt.training - 	Hypothesis: Aber diese unterstützt das Ernsthüllung dieses Problem, weil es nicht die Dickness des Eisolieren.
2024-05-02 11:28:32,834 - INFO - joeynmt.training - Example #2
2024-05-02 11:28:32,834 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 11:28:32,834 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 11:28:32,834 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'tik@@', 'tik@@', 'et@@', 'ische', 'E@@', 'is@@', '-@@', 'E@@', 'is', ',', 'ist', 'in', 'einem', 'Sin@@', 'n', ',', 'der', 'das', 'Her@@', 'z@@', '-@@', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 11:28:32,834 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 11:28:32,834 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 11:28:32,834 - INFO - joeynmt.training - 	Hypothesis: Das Artiktiketische Eis-Eis, ist in einem Sinn, der das Herz-Klima des globalen Klimawandel.
2024-05-02 11:28:32,835 - INFO - joeynmt.training - Example #3
2024-05-02 11:28:32,835 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 11:28:32,835 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 11:28:32,835 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'exp@@', 'an@@', 'di@@', 'eren', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'fol@@', 'g@@', 's@@', 'mit@@', 'tel', '.', '</s>']
2024-05-02 11:28:32,835 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 11:28:32,835 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 11:28:32,835 - INFO - joeynmt.training - 	Hypothesis: Es expandieren und Kontrakte in Sommer und Verfolgsmittel.
2024-05-02 11:28:32,835 - INFO - joeynmt.training - Example #4
2024-05-02 11:28:32,836 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 11:28:32,836 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 11:28:32,836 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', ',', 'dass', 'Sie', 'eine', 'Re@@', 'gen@@', 'l@@', 'eh@@', 'm', 'sein', 'wird', '.', '</s>']
2024-05-02 11:28:32,836 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 11:28:32,836 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 11:28:32,836 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde, dass Sie eine Regenlehm sein wird.
2024-05-02 11:29:14,436 - INFO - joeynmt.training - Epoch   6, Step:    22100, Batch Loss:     1.598314, Batch Acc: 0.572306, Tokens per Sec:     1763, Lr: 0.000300
2024-05-02 11:29:56,788 - INFO - joeynmt.training - Epoch   6, Step:    22200, Batch Loss:     1.551087, Batch Acc: 0.566339, Tokens per Sec:     1785, Lr: 0.000300
2024-05-02 11:30:39,085 - INFO - joeynmt.training - Epoch   6, Step:    22300, Batch Loss:     1.310178, Batch Acc: 0.568762, Tokens per Sec:     1776, Lr: 0.000300
2024-05-02 11:31:20,896 - INFO - joeynmt.training - Epoch   6, Step:    22400, Batch Loss:     1.513161, Batch Acc: 0.564997, Tokens per Sec:     1804, Lr: 0.000300
2024-05-02 11:32:03,233 - INFO - joeynmt.training - Epoch   6, Step:    22500, Batch Loss:     1.651593, Batch Acc: 0.568808, Tokens per Sec:     1759, Lr: 0.000300
2024-05-02 11:32:03,233 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 11:32:03,233 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 11:34:19,498 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.67, ppl:   5.30, acc:   0.53, generation: 136.1451[sec], evaluation: 0.0000[sec]
2024-05-02 11:34:19,500 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 11:34:19,728 - INFO - joeynmt.helpers - delete models/bpe_4k/20000.ckpt
2024-05-02 11:34:19,731 - INFO - joeynmt.training - Example #0
2024-05-02 11:34:19,731 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 11:34:19,731 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 11:34:19,731 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Di@@', 'as', 'ge@@', 'zeigt', ',', 'dass', 'die', 'Ar@@', 'kt@@', 'is', 'k@@', 'ti@@', 'k', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', '4@@', '8', 'Stat@@', 'en', 'der', 'Größ@@', 'e', '4@@', '8', 'Stat@@', 'en', 'der', 'B@@', 'erg@@', 'wer@@', 'ke', ',', 'hat', 'Son@@', 'nen@@', 'st@@', 'ad@@', 't', ',', 'hat', 'Son@@', 'nen@@', 'sch@@', 'hr@@', 'en', '.', '</s>']
2024-05-02 11:34:19,732 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 11:34:19,732 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 11:34:19,732 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Dias gezeigt, dass die Arktis ktik, die für die meisten drei Millionen Jahre die Größe 48 Staten der Größe 48 Staten der Bergwerke, hat Sonnenstadt, hat Sonnenschhren.
2024-05-02 11:34:19,732 - INFO - joeynmt.training - Example #1
2024-05-02 11:34:19,732 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 11:34:19,732 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 11:34:19,732 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'das', 'Er@@', 'n@@', 's@@', 'th@@', 'ü@@', 'll@@', 'ung', 'dieses', 'Problem', ',', 'denn', 'es', 'zeigt', 'nicht', 'das', 'D@@', 'ick@@', 'n@@', 'ess', '.', '</s>']
2024-05-02 11:34:19,733 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 11:34:19,733 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 11:34:19,733 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung das Ernsthüllung dieses Problem, denn es zeigt nicht das Dickness.
2024-05-02 11:34:19,733 - INFO - joeynmt.training - Example #2
2024-05-02 11:34:19,733 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 11:34:19,733 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 11:34:19,733 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'das', 'Her@@', 'z@@', 'sch@@', 'lie@@', 'ßen', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'system', '.', '</s>']
2024-05-02 11:34:19,734 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 11:34:19,734 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 11:34:19,734 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eis ist, in einem Sinne, der das Herzschließen Herz des globalen Klimasystem.
2024-05-02 11:34:19,734 - INFO - joeynmt.training - Example #3
2024-05-02 11:34:19,734 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 11:34:19,734 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 11:34:19,734 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'exp@@', 'an@@', 'di@@', 'eren', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 11:34:19,734 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 11:34:19,734 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 11:34:19,735 - INFO - joeynmt.training - 	Hypothesis: Es expandieren und Kontrakte in Sommer.
2024-05-02 11:34:19,735 - INFO - joeynmt.training - Example #4
2024-05-02 11:34:19,735 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 11:34:19,735 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 11:34:19,735 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', ',', 'dass', 'es', 'ein', 'F@@', 'ast@@', '-@@', 'F@@', 'el@@', 'd@@', 'd@@', 'heit', 'passiert', 'ist', '.', '</s>']
2024-05-02 11:34:19,735 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 11:34:19,735 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 11:34:19,735 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde, dass es ein Fast-Felddheit passiert ist.
2024-05-02 11:35:02,663 - INFO - joeynmt.training - Epoch   6, Step:    22600, Batch Loss:     1.647280, Batch Acc: 0.569944, Tokens per Sec:     1725, Lr: 0.000300
2024-05-02 11:35:46,983 - INFO - joeynmt.training - Epoch   6, Step:    22700, Batch Loss:     1.374448, Batch Acc: 0.566961, Tokens per Sec:     1688, Lr: 0.000300
2024-05-02 11:36:28,574 - INFO - joeynmt.training - Epoch   6, Step:    22800, Batch Loss:     1.439591, Batch Acc: 0.567945, Tokens per Sec:     1734, Lr: 0.000300
2024-05-02 11:37:11,067 - INFO - joeynmt.training - Epoch   6, Step:    22900, Batch Loss:     1.468659, Batch Acc: 0.565354, Tokens per Sec:     1771, Lr: 0.000300
2024-05-02 11:37:54,140 - INFO - joeynmt.training - Epoch   6, Step:    23000, Batch Loss:     1.644092, Batch Acc: 0.561202, Tokens per Sec:     1758, Lr: 0.000300
2024-05-02 11:37:54,140 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 11:37:54,140 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 11:40:17,414 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.66, ppl:   5.28, acc:   0.53, generation: 143.1468[sec], evaluation: 0.0000[sec]
2024-05-02 11:40:17,415 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 11:40:17,647 - INFO - joeynmt.helpers - delete models/bpe_4k/20500.ckpt
2024-05-02 11:40:17,650 - INFO - joeynmt.training - Example #0
2024-05-02 11:40:17,650 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 11:40:17,650 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 11:40:17,650 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'gen', 'ge@@', 'zeigt', ',', 'dass', 'die', 'Ar@@', 'kt@@', 'is', 'die', 'Ar@@', 'kt@@', 'is', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', '4@@', '8', 'Stat@@', 'en', 'der', 'n@@', 'ie@@', 'dri@@', 'ge', '4@@', '8', 'Stat@@', 't', ',', 'die', 'Son@@', 'ne', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 11:40:17,651 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 11:40:17,651 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 11:40:17,651 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folgen gezeigt, dass die Arktis die Arktis, die für die meisten drei Millionen Jahre der Größe der letzten drei Millionen Jahre der Größe 48 Staten der niedrige 48 Statt, die Sonne von 40 Prozent.
2024-05-02 11:40:17,651 - INFO - joeynmt.training - Example #1
2024-05-02 11:40:17,651 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 11:40:17,651 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 11:40:17,652 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'unter@@', 'st@@', 'ütz@@', 't', 'das', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'Problem', ',', 'denn', 'es', 'zeigt', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 11:40:17,652 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 11:40:17,652 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 11:40:17,652 - INFO - joeynmt.training - 	Hypothesis: Aber diese unterstützt das Ernsthaftigkeit dieses Problem, denn es zeigt nicht die Dickness des Eis.
2024-05-02 11:40:17,652 - INFO - joeynmt.training - Example #2
2024-05-02 11:40:17,652 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 11:40:17,652 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 11:40:17,652 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'is', 'ist', 'E@@', 'is', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'das', 'Her@@', 'z@@', 'b@@', 'st', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wandel@@', 's', '.', '</s>']
2024-05-02 11:40:17,653 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 11:40:17,653 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 11:40:17,653 - INFO - joeynmt.training - 	Hypothesis: Die Arktis ist Eis, in einem Sinne, der das Herzbst des globalen Klimawandels.
2024-05-02 11:40:17,653 - INFO - joeynmt.training - Example #3
2024-05-02 11:40:17,653 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 11:40:17,653 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 11:40:17,653 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'den', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'halten', 'in', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'halten', '.', '</s>']
2024-05-02 11:40:17,654 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 11:40:17,654 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 11:40:17,654 - INFO - joeynmt.training - 	Hypothesis: Es gibt in den Sommer und Verhalten in Sommer und Verhalten.
2024-05-02 11:40:17,654 - INFO - joeynmt.training - Example #4
2024-05-02 11:40:17,654 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 11:40:17,654 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 11:40:17,654 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', 'ein', 'R@@', 'ra@@', 'pi@@', 'de', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'gesch@@', 'a@@', 'h', 'ist', '.', '</s>']
2024-05-02 11:40:17,655 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 11:40:17,655 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 11:40:17,655 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde ein Rrapide von dem, was in den letzten 25 Jahren geschah ist.
2024-05-02 11:41:00,213 - INFO - joeynmt.training - Epoch   6, Step:    23100, Batch Loss:     1.452367, Batch Acc: 0.571385, Tokens per Sec:     1744, Lr: 0.000300
2024-05-02 11:41:41,981 - INFO - joeynmt.training - Epoch   6, Step:    23200, Batch Loss:     1.616877, Batch Acc: 0.563210, Tokens per Sec:     1765, Lr: 0.000300
2024-05-02 11:42:23,084 - INFO - joeynmt.training - Epoch   6, Step:    23300, Batch Loss:     1.538496, Batch Acc: 0.567898, Tokens per Sec:     1772, Lr: 0.000300
2024-05-02 11:43:04,662 - INFO - joeynmt.training - Epoch   6, Step:    23400, Batch Loss:     1.519138, Batch Acc: 0.566290, Tokens per Sec:     1787, Lr: 0.000300
2024-05-02 11:43:47,044 - INFO - joeynmt.training - Epoch   6, Step:    23500, Batch Loss:     1.532939, Batch Acc: 0.564825, Tokens per Sec:     1731, Lr: 0.000300
2024-05-02 11:43:47,044 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 11:43:47,045 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 11:45:49,034 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.66, ppl:   5.27, acc:   0.53, generation: 121.8754[sec], evaluation: 0.0000[sec]
2024-05-02 11:45:49,035 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 11:45:49,263 - INFO - joeynmt.helpers - delete models/bpe_4k/21500.ckpt
2024-05-02 11:45:49,266 - INFO - joeynmt.training - Example #0
2024-05-02 11:45:49,266 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 11:45:49,266 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 11:45:49,266 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'des', 'ge@@', 'zeigt', ',', 'dass', 'die', 'Ar@@', 'kt@@', 'is', 'ge@@', 'zeigt', ',', 'dass', 'die', 'Ar@@', 'kt@@', 'is', ',', 'die', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'n@@', 'ie@@', 'dri@@', 'g', '4@@', '8', 'Stat@@', 'en', 'der', 'n@@', 'ie@@', 'dri@@', 'g@@', 'sten', '4@@', '8', 'Stat@@', 't', '.', '</s>']
2024-05-02 11:45:49,267 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 11:45:49,267 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 11:45:49,267 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Foldes gezeigt, dass die Arktis gezeigt, dass die Arktis, die die meisten drei Millionen Jahre die Größe der niedrig 48 Staten der niedrigsten 48 Statt.
2024-05-02 11:45:49,267 - INFO - joeynmt.training - Example #1
2024-05-02 11:45:49,267 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 11:45:49,267 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 11:45:49,268 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'des', 'Er@@', 'n@@', 'ähr@@', 'ens', 'das', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'der', 'E@@', 'is', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 11:45:49,268 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 11:45:49,268 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 11:45:49,268 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung des Ernährens das Problem, weil es nicht die Dickness der Eis des Eis.
2024-05-02 11:45:49,268 - INFO - joeynmt.training - Example #2
2024-05-02 11:45:49,268 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 11:45:49,268 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 11:45:49,268 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'is', 'ist', 'E@@', 'is@@', 'k@@', 'et@@', 'ische', 'E@@', 'is@@', '-@@', 'Z@@', 'a@@', 'p', 'ist', ',', 'der', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', '-@@', 'Kli@@', 'ma@@', '-@@', 'System', '.', '</s>']
2024-05-02 11:45:49,269 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 11:45:49,269 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 11:45:49,269 - INFO - joeynmt.training - 	Hypothesis: Der Arktis ist Eisketische Eis-Zap ist, der das Herz des globalen Klima-Klima-System.
2024-05-02 11:45:49,269 - INFO - joeynmt.training - Example #3
2024-05-02 11:45:49,269 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 11:45:49,269 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 11:45:49,269 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'sich', 'in', 'den', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ti@@', 'eren', '.', '</s>']
2024-05-02 11:45:49,270 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 11:45:49,270 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 11:45:49,270 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in den Winter und Kontraktieren.
2024-05-02 11:45:49,270 - INFO - joeynmt.training - Example #4
2024-05-02 11:45:49,270 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 11:45:49,270 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 11:45:49,270 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'Ihnen', 'werde', 'ein', 'R@@', 'a@@', 'di@@', 'er@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'vor@@', 'h@@', 'an@@', 'den', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'gesch@@', 'ehen', 'ist', '.', '</s>']
2024-05-02 11:45:49,271 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 11:45:49,271 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 11:45:49,271 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen Ihnen werde ein Radier-Fast-Fast-vorhanden, was über die letzten 25 Jahren geschehen ist.
2024-05-02 11:46:30,414 - INFO - joeynmt.training - Epoch   6, Step:    23600, Batch Loss:     1.465013, Batch Acc: 0.565951, Tokens per Sec:     1784, Lr: 0.000300
2024-05-02 11:47:13,357 - INFO - joeynmt.training - Epoch   6, Step:    23700, Batch Loss:     1.503018, Batch Acc: 0.569964, Tokens per Sec:     1781, Lr: 0.000300
2024-05-02 11:47:55,145 - INFO - joeynmt.training - Epoch   6, Step:    23800, Batch Loss:     1.683713, Batch Acc: 0.566526, Tokens per Sec:     1817, Lr: 0.000300
2024-05-02 11:48:36,764 - INFO - joeynmt.training - Epoch   6, Step:    23900, Batch Loss:     1.553965, Batch Acc: 0.564435, Tokens per Sec:     1803, Lr: 0.000300
2024-05-02 11:49:18,963 - INFO - joeynmt.training - Epoch   6, Step:    24000, Batch Loss:     1.464862, Batch Acc: 0.558439, Tokens per Sec:     1756, Lr: 0.000300
2024-05-02 11:49:18,963 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 11:49:18,963 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 11:51:19,144 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.65, ppl:   5.22, acc:   0.53, generation: 120.0670[sec], evaluation: 0.0000[sec]
2024-05-02 11:51:19,146 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 11:51:19,362 - INFO - joeynmt.helpers - delete models/bpe_4k/21000.ckpt
2024-05-02 11:51:19,366 - INFO - joeynmt.training - Example #0
2024-05-02 11:51:19,367 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 11:51:19,367 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 11:51:19,367 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'ter@@', 'in', 'ge@@', 'zeigt', ',', 'dass', 'die', 'Ar@@', 'kt@@', 'is', ',', 'die', 'die', 'die', 'ar@@', 'k@@', 'ti@@', 'e@@', 'f', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', '4@@', '8', 'Stat@@', 'en', 'der', 'unter@@', 'en', '4@@', '8', 'Stat@@', 'en', ',', 'Son@@', 'nen@@', 'st@@', 'ec@@', 'kt', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 11:51:19,367 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 11:51:19,367 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 11:51:19,367 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folterin gezeigt, dass die Arktis, die die die arktief, die für die meisten drei Millionen Jahre die Größe 48 Staten der unteren 48 Staten, Sonnensteckt von 40 Prozent.
2024-05-02 11:51:19,368 - INFO - joeynmt.training - Example #1
2024-05-02 11:51:19,368 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 11:51:19,368 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 11:51:19,368 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'des', 'S@@', 'eri@@', 'ums', 'des', 'spezi@@', 'ellen', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 11:51:19,368 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 11:51:19,368 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 11:51:19,368 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung des Seriums des speziellen Problem, weil es nicht die Dickness des Eis des Eis.
2024-05-02 11:51:19,368 - INFO - joeynmt.training - Example #2
2024-05-02 11:51:19,369 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 11:51:19,369 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 11:51:19,369 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'k@@', 'ti@@', 'k', 'ist', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'das', 'Her@@', 'zen', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', '.', '</s>']
2024-05-02 11:51:19,369 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 11:51:19,369 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 11:51:19,369 - INFO - joeynmt.training - 	Hypothesis: Der Arktik ist in einem Sinne, der das Herzen des globalen Klima des globalen Klima des globalen Klima.
2024-05-02 11:51:19,369 - INFO - joeynmt.training - Example #3
2024-05-02 11:51:19,370 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 11:51:19,370 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 11:51:19,370 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 11:51:19,370 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 11:51:19,370 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 11:51:19,370 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Kontrakten in Sommer.
2024-05-02 11:51:19,370 - INFO - joeynmt.training - Example #4
2024-05-02 11:51:19,370 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 11:51:19,370 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 11:51:19,371 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'dass', 'Sie', 'ein', 'R@@', 'oh@@', 'stoff@@', '-@@', 'F@@', 'ast@@', 'st@@', 'off', 'davon', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', '.', '</s>']
2024-05-02 11:51:19,371 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 11:51:19,371 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 11:51:19,371 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, dass Sie ein Rohstoff-Faststoff davon, was in den letzten 25 Jahren passiert.
2024-05-02 11:52:03,200 - INFO - joeynmt.training - Epoch   6, Step:    24100, Batch Loss:     1.522674, Batch Acc: 0.564904, Tokens per Sec:     1677, Lr: 0.000300
2024-05-02 11:52:44,883 - INFO - joeynmt.training - Epoch   6, Step:    24200, Batch Loss:     1.625144, Batch Acc: 0.564469, Tokens per Sec:     1792, Lr: 0.000300
2024-05-02 11:53:26,578 - INFO - joeynmt.training - Epoch   6, Step:    24300, Batch Loss:     1.508069, Batch Acc: 0.567030, Tokens per Sec:     1770, Lr: 0.000300
2024-05-02 11:54:07,637 - INFO - joeynmt.training - Epoch   6, Step:    24400, Batch Loss:     1.589082, Batch Acc: 0.567804, Tokens per Sec:     1835, Lr: 0.000300
2024-05-02 11:54:49,706 - INFO - joeynmt.training - Epoch   6, Step:    24500, Batch Loss:     1.461112, Batch Acc: 0.563675, Tokens per Sec:     1752, Lr: 0.000300
2024-05-02 11:54:49,707 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 11:54:49,707 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 11:56:49,108 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.66, ppl:   5.27, acc:   0.53, generation: 119.2867[sec], evaluation: 0.0000[sec]
2024-05-02 11:56:49,336 - INFO - joeynmt.helpers - delete models/bpe_4k/22000.ckpt
2024-05-02 11:56:49,340 - INFO - joeynmt.training - Example #0
2024-05-02 11:56:49,340 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 11:56:49,340 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 11:56:49,340 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'as', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', ',', 'die', 'die', 'größ@@', 'te', '4@@', '8', 'Stat@@', 'en', ',', 'die', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', '4@@', '8', 'Stat@@', 'en', 'der', '&@@', 'qu@@', 'o@@', 't@@', ';', 'unter@@', 'e', '&@@', 'qu@@', 'o@@', 't@@', ';', ',', 'hat', 'die', 'Son@@', 'de', '40', 'Prozent', '.', '</s>']
2024-05-02 11:56:49,341 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 11:56:49,341 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 11:56:49,341 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dias gezeigt, dass die arktische Eis, die die größte 48 Staten, die die meisten drei Millionen Jahre die Größe 48 Staten der "untere", hat die Sonde 40 Prozent.
2024-05-02 11:56:49,341 - INFO - joeynmt.training - Example #1
2024-05-02 11:56:49,341 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 11:56:49,341 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 11:56:49,341 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'such@@', 'ungen', 'des', 'Er@@', 'n@@', 'ess', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 11:56:49,342 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 11:56:49,342 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 11:56:49,342 - INFO - joeynmt.training - 	Hypothesis: Aber diese Untersuchungen des Erness dieses spezielle Problem, weil es nicht die Dickness des Eis.
2024-05-02 11:56:49,342 - INFO - joeynmt.training - Example #2
2024-05-02 11:56:49,342 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 11:56:49,342 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 11:56:49,342 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'is', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 11:56:49,343 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 11:56:49,343 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 11:56:49,343 - INFO - joeynmt.training - 	Hypothesis: Die Arktis ist, in einem Sinne, der Herz des globalen Klimawandel.
2024-05-02 11:56:49,343 - INFO - joeynmt.training - Example #3
2024-05-02 11:56:49,343 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 11:56:49,343 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 11:56:49,343 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'es', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 11:56:49,343 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 11:56:49,343 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 11:56:49,344 - INFO - joeynmt.training - 	Hypothesis: Es gibt es in Winter und Kontrakten in Sommer.
2024-05-02 11:56:49,344 - INFO - joeynmt.training - Example #4
2024-05-02 11:56:49,344 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 11:56:49,344 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 11:56:49,344 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', 'ein', 'R@@', 'oh@@', 'stoff@@', 'en', 'davon', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahre', 'passiert', 'ist', '.', '</s>']
2024-05-02 11:56:49,344 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 11:56:49,344 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 11:56:49,344 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde ein Rohstoffen davon, was über die letzten 25 Jahre passiert ist.
2024-05-02 11:57:31,511 - INFO - joeynmt.training - Epoch   6, Step:    24600, Batch Loss:     1.585617, Batch Acc: 0.558335, Tokens per Sec:     1720, Lr: 0.000300
2024-05-02 11:58:12,634 - INFO - joeynmt.training - Epoch   6, Step:    24700, Batch Loss:     1.565845, Batch Acc: 0.563172, Tokens per Sec:     1837, Lr: 0.000300
2024-05-02 11:58:55,019 - INFO - joeynmt.training - Epoch   6, Step:    24800, Batch Loss:     1.598361, Batch Acc: 0.564848, Tokens per Sec:     1742, Lr: 0.000300
2024-05-02 11:59:37,459 - INFO - joeynmt.training - Epoch   6, Step:    24900, Batch Loss:     1.669072, Batch Acc: 0.565267, Tokens per Sec:     1744, Lr: 0.000300
2024-05-02 12:00:19,836 - INFO - joeynmt.training - Epoch   6, Step:    25000, Batch Loss:     1.588595, Batch Acc: 0.567261, Tokens per Sec:     1768, Lr: 0.000300
2024-05-02 12:00:19,836 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 12:00:19,836 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 12:02:37,194 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.16, acc:   0.54, generation: 137.2392[sec], evaluation: 0.0000[sec]
2024-05-02 12:02:37,196 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 12:02:37,420 - INFO - joeynmt.helpers - delete models/bpe_4k/22500.ckpt
2024-05-02 12:02:37,423 - INFO - joeynmt.training - Example #0
2024-05-02 12:02:37,424 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 12:02:37,424 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 12:02:37,424 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'gen', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', '-@@', 'E@@', 'is@@', '-@@', 'E@@', 'is@@', '-@@', 'C@@', 'a@@', 'p', ',', 'das', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'des', 'unter@@', 'en', '4@@', '8', 'Stat@@', 't', ',', 'hat', 'Son@@', 'nen@@', 'st@@', 'ad@@', 't', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 12:02:37,424 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 12:02:37,424 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 12:02:37,425 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folgen gezeigt, dass die arktische Eis-Eis-Eis-Cap, das für die meisten drei Millionen Jahre der Größe des unteren 48 Statt, hat Sonnenstadt von 40 Prozent.
2024-05-02 12:02:37,425 - INFO - joeynmt.training - Example #1
2024-05-02 12:02:37,425 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 12:02:37,425 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 12:02:37,425 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schie@@', 'de', 'das', 'Er@@', 'n@@', 's@@', 'th@@', 'ick@@', 'n@@', 'ess', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 12:02:37,425 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 12:02:37,425 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 12:02:37,426 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschiede das Ernsthickness dieses Problem, weil es nicht die Dickness des Eis.
2024-05-02 12:02:37,426 - INFO - joeynmt.training - Example #2
2024-05-02 12:02:37,426 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 12:02:37,426 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 12:02:37,426 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'et@@', 'te', 'ist', ',', 'im', 'Sin@@', 'n', ',', 'der', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', '-@@', 'System', '.', '</s>']
2024-05-02 12:02:37,426 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 12:02:37,426 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 12:02:37,426 - INFO - joeynmt.training - 	Hypothesis: Das arktische Eiskette ist, im Sinn, der das Herz des globalen Klima-System.
2024-05-02 12:02:37,427 - INFO - joeynmt.training - Example #3
2024-05-02 12:02:37,427 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 12:02:37,427 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 12:02:37,427 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'den', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'tion@@', 'en', 'in', 'S@@', 'omm@@', 'er', 'und', 'ver@@', 'fol@@', 'gt', '.', '</s>']
2024-05-02 12:02:37,427 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 12:02:37,427 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 12:02:37,427 - INFO - joeynmt.training - 	Hypothesis: Es gibt in den Winter und Kontraktionen in Sommer und verfolgt.
2024-05-02 12:02:37,427 - INFO - joeynmt.training - Example #4
2024-05-02 12:02:37,428 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 12:02:37,428 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 12:02:37,428 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', ',', 'wird', 'ein', 'R@@', 'oh@@', 'n@@', 't', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'gesch@@', 'a@@', 'h', '.', '</s>']
2024-05-02 12:02:37,428 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 12:02:37,428 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 12:02:37,428 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen, wird ein Rohnt von dem, was in den letzten 25 Jahren geschah.
2024-05-02 12:03:19,206 - INFO - joeynmt.training - Epoch   6, Step:    25100, Batch Loss:     1.799549, Batch Acc: 0.566989, Tokens per Sec:     1780, Lr: 0.000300
2024-05-02 12:04:00,875 - INFO - joeynmt.training - Epoch   6, Step:    25200, Batch Loss:     1.552330, Batch Acc: 0.568936, Tokens per Sec:     1789, Lr: 0.000300
2024-05-02 12:04:43,812 - INFO - joeynmt.training - Epoch   6, Step:    25300, Batch Loss:     1.527195, Batch Acc: 0.565351, Tokens per Sec:     1741, Lr: 0.000300
2024-05-02 12:05:26,478 - INFO - joeynmt.training - Epoch   6, Step:    25400, Batch Loss:     1.698070, Batch Acc: 0.563490, Tokens per Sec:     1763, Lr: 0.000300
2024-05-02 12:06:08,240 - INFO - joeynmt.training - Epoch   6, Step:    25500, Batch Loss:     1.540576, Batch Acc: 0.559823, Tokens per Sec:     1754, Lr: 0.000300
2024-05-02 12:06:08,240 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 12:06:08,240 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 12:08:19,272 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.18, acc:   0.54, generation: 130.9174[sec], evaluation: 0.0000[sec]
2024-05-02 12:08:19,494 - INFO - joeynmt.helpers - delete models/bpe_4k/23000.ckpt
2024-05-02 12:08:19,498 - INFO - joeynmt.training - Example #0
2024-05-02 12:08:19,499 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 12:08:19,499 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 12:08:19,499 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'des', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', 'sch@@', 'icht', ',', 'die', 'die', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', '4@@', '8', 'Jahren', 'die', 'Größ@@', 'e', 'des', 'unter@@', 'en', '4@@', '8', 'Jahren', 'die', 'Größ@@', 'e', 'des', 'unter@@', 'en', '4@@', '8', 'Prozent', '.', '</s>']
2024-05-02 12:08:19,499 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 12:08:19,499 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 12:08:19,499 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Foldes gezeigt, dass die arktischen Eisschicht, die die die meisten drei Millionen Jahre die Größe der letzten drei Millionen Jahre die Größe 48 Jahren die Größe des unteren 48 Jahren die Größe des unteren 48 Prozent.
2024-05-02 12:08:19,499 - INFO - joeynmt.training - Example #1
2024-05-02 12:08:19,500 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 12:08:19,500 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 12:08:19,500 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'nehmen', 'das', 'Er@@', 'n@@', 'ähr@@', 'ung', 'dieses', 'spezi@@', 'es', 'Problem', ',', 'denn', 'es', 'zeigt', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is@@', 'es', 'zeigen', '.', '</s>']
2024-05-02 12:08:19,500 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 12:08:19,500 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 12:08:19,500 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unternehmen das Ernährung dieses spezies Problem, denn es zeigt die Dickness des Eises zeigen.
2024-05-02 12:08:19,500 - INFO - joeynmt.training - Example #2
2024-05-02 12:08:19,501 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 12:08:19,501 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 12:08:19,501 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'is', 'ist', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 12:08:19,501 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 12:08:19,501 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 12:08:19,501 - INFO - joeynmt.training - 	Hypothesis: Der Arktis ist ist, in einem Sinne, der Herz des globalen Klimawandel.
2024-05-02 12:08:19,501 - INFO - joeynmt.training - Example #3
2024-05-02 12:08:19,501 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 12:08:19,502 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 12:08:19,502 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'in', 'Win@@', 'ter', 'und', 'Ver@@', 'häl@@', 'tn@@', 'is', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 12:08:19,502 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 12:08:19,502 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 12:08:19,502 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Verhältnis in Sommer.
2024-05-02 12:08:19,502 - INFO - joeynmt.training - Example #4
2024-05-02 12:08:19,502 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 12:08:19,503 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 12:08:19,503 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zeigen', ',', 'dass', 'Sie', 'ein', 'R@@', 'oh@@', 'nung', 'von', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', ',', 'die', 'ich', 'Ihnen', 'gesch@@', 'ehen', 'wird', '.', '</s>']
2024-05-02 12:08:19,503 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 12:08:19,503 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 12:08:19,503 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeigen, dass Sie ein Rohnung von Fast-Fast-Fast-Fast-Fast-Fast-Fast-, die ich Ihnen geschehen wird.
2024-05-02 12:08:57,901 - INFO - joeynmt.training - Epoch   6, Step:    25600, Batch Loss:     1.668495, Batch Acc: 0.560454, Tokens per Sec:     1939, Lr: 0.000300
2024-05-02 12:09:36,755 - INFO - joeynmt.training - Epoch   6, Step:    25700, Batch Loss:     1.763606, Batch Acc: 0.561903, Tokens per Sec:     1922, Lr: 0.000300
2024-05-02 12:10:15,273 - INFO - joeynmt.training - Epoch   6, Step:    25800, Batch Loss:     1.441663, Batch Acc: 0.568771, Tokens per Sec:     1901, Lr: 0.000300
2024-05-02 12:10:53,711 - INFO - joeynmt.training - Epoch   6, Step:    25900, Batch Loss:     1.496890, Batch Acc: 0.563400, Tokens per Sec:     1925, Lr: 0.000300
2024-05-02 12:10:55,186 - INFO - joeynmt.training - Epoch   6: total training loss 6652.79
2024-05-02 12:10:55,186 - INFO - joeynmt.training - EPOCH 7
2024-05-02 12:11:34,155 - INFO - joeynmt.training - Epoch   7, Step:    26000, Batch Loss:     1.559717, Batch Acc: 0.583510, Tokens per Sec:     1891, Lr: 0.000300
2024-05-02 12:11:34,155 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 12:11:34,155 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 12:13:42,426 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.63, ppl:   5.12, acc:   0.54, generation: 128.1577[sec], evaluation: 0.0000[sec]
2024-05-02 12:13:42,428 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 12:13:42,647 - INFO - joeynmt.helpers - delete models/bpe_4k/23500.ckpt
2024-05-02 12:13:42,650 - INFO - joeynmt.training - Example #0
2024-05-02 12:13:42,650 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 12:13:42,651 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 12:13:42,651 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'gen', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'a@@', 'p', ',', 'die', 'die', 'die', 'größ@@', 'eren', '4@@', '8', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'des', 'unter@@', 'en', '4@@', '8', '%', '.', '</s>']
2024-05-02 12:13:42,651 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 12:13:42,651 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 12:13:42,651 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Folgen gezeigt, dass die arktische Eiskap, die die die größeren 48 Millionen Jahre die Größe des unteren 48%.
2024-05-02 12:13:42,651 - INFO - joeynmt.training - Example #1
2024-05-02 12:13:42,652 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 12:13:42,652 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 12:13:42,652 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'des', 'Er@@', 'n@@', 'ähr@@', 'ungs@@', 'ab@@', 'häng@@', 'ig@@', 'keit', 'dieses', 'Problem', ',', 'denn', 'es', 'zeigt', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 12:13:42,652 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 12:13:42,652 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 12:13:42,652 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung des Ernährungsabhängigkeit dieses Problem, denn es zeigt nicht die Dickness des Eis.
2024-05-02 12:13:42,652 - INFO - joeynmt.training - Example #2
2024-05-02 12:13:42,653 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 12:13:42,653 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 12:13:42,653 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'kt@@', 'is', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'das', 'Her@@', 'zen', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wan@@', 'del', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 12:13:42,653 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 12:13:42,653 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 12:13:42,653 - INFO - joeynmt.training - 	Hypothesis: Das Arktis ist, in einem Sinne, der das Herzen des globalen Klimawandel des globalen Klimawandel.
2024-05-02 12:13:42,653 - INFO - joeynmt.training - Example #3
2024-05-02 12:13:42,653 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 12:13:42,653 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 12:13:42,653 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'sich', 'in', 'den', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'halten', 'in', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'halten', '.', '</s>']
2024-05-02 12:13:42,654 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 12:13:42,654 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 12:13:42,654 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in den Sommer und Verhalten in Sommer und Verhalten.
2024-05-02 12:13:42,654 - INFO - joeynmt.training - Example #4
2024-05-02 12:13:42,654 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 12:13:42,654 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 12:13:42,654 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'werde', 'eine', 'R@@', 'as@@', 'se', 'zeigen', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahre', 'passiert', 'ist', '.', '</s>']
2024-05-02 12:13:42,655 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 12:13:42,655 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 12:13:42,655 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen werde eine Rasse zeigen, was über die letzten 25 Jahre passiert ist.
2024-05-02 12:14:21,819 - INFO - joeynmt.training - Epoch   7, Step:    26100, Batch Loss:     1.501404, Batch Acc: 0.585561, Tokens per Sec:     1857, Lr: 0.000300
2024-05-02 12:15:01,201 - INFO - joeynmt.training - Epoch   7, Step:    26200, Batch Loss:     1.661122, Batch Acc: 0.579930, Tokens per Sec:     1885, Lr: 0.000300
2024-05-02 12:15:40,840 - INFO - joeynmt.training - Epoch   7, Step:    26300, Batch Loss:     1.238583, Batch Acc: 0.581160, Tokens per Sec:     1829, Lr: 0.000300
2024-05-02 12:16:18,980 - INFO - joeynmt.training - Epoch   7, Step:    26400, Batch Loss:     1.468333, Batch Acc: 0.580255, Tokens per Sec:     1936, Lr: 0.000300
2024-05-02 12:16:59,461 - INFO - joeynmt.training - Epoch   7, Step:    26500, Batch Loss:     1.575565, Batch Acc: 0.584083, Tokens per Sec:     1811, Lr: 0.000300
2024-05-02 12:16:59,461 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 12:16:59,461 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 12:18:55,468 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.17, acc:   0.54, generation: 115.8966[sec], evaluation: 0.0000[sec]
2024-05-02 12:18:55,685 - INFO - joeynmt.helpers - delete models/bpe_4k/24500.ckpt
2024-05-02 12:18:55,688 - INFO - joeynmt.training - Example #0
2024-05-02 12:18:55,689 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 12:18:55,689 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 12:18:55,689 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'gen', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', 'k@@', 'ti@@', 'en', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'von', 'den', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', '4@@', '8', 'Stat@@', 'us', 'der', 'größ@@', 'ten', '4@@', '8', 'Stat@@', 'us', '.', '</s>']
2024-05-02 12:18:55,689 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 12:18:55,689 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 12:18:55,689 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Folgen gezeigt, dass die arktischen Eisktien, die für die meisten drei Millionen Jahre die Größe von den letzten drei Millionen Jahre der Größe 48 Status der größten 48 Status.
2024-05-02 12:18:55,689 - INFO - joeynmt.training - Example #1
2024-05-02 12:18:55,690 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 12:18:55,690 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 12:18:55,690 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'des', 'Er@@', 'n@@', 'ähr@@', 'ungs@@', 'weise', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Sch@@', 'wer@@', 't@@', 'vol@@', 'l', 'des', 'E@@', 'is@@', 's@@', 'wer@@', 't@@', 'vol@@', 'l', 'ist', '.', '</s>']
2024-05-02 12:18:55,690 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 12:18:55,690 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 12:18:55,690 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung des Ernährungsweise dieses spezielle Problem, weil es nicht die Schwertvoll des Eisswertvoll ist.
2024-05-02 12:18:55,690 - INFO - joeynmt.training - Example #2
2024-05-02 12:18:55,690 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 12:18:55,691 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 12:18:55,691 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'is', 'ist', 'ist', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wandel@@', 's', '.', '</s>']
2024-05-02 12:18:55,691 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 12:18:55,691 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 12:18:55,691 - INFO - joeynmt.training - 	Hypothesis: Der Arktis ist ist ist, in einem Sinne, der das Herz des globalen Klimawandels.
2024-05-02 12:18:55,691 - INFO - joeynmt.training - Example #3
2024-05-02 12:18:55,691 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 12:18:55,691 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 12:18:55,691 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', '.', '</s>']
2024-05-02 12:18:55,692 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 12:18:55,692 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 12:18:55,692 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Kontrakten in Sommer und Kontrakten.
2024-05-02 12:18:55,692 - INFO - joeynmt.training - Example #4
2024-05-02 12:18:55,692 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 12:18:55,692 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 12:18:55,692 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', ',', 'wird', 'ein', 'R@@', 'oh@@', 'stoff@@', 'dessen', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'gesch@@', 'ehen', 'ist', '.', '</s>']
2024-05-02 12:18:55,693 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 12:18:55,693 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 12:18:55,693 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen, wird ein Rohstoffdessen, was in den letzten 25 Jahren geschehen ist.
2024-05-02 12:19:36,197 - INFO - joeynmt.training - Epoch   7, Step:    26600, Batch Loss:     1.509474, Batch Acc: 0.585959, Tokens per Sec:     1771, Lr: 0.000300
2024-05-02 12:20:15,864 - INFO - joeynmt.training - Epoch   7, Step:    26700, Batch Loss:     1.542469, Batch Acc: 0.579524, Tokens per Sec:     1917, Lr: 0.000300
2024-05-02 12:20:54,578 - INFO - joeynmt.training - Epoch   7, Step:    26800, Batch Loss:     1.407645, Batch Acc: 0.583523, Tokens per Sec:     1908, Lr: 0.000300
2024-05-02 12:21:33,612 - INFO - joeynmt.training - Epoch   7, Step:    26900, Batch Loss:     1.423770, Batch Acc: 0.579503, Tokens per Sec:     1968, Lr: 0.000300
2024-05-02 12:22:12,335 - INFO - joeynmt.training - Epoch   7, Step:    27000, Batch Loss:     1.513663, Batch Acc: 0.580588, Tokens per Sec:     1896, Lr: 0.000300
2024-05-02 12:22:12,335 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 12:22:12,335 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 12:24:12,555 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.14, acc:   0.54, generation: 120.1089[sec], evaluation: 0.0000[sec]
2024-05-02 12:24:12,769 - INFO - joeynmt.helpers - delete models/bpe_4k/24000.ckpt
2024-05-02 12:24:12,774 - INFO - joeynmt.training - Example #0
2024-05-02 12:24:12,774 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 12:24:12,774 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 12:24:12,774 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'des', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'ti@@', 'k', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', '4@@', '8', 'Stat@@', 't', ',', 'die', 'größ@@', 'e', '4@@', '8', 'Stat@@', 't', ',', 'die', 'größ@@', 'e', '4@@', '8', 'Stat@@', 't', ',', 'die', 'un@@', 'be@@', 'ding@@', 't', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 12:24:12,775 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 12:24:12,775 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 12:24:12,775 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Foldes gezeigt, dass die arktische Eisktik gezeigt, dass die arktischen 48 Statt, die größe 48 Statt, die größe 48 Statt, die unbedingt von 40 Prozent.
2024-05-02 12:24:12,775 - INFO - joeynmt.training - Example #1
2024-05-02 12:24:12,775 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 12:24:12,775 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 12:24:12,776 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'l@@', 'ag@@', 'en@@', 'der', 'Er@@', 'n@@', 'n@@', 's@@', 'th@@', 'ü@@', 'll@@', 'ung', 'dieses', 'spezi@@', 'ellen', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'd@@', 'ü@@', 'l@@', 'tige', 'E@@', 'is', '.', '</s>']
2024-05-02 12:24:12,776 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 12:24:12,776 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 12:24:12,776 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterlagender Ernnsthüllung dieses speziellen Problem, weil es nicht die dültige Eis.
2024-05-02 12:24:12,776 - INFO - joeynmt.training - Example #2
2024-05-02 12:24:12,776 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 12:24:12,776 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 12:24:12,776 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', ',', 'in', 'einem', 'Sin@@', 'n', ',', 'der', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', '-@@', 'Kli@@', 'ma@@', '-@@', 'System', '.', '</s>']
2024-05-02 12:24:12,777 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 12:24:12,777 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 12:24:12,777 - INFO - joeynmt.training - 	Hypothesis: Das Arktische Eis ist, in einem Sinn, der das Herz des globalen Klima-Klima-System.
2024-05-02 12:24:12,777 - INFO - joeynmt.training - Example #3
2024-05-02 12:24:12,777 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 12:24:12,777 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 12:24:12,777 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 12:24:12,778 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 12:24:12,778 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 12:24:12,778 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontrakte im Sommer.
2024-05-02 12:24:12,778 - INFO - joeynmt.training - Example #4
2024-05-02 12:24:12,778 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 12:24:12,778 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 12:24:12,778 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'ein', 'R@@', 'a@@', 'di@@', 'er@@', '-@@', 'F@@', 'el@@', 'd', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'gesch@@', 'a@@', 'h', '.', '</s>']
2024-05-02 12:24:12,778 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 12:24:12,779 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 12:24:12,779 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen ein Radier-Feld, was über die letzten 25 Jahren geschah.
2024-05-02 12:24:52,284 - INFO - joeynmt.training - Epoch   7, Step:    27100, Batch Loss:     1.588200, Batch Acc: 0.581387, Tokens per Sec:     1853, Lr: 0.000300
2024-05-02 12:25:31,883 - INFO - joeynmt.training - Epoch   7, Step:    27200, Batch Loss:     1.512297, Batch Acc: 0.582996, Tokens per Sec:     1933, Lr: 0.000300
2024-05-02 12:26:10,618 - INFO - joeynmt.training - Epoch   7, Step:    27300, Batch Loss:     1.298643, Batch Acc: 0.575165, Tokens per Sec:     1914, Lr: 0.000300
2024-05-02 12:26:49,418 - INFO - joeynmt.training - Epoch   7, Step:    27400, Batch Loss:     1.520500, Batch Acc: 0.577551, Tokens per Sec:     1931, Lr: 0.000300
2024-05-02 12:27:29,130 - INFO - joeynmt.training - Epoch   7, Step:    27500, Batch Loss:     1.501967, Batch Acc: 0.578369, Tokens per Sec:     1850, Lr: 0.000300
2024-05-02 12:27:29,130 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 12:27:29,130 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 12:29:26,139 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.63, ppl:   5.12, acc:   0.54, generation: 116.8966[sec], evaluation: 0.0000[sec]
2024-05-02 12:29:26,140 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 12:29:26,353 - INFO - joeynmt.helpers - delete models/bpe_4k/25500.ckpt
2024-05-02 12:29:26,356 - INFO - joeynmt.training - Example #0
2024-05-02 12:29:26,356 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 12:29:26,356 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 12:29:26,356 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', '-@@', '-@@', '-@@', 'E@@', 'is@@', '-@@', '-@@', 'St@@', 'ar@@', 'kt@@', 'is', ',', 'die', 'für', 'die', 'meisten', '4@@', '8', 'Stat@@', 'en', ',', 'der', 'die', 'Größ@@', 'e', 'des', 'unter@@', 'en', '4@@', '8', 'Stat@@', 't', ',', 'Son@@', 'nen@@', 'st@@', 'ad@@', 't', ',', 'Son@@', 'nen@@', 'st@@', 'ad@@', 't', '.', '</s>']
2024-05-02 12:29:26,357 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 12:29:26,357 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 12:29:26,357 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folien gezeigt, dass die arktische Eis---Eis--Starktis, die für die meisten 48 Staten, der die Größe des unteren 48 Statt, Sonnenstadt, Sonnenstadt.
2024-05-02 12:29:26,357 - INFO - joeynmt.training - Example #1
2024-05-02 12:29:26,357 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 12:29:26,357 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 12:29:26,357 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'des', 'S@@', 'eri@@', 'ums', 'dieses', 'Problem', ',', 'denn', 'es', 'zeigt', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 12:29:26,358 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 12:29:26,358 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 12:29:26,358 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung des Seriums dieses Problem, denn es zeigt nicht die Dickness des Eis des Eis.
2024-05-02 12:29:26,358 - INFO - joeynmt.training - Example #2
2024-05-02 12:29:26,358 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 12:29:26,358 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 12:29:26,358 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', '-@@', 'C@@', 'a@@', 'p', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 12:29:26,359 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 12:29:26,359 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 12:29:26,359 - INFO - joeynmt.training - 	Hypothesis: Der arktische Eis-Cap ist, in einem Sinne, der das Herz des globalen Klimawandel.
2024-05-02 12:29:26,359 - INFO - joeynmt.training - Example #3
2024-05-02 12:29:26,359 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 12:29:26,359 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 12:29:26,359 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Ver@@', 'fol@@', 'g@@', 's@@', 'au@@', 'er', 'und', 'ver@@', 'fol@@', 'gt', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 12:29:26,359 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 12:29:26,359 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 12:29:26,360 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Verfolgsauer und verfolgt in Sommer.
2024-05-02 12:29:26,360 - INFO - joeynmt.training - Example #4
2024-05-02 12:29:26,360 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 12:29:26,360 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 12:29:26,360 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'ein', 'R@@', 'as@@', 'ik@@', 'en', 'sein', 'schn@@', 'eller', 'Vor@@', 'wär@@', 'ts@@', 'l@@', 'age', 'sein', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'passier@@', 'te', '.', '</s>']
2024-05-02 12:29:26,360 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 12:29:26,360 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 12:29:26,360 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen ein Rasiken sein schneller Vorwärtslage sein, was über die letzten 25 Jahren passierte.
2024-05-02 12:30:05,477 - INFO - joeynmt.training - Epoch   7, Step:    27600, Batch Loss:     1.451565, Batch Acc: 0.578585, Tokens per Sec:     1880, Lr: 0.000300
2024-05-02 12:30:44,722 - INFO - joeynmt.training - Epoch   7, Step:    27700, Batch Loss:     1.417107, Batch Acc: 0.575538, Tokens per Sec:     1867, Lr: 0.000300
2024-05-02 12:31:23,541 - INFO - joeynmt.training - Epoch   7, Step:    27800, Batch Loss:     1.576962, Batch Acc: 0.574616, Tokens per Sec:     1899, Lr: 0.000300
2024-05-02 12:32:03,177 - INFO - joeynmt.training - Epoch   7, Step:    27900, Batch Loss:     1.477507, Batch Acc: 0.577480, Tokens per Sec:     1927, Lr: 0.000300
2024-05-02 12:32:42,901 - INFO - joeynmt.training - Epoch   7, Step:    28000, Batch Loss:     1.500346, Batch Acc: 0.581594, Tokens per Sec:     1880, Lr: 0.000300
2024-05-02 12:32:42,901 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 12:32:42,901 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 12:34:57,070 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.63, ppl:   5.10, acc:   0.54, generation: 134.0533[sec], evaluation: 0.0000[sec]
2024-05-02 12:34:57,072 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 12:34:57,291 - INFO - joeynmt.helpers - delete models/bpe_4k/26500.ckpt
2024-05-02 12:34:57,294 - INFO - joeynmt.training - Example #0
2024-05-02 12:34:57,295 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 12:34:57,295 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 12:34:57,295 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'des', 'ge@@', 'zeigt', ',', 'dass', 'die', 'Ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', 'k@@', 'ap@@', 'p', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'von', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', '4@@', '8', 'Stat@@', 'en', ',', 'der', 'Son@@', 'ne', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 12:34:57,295 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 12:34:57,296 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 12:34:57,296 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Foldes gezeigt, dass die Arktischen Eiskapp, die für die meisten drei Millionen Jahre die Größe von der letzten drei Millionen Jahre der Größe 48 Staten, der Sonne von 40 Prozent.
2024-05-02 12:34:57,296 - INFO - joeynmt.training - Example #1
2024-05-02 12:34:57,296 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 12:34:57,296 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 12:34:57,296 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'hal@@', 'tung', 'des', 'Er@@', 'n@@', 'ähr@@', 'ens', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'denn', 'es', 'zeigt', 'nicht', 'die', 'd@@', 'ü@@', 're', 'der', 'E@@', 'is', '.', '</s>']
2024-05-02 12:34:57,296 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 12:34:57,296 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 12:34:57,297 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterhaltung des Ernährens dieses spezielle Problem, denn es zeigt nicht die düre der Eis.
2024-05-02 12:34:57,297 - INFO - joeynmt.training - Example #2
2024-05-02 12:34:57,297 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 12:34:57,297 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 12:34:57,297 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'is', 'ist', ',', 'in', 'einem', 'Sin@@', 'n', ',', 'der', 'das', 'Her@@', 'z@@', 'sch@@', 'lie@@', 'f', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 12:34:57,297 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 12:34:57,297 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 12:34:57,297 - INFO - joeynmt.training - 	Hypothesis: Der Arktis ist, in einem Sinn, der das Herzschlief des globalen Klimawandel.
2024-05-02 12:34:57,298 - INFO - joeynmt.training - Example #3
2024-05-02 12:34:57,298 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 12:34:57,298 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 12:34:57,298 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'sich', 'in', 'Win@@', 'ter', 'und', 'Ver@@', 'fol@@', 'g@@', 's@@', 'mit@@', 'er', '.', '</s>']
2024-05-02 12:34:57,298 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 12:34:57,298 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 12:34:57,298 - INFO - joeynmt.training - 	Hypothesis: Es gibt sich in Winter und Verfolgsmiter.
2024-05-02 12:34:57,298 - INFO - joeynmt.training - Example #4
2024-05-02 12:34:57,299 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 12:34:57,299 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 12:34:57,299 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'zei@@', 'ge', ',', 'dass', 'Sie', 'ein', 'R@@', 'a@@', 'di@@', 'um', 'schn@@', 'eller', 'sein', 'wird', '.', '</s>']
2024-05-02 12:34:57,299 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 12:34:57,299 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 12:34:57,299 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich zeige, dass Sie ein Radium schneller sein wird.
2024-05-02 12:35:36,678 - INFO - joeynmt.training - Epoch   7, Step:    28100, Batch Loss:     1.372967, Batch Acc: 0.577152, Tokens per Sec:     1826, Lr: 0.000300
2024-05-02 12:36:16,360 - INFO - joeynmt.training - Epoch   7, Step:    28200, Batch Loss:     1.333348, Batch Acc: 0.579951, Tokens per Sec:     1934, Lr: 0.000300
2024-05-02 12:36:56,432 - INFO - joeynmt.training - Epoch   7, Step:    28300, Batch Loss:     1.529125, Batch Acc: 0.573079, Tokens per Sec:     1826, Lr: 0.000300
2024-05-02 12:37:35,412 - INFO - joeynmt.training - Epoch   7, Step:    28400, Batch Loss:     1.442709, Batch Acc: 0.577252, Tokens per Sec:     1952, Lr: 0.000300
2024-05-02 12:38:16,041 - INFO - joeynmt.training - Epoch   7, Step:    28500, Batch Loss:     1.415896, Batch Acc: 0.579601, Tokens per Sec:     1865, Lr: 0.000300
2024-05-02 12:38:16,041 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 12:38:16,042 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 12:39:57,121 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.62, ppl:   5.06, acc:   0.54, generation: 100.9661[sec], evaluation: 0.0000[sec]
2024-05-02 12:39:57,123 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 12:39:57,343 - INFO - joeynmt.helpers - delete models/bpe_4k/25000.ckpt
2024-05-02 12:39:57,346 - INFO - joeynmt.training - Example #0
2024-05-02 12:39:57,347 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 12:39:57,347 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 12:39:57,347 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'zwei', 'Fol@@', 'ten', ',', 'so', 'dass', 'die', 'Ar@@', 'z@@', 't', 'K@@', 'r@@', 'au@@', 'm', 'ent@@', 'wer@@', 'fen', ',', 'die', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'von', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'des', 'unter@@', 'en', '4@@', '8', 'Prozent', '.', '</s>']
2024-05-02 12:39:57,347 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 12:39:57,347 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 12:39:57,347 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese zwei Folten, so dass die Arzt Kraum entwerfen, die für die meisten letzten drei Millionen Jahre der Größe von der letzten drei Millionen Jahre der Größe des unteren 48 Prozent.
2024-05-02 12:39:57,347 - INFO - joeynmt.training - Example #1
2024-05-02 12:39:57,348 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 12:39:57,348 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 12:39:57,348 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'hal@@', 't@@', 'ungs@@', 'st@@', 'ie@@', 'g', 'dieses', 'Problem', ',', 'weil', 'es', 'das', 'E@@', 'is', 'nicht', 'das', 'D@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 12:39:57,348 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 12:39:57,348 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 12:39:57,348 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterhaltungsstieg dieses Problem, weil es das Eis nicht das Dickness des Eis.
2024-05-02 12:39:57,348 - INFO - joeynmt.training - Example #2
2024-05-02 12:39:57,349 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 12:39:57,349 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 12:39:57,349 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'tik@@', 'et@@', 'ische', 'E@@', 'is', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'das', 'Her@@', 'z@@', '-@@', 'Her@@', 'z@@', '-@@', 'Her@@', 'z@@', '-@@', 'System', '.', '</s>']
2024-05-02 12:39:57,349 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 12:39:57,349 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 12:39:57,349 - INFO - joeynmt.training - 	Hypothesis: Die Artiketische Eis ist, in einem Sinne, der das Herz-Herz-Herz-System.
2024-05-02 12:39:57,349 - INFO - joeynmt.training - Example #3
2024-05-02 12:39:57,349 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 12:39:57,349 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 12:39:57,350 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Ver@@', 'fol@@', 'gen', 'im', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'fol@@', 'gen', '.', '</s>']
2024-05-02 12:39:57,350 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 12:39:57,350 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 12:39:57,350 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Verfolgen im Sommer und Verfolgen.
2024-05-02 12:39:57,350 - INFO - joeynmt.training - Example #4
2024-05-02 12:39:57,350 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 12:39:57,350 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 12:39:57,350 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', 'ein', 'R@@', 'ra@@', 'pi@@', 'd', 'sein', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahre', 'passiert', 'ist', '.', '</s>']
2024-05-02 12:39:57,351 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 12:39:57,351 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 12:39:57,351 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde ein Rrapid sein, was über die letzten 25 Jahre passiert ist.
2024-05-02 12:40:36,224 - INFO - joeynmt.training - Epoch   7, Step:    28600, Batch Loss:     1.433428, Batch Acc: 0.581589, Tokens per Sec:     1880, Lr: 0.000300
2024-05-02 12:41:15,598 - INFO - joeynmt.training - Epoch   7, Step:    28700, Batch Loss:     1.543947, Batch Acc: 0.578269, Tokens per Sec:     1914, Lr: 0.000300
2024-05-02 12:41:55,049 - INFO - joeynmt.training - Epoch   7, Step:    28800, Batch Loss:     1.648516, Batch Acc: 0.578631, Tokens per Sec:     1859, Lr: 0.000300
2024-05-02 12:42:35,308 - INFO - joeynmt.training - Epoch   7, Step:    28900, Batch Loss:     1.515232, Batch Acc: 0.573904, Tokens per Sec:     1848, Lr: 0.000300
2024-05-02 12:43:14,733 - INFO - joeynmt.training - Epoch   7, Step:    29000, Batch Loss:     1.570778, Batch Acc: 0.571989, Tokens per Sec:     1889, Lr: 0.000300
2024-05-02 12:43:14,734 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 12:43:14,734 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 12:45:20,256 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.62, ppl:   5.07, acc:   0.55, generation: 125.4069[sec], evaluation: 0.0000[sec]
2024-05-02 12:45:20,478 - INFO - joeynmt.helpers - delete models/bpe_4k/27000.ckpt
2024-05-02 12:45:20,481 - INFO - joeynmt.training - Example #0
2024-05-02 12:45:20,481 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 12:45:20,482 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 12:45:20,482 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'as', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is', 'ver@@', 'größ@@', 'ert', ',', 'die', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'der', 'unter@@', 'en', '4@@', '8', 'Stat@@', 'en', ',', 'hat', 'Son@@', 'nen@@', 'ne', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 12:45:20,482 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 12:45:20,482 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 12:45:20,482 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dias gezeigt, dass die arktischen Eis vergrößert, die für die meisten letzten drei Millionen Jahre die Größe der letzten drei Millionen Jahre der Größe der unteren 48 Staten, hat Sonnenne von 40 Prozent.
2024-05-02 12:45:20,482 - INFO - joeynmt.training - Example #1
2024-05-02 12:45:20,483 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 12:45:20,483 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 12:45:20,483 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'unter@@', 'st@@', 'ütz@@', 't', 'das', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'Problem', ',', 'denn', 'es', 'zeigt', 'nicht', 'die', 'Sch@@', 'ön@@', 'heit', '.', '</s>']
2024-05-02 12:45:20,483 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 12:45:20,483 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 12:45:20,483 - INFO - joeynmt.training - 	Hypothesis: Aber diese unterstützt das Ernsthaftigkeit dieses Problem, denn es zeigt nicht die Schönheit.
2024-05-02 12:45:20,483 - INFO - joeynmt.training - Example #2
2024-05-02 12:45:20,483 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 12:45:20,484 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 12:45:20,484 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'z@@', 't', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'das', 'Her@@', 'z@@', '-@@', 'Her@@', 'z@@', '-@@', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 12:45:20,484 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 12:45:20,484 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 12:45:20,484 - INFO - joeynmt.training - 	Hypothesis: Der Arzt ist, in einem Sinne, der das Herz-Herz-Klima des globalen Klimawandel.
2024-05-02 12:45:20,484 - INFO - joeynmt.training - Example #3
2024-05-02 12:45:20,484 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 12:45:20,484 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 12:45:20,484 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'h@@', 'ieren', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'h@@', 'ieren', '.', '</s>']
2024-05-02 12:45:20,485 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 12:45:20,485 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 12:45:20,485 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Kontrahieren in Sommer und Kontrahieren.
2024-05-02 12:45:20,485 - INFO - joeynmt.training - Example #4
2024-05-02 12:45:20,485 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 12:45:20,485 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 12:45:20,485 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'dass', 'Sie', 'ein', 'R@@', 'as@@', 'se', 'von', 'dem', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 12:45:20,486 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 12:45:20,486 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 12:45:20,486 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, dass Sie ein Rasse von dem, was über die letzten 25 Jahren passiert ist.
2024-05-02 12:46:00,144 - INFO - joeynmt.training - Epoch   7, Step:    29100, Batch Loss:     1.487869, Batch Acc: 0.572612, Tokens per Sec:     1847, Lr: 0.000300
2024-05-02 12:46:39,093 - INFO - joeynmt.training - Epoch   7, Step:    29200, Batch Loss:     1.487392, Batch Acc: 0.572851, Tokens per Sec:     1866, Lr: 0.000300
2024-05-02 12:47:18,109 - INFO - joeynmt.training - Epoch   7, Step:    29300, Batch Loss:     1.605824, Batch Acc: 0.572003, Tokens per Sec:     1913, Lr: 0.000300
2024-05-02 12:47:56,972 - INFO - joeynmt.training - Epoch   7, Step:    29400, Batch Loss:     1.485128, Batch Acc: 0.575190, Tokens per Sec:     1929, Lr: 0.000300
2024-05-02 12:48:35,690 - INFO - joeynmt.training - Epoch   7, Step:    29500, Batch Loss:     1.457645, Batch Acc: 0.573104, Tokens per Sec:     1927, Lr: 0.000300
2024-05-02 12:48:35,691 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 12:48:35,691 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 12:50:32,877 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.62, ppl:   5.03, acc:   0.55, generation: 117.0755[sec], evaluation: 0.0000[sec]
2024-05-02 12:50:32,879 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 12:50:33,092 - INFO - joeynmt.helpers - delete models/bpe_4k/26000.ckpt
2024-05-02 12:50:33,097 - INFO - joeynmt.training - Example #0
2024-05-02 12:50:33,097 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 12:50:33,097 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 12:50:33,097 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'as', 'so', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', 'sch@@', 'icht', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', '4@@', '8', 'Stat@@', 'en', 'der', 'n@@', 'ie@@', 'dri@@', 'ge', '4@@', '8', 'Stat@@', 'en', ',', 'hat', 'sich', 'Son@@', 'nen@@', 'ne', ',', 'hat', 'sich', 'Son@@', 'nen@@', 'ne', ',', 'hat', 'sich', 'Son@@', 'nen@@', 'ne', ',', 'die', 'von', '40', '%', '.', '</s>']
2024-05-02 12:50:33,098 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 12:50:33,098 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 12:50:33,098 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dias so, dass die arktischen Eisschicht, die für die meisten drei Millionen Jahre die Größe 48 Staten der niedrige 48 Staten, hat sich Sonnenne, hat sich Sonnenne, hat sich Sonnenne, die von 40%.
2024-05-02 12:50:33,098 - INFO - joeynmt.training - Example #1
2024-05-02 12:50:33,098 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 12:50:33,098 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 12:50:33,098 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'des', 'Er@@', 'n@@', 'ähr@@', 'ungs@@', 'wer@@', 'k', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'd@@', 'ün@@', 'ne', 'der', 'E@@', 'is@@', 's@@', 'wer@@', 't', '.', '</s>']
2024-05-02 12:50:33,099 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 12:50:33,099 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 12:50:33,099 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung des Ernährungswerk dieses Problem, weil es nicht die dünne der Eisswert.
2024-05-02 12:50:33,099 - INFO - joeynmt.training - Example #2
2024-05-02 12:50:33,099 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 12:50:33,099 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 12:50:33,099 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'z@@', 't', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'das', 'Her@@', 'z@@', '-@@', 'Her@@', 'z@@', '-@@', 'Her@@', 'z@@', '-@@', 'System', '.', '</s>']
2024-05-02 12:50:33,100 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 12:50:33,100 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 12:50:33,100 - INFO - joeynmt.training - 	Hypothesis: Der Arzt ist, in einem Sinne, das Herz-Herz-Herz-System.
2024-05-02 12:50:33,100 - INFO - joeynmt.training - Example #3
2024-05-02 12:50:33,100 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 12:50:33,100 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 12:50:33,100 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'sich', 'in', 'Win@@', 'ter', 'und', 'Ver@@', 'halten', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 12:50:33,101 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 12:50:33,101 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 12:50:33,101 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und Verhalten in Sommer.
2024-05-02 12:50:33,101 - INFO - joeynmt.training - Example #4
2024-05-02 12:50:33,101 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 12:50:33,101 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 12:50:33,101 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', ',', 'dass', 'ein', 'R@@', 'a@@', 'di@@', 'er@@', 'ung', 'von', 'dem', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 12:50:33,101 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 12:50:33,102 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 12:50:33,102 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde, dass ein Radierung von dem, was über die letzten 25 Jahren passiert ist.
2024-05-02 12:51:12,512 - INFO - joeynmt.training - Epoch   7, Step:    29600, Batch Loss:     1.574887, Batch Acc: 0.573938, Tokens per Sec:     1921, Lr: 0.000300
2024-05-02 12:51:51,895 - INFO - joeynmt.training - Epoch   7, Step:    29700, Batch Loss:     1.516513, Batch Acc: 0.577176, Tokens per Sec:     1885, Lr: 0.000300
2024-05-02 12:52:30,481 - INFO - joeynmt.training - Epoch   7, Step:    29800, Batch Loss:     1.429996, Batch Acc: 0.570146, Tokens per Sec:     1974, Lr: 0.000300
2024-05-02 12:53:09,548 - INFO - joeynmt.training - Epoch   7, Step:    29900, Batch Loss:     1.573909, Batch Acc: 0.577915, Tokens per Sec:     1892, Lr: 0.000300
2024-05-02 12:53:47,491 - INFO - joeynmt.training - Epoch   7, Step:    30000, Batch Loss:     1.519541, Batch Acc: 0.575944, Tokens per Sec:     1994, Lr: 0.000300
2024-05-02 12:53:47,491 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 12:53:47,491 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 12:55:29,154 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   4.99, acc:   0.55, generation: 101.5476[sec], evaluation: 0.0000[sec]
2024-05-02 12:55:29,155 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 12:55:29,377 - INFO - joeynmt.helpers - delete models/bpe_4k/27500.ckpt
2024-05-02 12:55:29,380 - INFO - joeynmt.training - Example #0
2024-05-02 12:55:29,380 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 12:55:29,380 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 12:55:29,381 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'gen', 'ge@@', 'zeigt', ',', 'dass', 'die', 'Ar@@', 'kt@@', 'is', 'der', 'Ar@@', 'tik@@', 'el', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'des', 'n@@', 'ie@@', 'dri@@', 'g@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'des', 'n@@', 'ie@@', 'dri@@', 'g@@', 'es', '4@@', '8', 'Sta@@', 'aten', ',', 'hat', 'sich', 'Son@@', 'nen@@', 'ne', ',', 'hat', 'durch', '40', 'Prozent', '.', '</s>']
2024-05-02 12:55:29,381 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 12:55:29,381 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 12:55:29,382 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Folgen gezeigt, dass die Arktis der Artikel für die meisten letzten drei Millionen Jahre die Größe des niedrigsten drei Millionen Jahre die Größe des niedriges 48 Staaten, hat sich Sonnenne, hat durch 40 Prozent.
2024-05-02 12:55:29,382 - INFO - joeynmt.training - Example #1
2024-05-02 12:55:29,382 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 12:55:29,382 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 12:55:29,382 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'des', 'Er@@', 'n@@', 'ähr@@', 'ungs@@', '-', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'd@@', 'ick@@', 'n@@', 'lose', 'E@@', 'is', '.', '</s>']
2024-05-02 12:55:29,382 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 12:55:29,382 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 12:55:29,383 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung des Ernährungs- dieses Problem, weil es nicht die dicknlose Eis.
2024-05-02 12:55:29,383 - INFO - joeynmt.training - Example #2
2024-05-02 12:55:29,383 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 12:55:29,383 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 12:55:29,383 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'is', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'das', 'Her@@', 'z@@', 'sch@@', 'lie@@', 'ßen', 'des', 'Kli@@', 'ma@@', 'wandel@@', 's', 'des', 'Kli@@', 'ma@@', 'wandel@@', 's', '.', '</s>']
2024-05-02 12:55:29,383 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 12:55:29,384 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 12:55:29,384 - INFO - joeynmt.training - 	Hypothesis: Der Arktis ist, in einem Sinne, der das Herzschließen des Klimawandels des Klimawandels.
2024-05-02 12:55:29,384 - INFO - joeynmt.training - Example #3
2024-05-02 12:55:29,384 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 12:55:29,384 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 12:55:29,384 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'sich', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ti@@', 'eren', 'im', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 12:55:29,384 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 12:55:29,385 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 12:55:29,385 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und Kontraktieren im Sommer.
2024-05-02 12:55:29,385 - INFO - joeynmt.training - Example #4
2024-05-02 12:55:29,385 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 12:55:29,385 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 12:55:29,385 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', 'ein', 'R@@', 'oh@@', 'n@@', 'en@@', 'st@@', 'off', 'davon', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahre', 'passiert', 'ist', '.', '</s>']
2024-05-02 12:55:29,385 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 12:55:29,385 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 12:55:29,386 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde ein Rohnenstoff davon, was über die letzten 25 Jahre passiert ist.
2024-05-02 12:56:08,271 - INFO - joeynmt.training - Epoch   7, Step:    30100, Batch Loss:     1.612484, Batch Acc: 0.575851, Tokens per Sec:     1888, Lr: 0.000300
2024-05-02 12:56:48,500 - INFO - joeynmt.training - Epoch   7, Step:    30200, Batch Loss:     1.450142, Batch Acc: 0.574098, Tokens per Sec:     1880, Lr: 0.000300
2024-05-02 12:56:58,606 - INFO - joeynmt.training - Epoch   7: total training loss 6453.95
2024-05-02 12:56:58,606 - INFO - joeynmt.training - EPOCH 8
2024-05-02 12:57:27,217 - INFO - joeynmt.training - Epoch   8, Step:    30300, Batch Loss:     1.430292, Batch Acc: 0.599943, Tokens per Sec:     1962, Lr: 0.000300
2024-05-02 12:58:06,655 - INFO - joeynmt.training - Epoch   8, Step:    30400, Batch Loss:     1.591488, Batch Acc: 0.592113, Tokens per Sec:     1932, Lr: 0.000300
2024-05-02 12:58:45,480 - INFO - joeynmt.training - Epoch   8, Step:    30500, Batch Loss:     1.408458, Batch Acc: 0.598950, Tokens per Sec:     1908, Lr: 0.000300
2024-05-02 12:58:45,480 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 12:58:45,480 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 13:00:46,682 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   5.01, acc:   0.55, generation: 121.0896[sec], evaluation: 0.0000[sec]
2024-05-02 13:00:46,900 - INFO - joeynmt.helpers - delete models/bpe_4k/28000.ckpt
2024-05-02 13:00:46,903 - INFO - joeynmt.training - Example #0
2024-05-02 13:00:46,904 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 13:00:46,904 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 13:00:46,904 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'des', 'ge@@', 'zeigt', 'habe', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', '-@@', 'Z@@', 'ah@@', 'n', 'ge@@', 'zeigt', ',', 'dass', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'des', 'unter@@', 'en', '4@@', '8', 'Stat@@', 't', ',', 'hat', 'Son@@', 'nen@@', 'i', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 13:00:46,904 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 13:00:46,905 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 13:00:46,905 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Foldes gezeigt habe, dass die arktischen Eis-Zahn gezeigt, dass die meisten drei Millionen Jahre die Größe des unteren 48 Statt, hat Sonneni von 40 Prozent.
2024-05-02 13:00:46,905 - INFO - joeynmt.training - Example #1
2024-05-02 13:00:46,905 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 13:00:46,905 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 13:00:46,905 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'unter@@', 'st@@', 'üt@@', 'zen', 'das', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Sch@@', 'wer@@', 't@@', 'sch@@', 'wer@@', 'er', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 13:00:46,905 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 13:00:46,905 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 13:00:46,906 - INFO - joeynmt.training - 	Hypothesis: Aber diese unterstützen das Ernsthaftigkeit dieses Problem, weil es nicht die Schwertschwerer des Eis.
2024-05-02 13:00:46,906 - INFO - joeynmt.training - Example #2
2024-05-02 13:00:46,906 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 13:00:46,906 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 13:00:46,906 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'tik@@', 'et@@', 'ische', 'E@@', 'is@@', 'k@@', 'ti@@', 'er', 'ist', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', '-@@', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 13:00:46,906 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 13:00:46,906 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 13:00:46,906 - INFO - joeynmt.training - 	Hypothesis: Der Artiketische Eisktier ist in einem Sinne, der das Herz des globalen Klima-Klimawandel.
2024-05-02 13:00:46,906 - INFO - joeynmt.training - Example #3
2024-05-02 13:00:46,907 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 13:00:46,907 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 13:00:46,907 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'exp@@', 'an@@', 'di@@', 'er@@', 'te', 'in', 'den', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'halten', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 13:00:46,907 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 13:00:46,907 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 13:00:46,907 - INFO - joeynmt.training - 	Hypothesis: Es expandierte in den Sommer und Verhalten in Sommer.
2024-05-02 13:00:46,907 - INFO - joeynmt.training - Example #4
2024-05-02 13:00:46,907 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 13:00:46,907 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 13:00:46,908 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', 'ein', 'R@@', 'a@@', 'di@@', 'o@@', 't@@', 'um', 'sein', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 13:00:46,908 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 13:00:46,908 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 13:00:46,908 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde ein Radiotum sein, was über die letzten 25 Jahren passiert ist.
2024-05-02 13:01:27,010 - INFO - joeynmt.training - Epoch   8, Step:    30600, Batch Loss:     1.460666, Batch Acc: 0.595199, Tokens per Sec:     1852, Lr: 0.000300
2024-05-02 13:02:07,543 - INFO - joeynmt.training - Epoch   8, Step:    30700, Batch Loss:     1.347033, Batch Acc: 0.597043, Tokens per Sec:     1891, Lr: 0.000300
2024-05-02 13:02:46,537 - INFO - joeynmt.training - Epoch   8, Step:    30800, Batch Loss:     1.491631, Batch Acc: 0.587581, Tokens per Sec:     1884, Lr: 0.000300
2024-05-02 13:03:25,855 - INFO - joeynmt.training - Epoch   8, Step:    30900, Batch Loss:     1.286592, Batch Acc: 0.592047, Tokens per Sec:     1940, Lr: 0.000300
2024-05-02 13:04:05,701 - INFO - joeynmt.training - Epoch   8, Step:    31000, Batch Loss:     1.467222, Batch Acc: 0.592140, Tokens per Sec:     1850, Lr: 0.000300
2024-05-02 13:04:05,701 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 13:04:05,701 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 13:05:52,867 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.62, ppl:   5.06, acc:   0.54, generation: 107.0562[sec], evaluation: 0.0000[sec]
2024-05-02 13:05:53,081 - INFO - joeynmt.helpers - delete models/bpe_4k/29000.ckpt
2024-05-02 13:05:53,088 - INFO - joeynmt.training - Example #0
2024-05-02 13:05:53,088 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 13:05:53,088 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 13:05:53,089 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'gen', 'ge@@', 'zeigt', ',', 'dass', 'die', 'Ar@@', 'kt@@', 'is', 'die', 'Ar@@', 'kt@@', 'is', 'ver@@', 'größ@@', 'ert', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'n@@', 'ie@@', 'dri@@', 'ge', '4@@', '8', 'Stat@@', 'en', 'der', 'unter@@', 'en', '4@@', '8', 'Stat@@', 'en', ',', 'Son@@', 'nen@@', 'ne', ',', 'hat', 'Son@@', 'nen@@', 'ne', '.', '</s>']
2024-05-02 13:05:53,089 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 13:05:53,089 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 13:05:53,089 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Folgen gezeigt, dass die Arktis die Arktis vergrößert, die für die meisten drei Millionen Jahre die Größe der niedrige 48 Staten der unteren 48 Staten, Sonnenne, hat Sonnenne.
2024-05-02 13:05:53,089 - INFO - joeynmt.training - Example #1
2024-05-02 13:05:53,090 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 13:05:53,090 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 13:05:53,090 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'hal@@', 't@@', 'ungs@@', 'ver@@', 'lang@@', 't', 'dieses', 'Problem', ',', 'denn', 'es', 'zeigt', 'die', 'Sch@@', 'wer@@', 't@@', 'sch@@', 'li@@', 'mm@@', 'ung', 'nicht', 'die', 'Sch@@', 'wer@@', 't', '.', '</s>']
2024-05-02 13:05:53,090 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 13:05:53,090 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 13:05:53,090 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterhaltungsverlangt dieses Problem, denn es zeigt die Schwertschlimmung nicht die Schwert.
2024-05-02 13:05:53,090 - INFO - joeynmt.training - Example #2
2024-05-02 13:05:53,090 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 13:05:53,091 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 13:05:53,091 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'tik@@', 'et@@', 'ische', 'E@@', 'is@@', 'k@@', 'et@@', 'ische', 'E@@', 'is@@', 'k@@', 'et@@', 'ische', 'E@@', 'is@@', 'b@@', 'ah@@', 'n', '.', '</s>']
2024-05-02 13:05:53,091 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 13:05:53,091 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 13:05:53,091 - INFO - joeynmt.training - 	Hypothesis: Der Artiketische Eisketische Eisketische Eisbahn.
2024-05-02 13:05:53,091 - INFO - joeynmt.training - Example #3
2024-05-02 13:05:53,091 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 13:05:53,091 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 13:05:53,091 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'h@@', 'ieren', 'im', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 13:05:53,092 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 13:05:53,092 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 13:05:53,092 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Sommer und Kontrahieren im Sommer.
2024-05-02 13:05:53,092 - INFO - joeynmt.training - Example #4
2024-05-02 13:05:53,092 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 13:05:53,092 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 13:05:53,092 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'zei@@', 'ge', 'Ihnen', 'ein', 'R@@', 'a@@', 'di@@', 'e@@', 'jen@@', 'igen', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', '.', '</s>']
2024-05-02 13:05:53,093 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 13:05:53,093 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 13:05:53,093 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich zeige Ihnen ein Radiejenigen, was in den letzten 25 Jahren passiert.
2024-05-02 13:06:32,851 - INFO - joeynmt.training - Epoch   8, Step:    31100, Batch Loss:     1.414865, Batch Acc: 0.592185, Tokens per Sec:     1833, Lr: 0.000300
2024-05-02 13:07:11,965 - INFO - joeynmt.training - Epoch   8, Step:    31200, Batch Loss:     1.477953, Batch Acc: 0.593048, Tokens per Sec:     1904, Lr: 0.000300
2024-05-02 13:07:51,295 - INFO - joeynmt.training - Epoch   8, Step:    31300, Batch Loss:     1.618289, Batch Acc: 0.592397, Tokens per Sec:     1891, Lr: 0.000300
2024-05-02 13:08:31,043 - INFO - joeynmt.training - Epoch   8, Step:    31400, Batch Loss:     1.663380, Batch Acc: 0.586019, Tokens per Sec:     1889, Lr: 0.000300
2024-05-02 13:09:09,689 - INFO - joeynmt.training - Epoch   8, Step:    31500, Batch Loss:     1.440527, Batch Acc: 0.592868, Tokens per Sec:     1984, Lr: 0.000300
2024-05-02 13:09:09,689 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 13:09:09,689 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 13:11:08,511 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   5.02, acc:   0.55, generation: 118.7105[sec], evaluation: 0.0000[sec]
2024-05-02 13:11:08,729 - INFO - joeynmt.helpers - delete models/bpe_4k/31000.ckpt
2024-05-02 13:11:08,732 - INFO - joeynmt.helpers - delete /Users/Iris/test/mt-exercise-5/models/bpe_4k/31000.ckpt
2024-05-02 13:11:08,732 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /Users/Iris/test/mt-exercise-5/models/bpe_4k/31000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/Users/Iris/test/mt-exercise-5/models/bpe_4k/31000.ckpt')
2024-05-02 13:11:08,732 - INFO - joeynmt.training - Example #0
2024-05-02 13:11:08,732 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 13:11:08,732 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 13:11:08,732 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'as', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', 'k@@', 'a@@', 'p', ',', 'die', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'L@@', 'eit@@', 'ung', 'von', 'den', 'unter@@', 'en', '4@@', '8', 'Stat@@', 'en', ',', 'die', 'Son@@', 'ne', '4@@', '8', 'Stat@@', 'en', 'Son@@', 'nen@@', 'ne', ',', 'hat', 'Son@@', 'nen@@', 'ne', ',', 'hat', 'sich', 'um', '40', 'Prozent', '.', '</s>']
2024-05-02 13:11:08,733 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 13:11:08,733 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 13:11:08,733 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dias gezeigt, dass die arktischen Eiskap, die die meisten letzten drei Millionen Jahre die Größe der Leitung von den unteren 48 Staten, die Sonne 48 Staten Sonnenne, hat Sonnenne, hat sich um 40 Prozent.
2024-05-02 13:11:08,733 - INFO - joeynmt.training - Example #1
2024-05-02 13:11:08,734 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 13:11:08,734 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 13:11:08,734 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'Stat@@', 't', 'das', 'Er@@', 'n@@', 'st', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Sch@@', 'wer@@', 't@@', 'sch@@', 'icht', 'des', 'E@@', 'is@@', 'es', 'nicht', 'zeigen', '.', '</s>']
2024-05-02 13:11:08,734 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 13:11:08,734 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 13:11:08,734 - INFO - joeynmt.training - 	Hypothesis: Aber diese UnterStatt das Ernst dieses spezielle Problem, weil es nicht die Schwertschicht des Eises nicht zeigen.
2024-05-02 13:11:08,734 - INFO - joeynmt.training - Example #2
2024-05-02 13:11:08,734 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 13:11:08,735 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 13:11:08,735 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'et@@', 'ische', 'E@@', 'is@@', 'k@@', 'u@@', 'li@@', 'eren', 'Sie', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wandel@@', 's', '.', '</s>']
2024-05-02 13:11:08,735 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 13:11:08,735 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 13:11:08,735 - INFO - joeynmt.training - 	Hypothesis: Der arktische Eisketische Eiskulieren Sie das Herz des globalen Klimawandels.
2024-05-02 13:11:08,735 - INFO - joeynmt.training - Example #3
2024-05-02 13:11:08,735 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 13:11:08,735 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 13:11:08,735 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'sich', 'in', 'den', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'im', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 13:11:08,736 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 13:11:08,736 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 13:11:08,736 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in den Sommer und Kontrakten im Sommer.
2024-05-02 13:11:08,736 - INFO - joeynmt.training - Example #4
2024-05-02 13:11:08,736 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 13:11:08,736 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 13:11:08,736 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', ',', 'dass', 'ein', 'Re@@', 'p@@', 'unkt', 'von', 'dem', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', '.', '</s>']
2024-05-02 13:11:08,737 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 13:11:08,737 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 13:11:08,737 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde, dass ein Repunkt von dem, was über die letzten 25 Jahren in den letzten 25 Jahren passiert.
2024-05-02 13:11:48,046 - INFO - joeynmt.training - Epoch   8, Step:    31600, Batch Loss:     1.380644, Batch Acc: 0.591459, Tokens per Sec:     1890, Lr: 0.000300
2024-05-02 13:12:26,798 - INFO - joeynmt.training - Epoch   8, Step:    31700, Batch Loss:     1.345267, Batch Acc: 0.588631, Tokens per Sec:     1906, Lr: 0.000300
2024-05-02 13:13:04,842 - INFO - joeynmt.training - Epoch   8, Step:    31800, Batch Loss:     1.480136, Batch Acc: 0.587026, Tokens per Sec:     1967, Lr: 0.000300
2024-05-02 13:13:43,922 - INFO - joeynmt.training - Epoch   8, Step:    31900, Batch Loss:     1.423702, Batch Acc: 0.587776, Tokens per Sec:     1873, Lr: 0.000300
2024-05-02 13:14:22,962 - INFO - joeynmt.training - Epoch   8, Step:    32000, Batch Loss:     1.523680, Batch Acc: 0.587657, Tokens per Sec:     1948, Lr: 0.000300
2024-05-02 13:14:22,962 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 13:14:22,963 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 13:16:35,964 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   5.00, acc:   0.55, generation: 132.8901[sec], evaluation: 0.0000[sec]
2024-05-02 13:16:36,178 - INFO - joeynmt.helpers - delete models/bpe_4k/28500.ckpt
2024-05-02 13:16:36,183 - INFO - joeynmt.training - Example #0
2024-05-02 13:16:36,183 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 13:16:36,183 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 13:16:36,183 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'des', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', 'k@@', 'et@@', 'te', ',', 'die', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'unter@@', 'en', '4@@', '8', 'Stat@@', 'en', ',', 'die', 'die', 'der', 'n@@', 'ie@@', 'dri@@', 'ger@@', 'en', '4@@', '8', 'Stat@@', 'en', ',', 'hat', 'sich', 'von', '40', 'Prozent', 'von', '.', '</s>']
2024-05-02 13:16:36,184 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 13:16:36,184 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 13:16:36,184 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Foldes gezeigt, dass die arktischen Eiskette, die die meisten drei Millionen Jahre die Größe der unteren 48 Staten, die die der niedrigeren 48 Staten, hat sich von 40 Prozent von.
2024-05-02 13:16:36,184 - INFO - joeynmt.training - Example #1
2024-05-02 13:16:36,184 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 13:16:36,184 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 13:16:36,185 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'hal@@', 't@@', 'ungs@@', 'ver@@', 'lang@@', 't', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'weil', 'es', 'den', 'Sch@@', 'wer@@', 't@@', 'sch@@', 'ätz@@', 't', '.', '</s>']
2024-05-02 13:16:36,185 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 13:16:36,185 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 13:16:36,185 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterhaltungsverlangt dieses spezielle Problem, weil es den Schwertschätzt.
2024-05-02 13:16:36,185 - INFO - joeynmt.training - Example #2
2024-05-02 13:16:36,185 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 13:16:36,185 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 13:16:36,185 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'et@@', 'te', 'ist', ',', 'in', 'einem', 'Sin@@', 'n', ',', 'der', 'das', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wandel@@', 's', '.', '</s>']
2024-05-02 13:16:36,186 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 13:16:36,186 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 13:16:36,186 - INFO - joeynmt.training - 	Hypothesis: Der arktische Eiskette ist, in einem Sinn, der das das Herz des globalen Klimawandels.
2024-05-02 13:16:36,186 - INFO - joeynmt.training - Example #3
2024-05-02 13:16:36,186 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 13:16:36,186 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 13:16:36,186 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'den', 'Win@@', 'ter', 'und', 'Ver@@', 'fol@@', 'g@@', 's@@', 'mit@@', 'tel', 'im', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 13:16:36,187 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 13:16:36,187 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 13:16:36,187 - INFO - joeynmt.training - 	Hypothesis: Es gibt in den Winter und Verfolgsmittel im Sommer.
2024-05-02 13:16:36,187 - INFO - joeynmt.training - Example #4
2024-05-02 13:16:36,187 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 13:16:36,187 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 13:16:36,187 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'wird', 'ein', 'R@@', 'a@@', 'di@@', 'er@@', '-@@', 'F@@', 'el@@', 'd', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 13:16:36,188 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 13:16:36,188 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 13:16:36,188 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, wird ein Radier-Feld, was über die letzten 25 Jahren passiert ist.
2024-05-02 13:17:16,746 - INFO - joeynmt.training - Epoch   8, Step:    32100, Batch Loss:     1.402428, Batch Acc: 0.583405, Tokens per Sec:     1836, Lr: 0.000300
2024-05-02 13:17:55,579 - INFO - joeynmt.training - Epoch   8, Step:    32200, Batch Loss:     1.545950, Batch Acc: 0.585629, Tokens per Sec:     1890, Lr: 0.000300
2024-05-02 13:18:36,287 - INFO - joeynmt.training - Epoch   8, Step:    32300, Batch Loss:     1.625178, Batch Acc: 0.584560, Tokens per Sec:     1781, Lr: 0.000300
2024-05-02 13:19:16,290 - INFO - joeynmt.training - Epoch   8, Step:    32400, Batch Loss:     1.468237, Batch Acc: 0.587048, Tokens per Sec:     1874, Lr: 0.000300
2024-05-02 13:19:55,138 - INFO - joeynmt.training - Epoch   8, Step:    32500, Batch Loss:     1.462690, Batch Acc: 0.590813, Tokens per Sec:     1924, Lr: 0.000300
2024-05-02 13:19:55,138 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 13:19:55,139 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 13:21:47,079 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   4.99, acc:   0.55, generation: 111.8261[sec], evaluation: 0.0000[sec]
2024-05-02 13:21:47,080 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 13:21:47,296 - INFO - joeynmt.helpers - delete models/bpe_4k/29500.ckpt
2024-05-02 13:21:47,299 - INFO - joeynmt.training - Example #0
2024-05-02 13:21:47,299 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 13:21:47,300 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 13:21:47,300 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'S@@', 'li@@', 'den', 'so', ',', 'dass', 'die', 'Ar@@', 'kt@@', 'is', ',', 'die', 'die', 'Ar@@', 'kt@@', 'is', ',', 'die', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'des', 'n@@', 'ie@@', 'dri@@', 'g@@', 'ten', '4@@', '8', 'Stat@@', 'en', ',', 'die', 'von', '40', '%', 'ver@@', 'ges@@', 'sen', 'hat', ',', 'um', '40', '%', 'ver@@', 'ges@@', 'sen', '.', '</s>']
2024-05-02 13:21:47,300 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 13:21:47,300 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 13:21:47,300 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Sliden so, dass die Arktis, die die Arktis, die die meisten drei Millionen Jahre die Größe der letzten drei Millionen Jahre die Größe des niedrigten 48 Staten, die von 40% vergessen hat, um 40% vergessen.
2024-05-02 13:21:47,300 - INFO - joeynmt.training - Example #1
2024-05-02 13:21:47,301 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 13:21:47,301 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 13:21:47,301 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'hal@@', 't@@', 'ungs@@', 'ver@@', 'lang@@', 't', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'denn', 'es', 'nicht', 'den', 'Sch@@', 'w@@', 'ein', '.', '</s>']
2024-05-02 13:21:47,301 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 13:21:47,301 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 13:21:47,301 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterhaltungsverlangt dieses spezielle Problem, denn es nicht den Schwein.
2024-05-02 13:21:47,301 - INFO - joeynmt.training - Example #2
2024-05-02 13:21:47,302 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 13:21:47,302 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 13:21:47,302 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'tik@@', 'et@@', 'ische', 'E@@', 'is@@', 'k@@', 'et@@', 'te', 'ist', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'das', 'Er@@', 'n@@', 'ähr@@', 'ung', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', '-@@', 'System', '.', '</s>']
2024-05-02 13:21:47,302 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 13:21:47,302 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 13:21:47,302 - INFO - joeynmt.training - 	Hypothesis: Die Artiketische Eiskette ist in einem Sinne, der das Ernährung des globalen Klima-System.
2024-05-02 13:21:47,302 - INFO - joeynmt.training - Example #3
2024-05-02 13:21:47,302 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 13:21:47,302 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 13:21:47,303 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'in', 'den', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'im', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 13:21:47,303 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 13:21:47,303 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 13:21:47,303 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in den Winter und Kontrakten im Sommer.
2024-05-02 13:21:47,303 - INFO - joeynmt.training - Example #4
2024-05-02 13:21:47,303 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 13:21:47,303 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 13:21:47,303 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', 'ein', 'R@@', 'a@@', 'di@@', 'er@@', '-@@', 'F@@', 'ast@@', '-@@', 'vor@@', 'wär@@', 'ts', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 13:21:47,304 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 13:21:47,304 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 13:21:47,304 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige ein Radier-Fast-vorwärts, was über die letzten 25 Jahren passiert ist.
2024-05-02 13:22:27,203 - INFO - joeynmt.training - Epoch   8, Step:    32600, Batch Loss:     1.409354, Batch Acc: 0.582201, Tokens per Sec:     1845, Lr: 0.000300
2024-05-02 13:23:07,227 - INFO - joeynmt.training - Epoch   8, Step:    32700, Batch Loss:     1.607809, Batch Acc: 0.584188, Tokens per Sec:     1892, Lr: 0.000300
2024-05-02 13:23:47,445 - INFO - joeynmt.training - Epoch   8, Step:    32800, Batch Loss:     1.319471, Batch Acc: 0.587553, Tokens per Sec:     1799, Lr: 0.000300
2024-05-02 13:24:28,711 - INFO - joeynmt.training - Epoch   8, Step:    32900, Batch Loss:     1.566975, Batch Acc: 0.582959, Tokens per Sec:     1813, Lr: 0.000300
2024-05-02 13:25:17,373 - INFO - joeynmt.training - Epoch   8, Step:    33000, Batch Loss:     1.351547, Batch Acc: 0.589030, Tokens per Sec:     1508, Lr: 0.000300
2024-05-02 13:25:17,373 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 13:25:17,373 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 13:27:29,513 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   5.00, acc:   0.55, generation: 132.0279[sec], evaluation: 0.0000[sec]
2024-05-02 13:27:29,728 - INFO - joeynmt.helpers - delete models/bpe_4k/31500.ckpt
2024-05-02 13:27:29,733 - INFO - joeynmt.training - Example #0
2024-05-02 13:27:29,733 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 13:27:29,733 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 13:27:29,733 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'as', 'so', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', 'k@@', 'tik@@', 'et@@', 'ische', 'E@@', 'is@@', 'k@@', 'et@@', 'te', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'des', 'n@@', 'ie@@', 'dri@@', 'gen', '4@@', '8', 'Stat@@', 'en', ',', 'hat', 'sich', 'Son@@', 'nen@@', 'st@@', 'age', 'um', '40', 'Prozent', '.', '</s>']
2024-05-02 13:27:29,734 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 13:27:29,734 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 13:27:29,734 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dias so, dass die arktischen Eisktiketische Eiskette, die für die meisten drei Millionen Jahre die Größe des niedrigen 48 Staten, hat sich Sonnenstage um 40 Prozent.
2024-05-02 13:27:29,734 - INFO - joeynmt.training - Example #1
2024-05-02 13:27:29,734 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 13:27:29,734 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 13:27:29,734 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'h@@', 'än@@', 'gen', 'das', 'Er@@', 'n@@', 'st', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'das', 'Sch@@', 'ick@@', 's@@', 'wer@@', 't', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 13:27:29,735 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 13:27:29,735 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 13:27:29,735 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterhängen das Ernst dieses spezielle Problem, weil es nicht das Schickswert des Eis.
2024-05-02 13:27:29,735 - INFO - joeynmt.training - Example #2
2024-05-02 13:27:29,735 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 13:27:29,735 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 13:27:29,735 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'ti@@', 'k', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 13:27:29,736 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 13:27:29,736 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 13:27:29,736 - INFO - joeynmt.training - 	Hypothesis: Der arktische Eisktik ist, in einem Sinne, der das Herz des globalen Klimawandel.
2024-05-02 13:27:29,736 - INFO - joeynmt.training - Example #3
2024-05-02 13:27:29,736 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 13:27:29,736 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 13:27:29,736 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'hl@@', 'en', 'im', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 13:27:29,736 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 13:27:29,736 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 13:27:29,736 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontrahlen im Sommer.
2024-05-02 13:27:29,737 - INFO - joeynmt.training - Example #4
2024-05-02 13:27:29,737 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 13:27:29,737 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 13:27:29,737 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'zei@@', 'ge', ',', 'wird', 'ein', 'R@@', 'a@@', 'di@@', 'en@@', 'st@@', '-@@', 'F@@', 'el@@', 'sei@@', 'te', 'des', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 13:27:29,737 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 13:27:29,737 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 13:27:29,737 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich zeige, wird ein Radienst-Felseite des, was über die letzten 25 Jahren passiert ist.
2024-05-02 13:28:07,026 - INFO - joeynmt.training - Epoch   8, Step:    33100, Batch Loss:     1.425775, Batch Acc: 0.584248, Tokens per Sec:     1962, Lr: 0.000300
2024-05-02 13:28:45,116 - INFO - joeynmt.training - Epoch   8, Step:    33200, Batch Loss:     1.417019, Batch Acc: 0.582428, Tokens per Sec:     1937, Lr: 0.000300
2024-05-02 13:29:22,864 - INFO - joeynmt.training - Epoch   8, Step:    33300, Batch Loss:     1.551055, Batch Acc: 0.588942, Tokens per Sec:     1948, Lr: 0.000300
2024-05-02 13:30:00,327 - INFO - joeynmt.training - Epoch   8, Step:    33400, Batch Loss:     1.527968, Batch Acc: 0.585959, Tokens per Sec:     2022, Lr: 0.000300
2024-05-02 13:30:38,977 - INFO - joeynmt.training - Epoch   8, Step:    33500, Batch Loss:     1.688820, Batch Acc: 0.585140, Tokens per Sec:     1954, Lr: 0.000300
2024-05-02 13:30:38,977 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 13:30:38,977 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 13:32:30,200 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.60, ppl:   4.95, acc:   0.55, generation: 111.1121[sec], evaluation: 0.0000[sec]
2024-05-02 13:32:30,210 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 13:32:30,410 - INFO - joeynmt.helpers - delete models/bpe_4k/30500.ckpt
2024-05-02 13:32:30,416 - INFO - joeynmt.training - Example #0
2024-05-02 13:32:30,416 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 13:32:30,416 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 13:32:30,416 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'des', 'zeig@@', 'te', ',', 'so', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', 'k@@', 'et@@', 'te', ',', 'die', 'die', 'für', 'die', 'meisten', '4@@', '8', 'Stat@@', 'en', ',', 'die', 'die', 'Größ@@', 'e', 'des', 'unter@@', 'en', '4@@', '8', 'Stat@@', 'en', ',', 'hat', 'Son@@', 'nen@@', 'be@@', 'l', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 13:32:30,417 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 13:32:30,417 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 13:32:30,417 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Foldes zeigte, so dass die arktischen Eiskette, die die für die meisten 48 Staten, die die Größe des unteren 48 Staten, hat Sonnenbel von 40 Prozent.
2024-05-02 13:32:30,417 - INFO - joeynmt.training - Example #1
2024-05-02 13:32:30,417 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 13:32:30,417 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 13:32:30,417 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'h@@', 'än@@', 'gen', 'das', 'Er@@', 'n@@', 'st', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'd@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is@@', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 13:32:30,418 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 13:32:30,418 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 13:32:30,418 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterhängen das Ernst dieses Problem, weil es nicht die dickness des Eises nicht die Dickness des Eis.
2024-05-02 13:32:30,418 - INFO - joeynmt.training - Example #2
2024-05-02 13:32:30,418 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 13:32:30,418 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 13:32:30,418 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', ',', 'in', 'ge@@', 'wis@@', 'ser', 'Weise', ',', 'das', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wandel@@', 's', '.', '</s>']
2024-05-02 13:32:30,419 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 13:32:30,419 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 13:32:30,419 - INFO - joeynmt.training - 	Hypothesis: Das arktische Eis ist, in gewisser Weise, das das Herz des globalen Klimawandels.
2024-05-02 13:32:30,419 - INFO - joeynmt.training - Example #3
2024-05-02 13:32:30,419 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 13:32:30,419 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 13:32:30,419 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'weiter@@', 't', 'in', 'Win@@', 'ter', 'und', 'Ver@@', 'fol@@', 'gen', 'in', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'fol@@', 'gen', '.', '</s>']
2024-05-02 13:32:30,419 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 13:32:30,419 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 13:32:30,420 - INFO - joeynmt.training - 	Hypothesis: Es erweitert in Winter und Verfolgen in Sommer und Verfolgen.
2024-05-02 13:32:30,420 - INFO - joeynmt.training - Example #4
2024-05-02 13:32:30,420 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 13:32:30,420 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 13:32:30,420 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'dass', 'Sie', 'eine', 'Re@@', 'p@@', 'unkt', 'von', 'dem', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahre', 'passiert', 'ist', '.', '</s>']
2024-05-02 13:32:30,420 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 13:32:30,420 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 13:32:30,420 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, dass Sie eine Repunkt von dem, was über die letzten 25 Jahre passiert ist.
2024-05-02 13:33:07,698 - INFO - joeynmt.training - Epoch   8, Step:    33600, Batch Loss:     1.383074, Batch Acc: 0.577858, Tokens per Sec:     1982, Lr: 0.000300
2024-05-02 13:33:45,244 - INFO - joeynmt.training - Epoch   8, Step:    33700, Batch Loss:     1.506193, Batch Acc: 0.584812, Tokens per Sec:     2003, Lr: 0.000300
2024-05-02 13:34:23,145 - INFO - joeynmt.training - Epoch   8, Step:    33800, Batch Loss:     1.248642, Batch Acc: 0.588752, Tokens per Sec:     1957, Lr: 0.000300
2024-05-02 13:35:01,399 - INFO - joeynmt.training - Epoch   8, Step:    33900, Batch Loss:     1.442136, Batch Acc: 0.586900, Tokens per Sec:     1952, Lr: 0.000300
2024-05-02 13:35:38,545 - INFO - joeynmt.training - Epoch   8, Step:    34000, Batch Loss:     1.745308, Batch Acc: 0.584499, Tokens per Sec:     1947, Lr: 0.000300
2024-05-02 13:35:38,546 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 13:35:38,546 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 13:38:29,215 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.60, ppl:   4.95, acc:   0.55, generation: 170.5594[sec], evaluation: 0.0000[sec]
2024-05-02 13:38:29,216 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 13:38:29,427 - INFO - joeynmt.helpers - delete models/bpe_4k/33000.ckpt
2024-05-02 13:38:29,430 - INFO - joeynmt.training - Example #0
2024-05-02 13:38:29,430 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 13:38:29,430 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 13:38:29,430 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Di@@', 'as', 'ge@@', 'zeigt', ',', 'dass', 'der', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', 'sch@@', 'e@@', 'f', ',', 'die', 'die', 'größ@@', 'te', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'Größ@@', 'e', 'der', 'ver@@', 'gan@@', 'gen@@', 'en', '4@@', '8', 'Sta@@', 'aten', ',', 'hat', 'Son@@', 'nen@@', 'st@@', 'un@@', 'k', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 13:38:29,431 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 13:38:29,431 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 13:38:29,431 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Dias gezeigt, dass der arktischen Eisschef, die die größte drei Millionen Jahre lang der Größe der vergangenen 48 Staaten, hat Sonnenstunk von 40 Prozent.
2024-05-02 13:38:29,431 - INFO - joeynmt.training - Example #1
2024-05-02 13:38:29,431 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 13:38:29,431 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 13:38:29,431 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'des', 'Er@@', 'n@@', 'st', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'd@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is@@', 'en@@', 'heit', 'zeigt', '.', '</s>']
2024-05-02 13:38:29,431 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 13:38:29,432 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 13:38:29,432 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung des Ernst dieses Problem, weil es nicht die dickness des Eisenheit zeigt.
2024-05-02 13:38:29,432 - INFO - joeynmt.training - Example #2
2024-05-02 13:38:29,432 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 13:38:29,432 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 13:38:29,432 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'sch@@', 'r@@', 'än@@', 'kt', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'das', 'das', 'das', 'welt@@', 'wei@@', 'te', 'Kli@@', 'ma@@', '-@@', 'B@@', 'erg@@', '-@@', 'System', '.', '</s>']
2024-05-02 13:38:29,432 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 13:38:29,432 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 13:38:29,432 - INFO - joeynmt.training - 	Hypothesis: Der arktische Eisschränkt, in einem Sinne, der das das das weltweite Klima-Berg-System.
2024-05-02 13:38:29,432 - INFO - joeynmt.training - Example #3
2024-05-02 13:38:29,433 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 13:38:29,433 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 13:38:29,433 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'sich', 'in', 'den', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'im', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 13:38:29,433 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 13:38:29,433 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 13:38:29,433 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in den Winter und Kontrakten im Sommer.
2024-05-02 13:38:29,433 - INFO - joeynmt.training - Example #4
2024-05-02 13:38:29,433 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 13:38:29,433 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 13:38:29,433 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'dass', 'ein', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', 'es', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 13:38:29,434 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 13:38:29,434 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 13:38:29,434 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, dass ein schnelles schnelles, was über die letzten 25 Jahren passiert ist.
2024-05-02 13:39:08,240 - INFO - joeynmt.training - Epoch   8, Step:    34100, Batch Loss:     1.427816, Batch Acc: 0.580991, Tokens per Sec:     1915, Lr: 0.000300
2024-05-02 13:39:45,328 - INFO - joeynmt.training - Epoch   8, Step:    34200, Batch Loss:     1.439668, Batch Acc: 0.583568, Tokens per Sec:     2011, Lr: 0.000300
2024-05-02 13:40:23,865 - INFO - joeynmt.training - Epoch   8, Step:    34300, Batch Loss:     1.512490, Batch Acc: 0.585309, Tokens per Sec:     1967, Lr: 0.000300
2024-05-02 13:41:00,298 - INFO - joeynmt.training - Epoch   8, Step:    34400, Batch Loss:     1.468051, Batch Acc: 0.577897, Tokens per Sec:     1992, Lr: 0.000300
2024-05-02 13:41:37,817 - INFO - joeynmt.training - Epoch   8, Step:    34500, Batch Loss:     1.414956, Batch Acc: 0.588646, Tokens per Sec:     2036, Lr: 0.000300
2024-05-02 13:41:37,817 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 13:41:37,817 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 13:43:46,682 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.59, ppl:   4.89, acc:   0.55, generation: 128.7628[sec], evaluation: 0.0000[sec]
2024-05-02 13:43:46,683 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 13:43:46,877 - INFO - joeynmt.helpers - delete models/bpe_4k/32000.ckpt
2024-05-02 13:43:46,879 - INFO - joeynmt.training - Example #0
2024-05-02 13:43:46,880 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 13:43:46,880 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 13:43:46,880 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Di@@', 'as', 'ge@@', 'zeigt', ',', 'dass', 'die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', '-@@', 'Z@@', 'ah@@', 'p', ',', 'die', 'die', 'meisten', '4@@', '8', 'Sta@@', 'aten', ',', 'die', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'aten', 'der', 'n@@', 'ie@@', 'dri@@', 'g@@', 'sten', '4@@', '8', 'Sta@@', 'aten', ',', 'die', 'Son@@', 'ne', '40', 'Prozent', '.', '</s>']
2024-05-02 13:43:46,880 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 13:43:46,880 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 13:43:46,880 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Dias gezeigt, dass die Arktische Eis-Zahp, die die meisten 48 Staaten, die die Größe 48 Staaten der niedrigsten 48 Staaten, die Sonne 40 Prozent.
2024-05-02 13:43:46,880 - INFO - joeynmt.training - Example #1
2024-05-02 13:43:46,881 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 13:43:46,881 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 13:43:46,881 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'des', 'S@@', 'eri@@', 'es', 'Problem', ',', 'denn', 'es', 'gibt', 'nicht', 'die', 'Sch@@', 'utz@@', 's@@', 'au@@', 'ber', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 13:43:46,881 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 13:43:46,881 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 13:43:46,881 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung des Series Problem, denn es gibt nicht die Schutzsauber des Eis.
2024-05-02 13:43:46,881 - INFO - joeynmt.training - Example #2
2024-05-02 13:43:46,881 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 13:43:46,881 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 13:43:46,881 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'sch@@', 'icht', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', '-@@', 'B@@', 'erg@@', '-@@', 'System', '.', '</s>']
2024-05-02 13:43:46,882 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 13:43:46,882 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 13:43:46,882 - INFO - joeynmt.training - 	Hypothesis: Der Arktische Eisschicht ist, in einem Sinne, der das Herz des globalen Klima-Berg-System.
2024-05-02 13:43:46,882 - INFO - joeynmt.training - Example #3
2024-05-02 13:43:46,882 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 13:43:46,882 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 13:43:46,882 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'sich', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'ti@@', 'k', '.', '</s>']
2024-05-02 13:43:46,883 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 13:43:46,883 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 13:43:46,883 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und Kontrakten in Sommer und Kontraktik.
2024-05-02 13:43:46,883 - INFO - joeynmt.training - Example #4
2024-05-02 13:43:46,883 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 13:43:46,883 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 13:43:46,883 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Di@@', 'a', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', 'ein', 'schn@@', 'ell@@', 'es', 'schn@@', 'eller', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', '.', '</s>']
2024-05-02 13:43:46,883 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 13:43:46,883 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 13:43:46,884 - INFO - joeynmt.training - 	Hypothesis: Die nächste Dia, die ich Ihnen zeige ein schnelles schneller von dem, was in den letzten 25 Jahren passiert.
2024-05-02 13:44:03,329 - INFO - joeynmt.training - Epoch   8: total training loss 6281.55
2024-05-02 13:44:03,329 - INFO - joeynmt.training - EPOCH 9
2024-05-02 13:44:24,266 - INFO - joeynmt.training - Epoch   9, Step:    34600, Batch Loss:     1.435715, Batch Acc: 0.605250, Tokens per Sec:     2047, Lr: 0.000300
2024-05-02 13:45:01,282 - INFO - joeynmt.training - Epoch   9, Step:    34700, Batch Loss:     1.455113, Batch Acc: 0.607576, Tokens per Sec:     2052, Lr: 0.000300
2024-05-02 13:45:39,948 - INFO - joeynmt.training - Epoch   9, Step:    34800, Batch Loss:     1.367005, Batch Acc: 0.604739, Tokens per Sec:     2007, Lr: 0.000300
2024-05-02 13:46:17,116 - INFO - joeynmt.training - Epoch   9, Step:    34900, Batch Loss:     1.511700, Batch Acc: 0.609233, Tokens per Sec:     1985, Lr: 0.000300
2024-05-02 13:46:55,281 - INFO - joeynmt.training - Epoch   9, Step:    35000, Batch Loss:     1.302077, Batch Acc: 0.603839, Tokens per Sec:     1963, Lr: 0.000300
2024-05-02 13:46:55,281 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 13:46:55,281 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 13:48:49,522 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.59, ppl:   4.92, acc:   0.55, generation: 114.1337[sec], evaluation: 0.0000[sec]
2024-05-02 13:48:49,724 - INFO - joeynmt.helpers - delete models/bpe_4k/30000.ckpt
2024-05-02 13:48:49,726 - INFO - joeynmt.training - Example #0
2024-05-02 13:48:49,727 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 13:48:49,727 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 13:48:49,727 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'gen', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', '-@@', 'Z@@', 'ah@@', 'n@@', 'b@@', 'ah@@', 'n', ',', 'die', 'die', 'die', 'meisten', '4@@', '8', 'Zu@@', 'ges@@', 't@@', 'ei@@', 'dig@@', 't', ',', 'die', 'die', 'meisten', '4@@', '8', 'Zu@@', 'g@@', 'ri@@', 'ff', ',', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 13:48:49,727 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 13:48:49,728 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 13:48:49,728 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folgen gezeigt, dass die arktische Eis-Zahnbahn, die die die meisten 48 Zugesteidigt, die die meisten 48 Zugriff, von 40 Prozent.
2024-05-02 13:48:49,728 - INFO - joeynmt.training - Example #1
2024-05-02 13:48:49,728 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 13:48:49,728 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 13:48:49,728 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'h@@', 'än@@', 'd@@', 'ungs@@', 'ver@@', 'fol@@', 'ge', 'dieses', 'Problem', ',', 'weil', 'es', 'das', 'Sch@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is', 'nicht', 'zeigt', '.', '</s>']
2024-05-02 13:48:49,728 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 13:48:49,728 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 13:48:49,728 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterhändungsverfolge dieses Problem, weil es das Schicksal des Eis nicht zeigt.
2024-05-02 13:48:49,729 - INFO - joeynmt.training - Example #2
2024-05-02 13:48:49,729 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 13:48:49,729 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 13:48:49,729 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', ',', 'in', 'ge@@', 'wis@@', 'ser', 'Weise', ',', 'das', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', '-@@', 'Kli@@', 'ma@@', '-@@', 'System', '.', '</s>']
2024-05-02 13:48:49,729 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 13:48:49,729 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 13:48:49,729 - INFO - joeynmt.training - 	Hypothesis: Die arktische Eis ist, in gewisser Weise, das das Herz des globalen Klima-Klima-System.
2024-05-02 13:48:49,729 - INFO - joeynmt.training - Example #3
2024-05-02 13:48:49,730 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 13:48:49,730 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 13:48:49,730 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'fol@@', 'gen', '.', '</s>']
2024-05-02 13:48:49,730 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 13:48:49,730 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 13:48:49,730 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Kontrakte im Sommer und Verfolgen.
2024-05-02 13:48:49,730 - INFO - joeynmt.training - Example #4
2024-05-02 13:48:49,730 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 13:48:49,730 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 13:48:49,731 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', 'ein', 'schn@@', 'eller', 'F@@', 'ast@@', '-@@', 'Vor@@', 'wär@@', 'ts@@', 'k@@', 'rä@@', 'f@@', 'ten', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passier@@', 'te', '.', '</s>']
2024-05-02 13:48:49,731 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 13:48:49,731 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 13:48:49,731 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige ein schneller Fast-Vorwärtskräften, was in den letzten 25 Jahren passierte.
2024-05-02 13:49:28,345 - INFO - joeynmt.training - Epoch   9, Step:    35100, Batch Loss:     1.310915, Batch Acc: 0.606367, Tokens per Sec:     1928, Lr: 0.000300
2024-05-02 13:50:06,176 - INFO - joeynmt.training - Epoch   9, Step:    35200, Batch Loss:     1.394701, Batch Acc: 0.602166, Tokens per Sec:     2004, Lr: 0.000300
2024-05-02 13:50:45,583 - INFO - joeynmt.training - Epoch   9, Step:    35300, Batch Loss:     1.414327, Batch Acc: 0.599768, Tokens per Sec:     1836, Lr: 0.000300
2024-05-02 13:51:25,004 - INFO - joeynmt.training - Epoch   9, Step:    35400, Batch Loss:     1.427514, Batch Acc: 0.601505, Tokens per Sec:     1882, Lr: 0.000300
2024-05-02 13:52:05,181 - INFO - joeynmt.training - Epoch   9, Step:    35500, Batch Loss:     1.281208, Batch Acc: 0.599775, Tokens per Sec:     1858, Lr: 0.000300
2024-05-02 13:52:05,181 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 13:52:05,181 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 13:54:25,782 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.60, ppl:   4.96, acc:   0.55, generation: 140.4882[sec], evaluation: 0.0000[sec]
2024-05-02 13:54:26,002 - INFO - joeynmt.helpers - delete models/bpe_4k/32500.ckpt
2024-05-02 13:54:26,008 - INFO - joeynmt.training - Example #0
2024-05-02 13:54:26,008 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 13:54:26,008 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 13:54:26,008 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'as', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', '-@@', 'Z@@', 'ah@@', 'p', ',', 'die', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'Größ@@', 'e', 'von', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'Größ@@', 'e', '4@@', '8', 'Stat@@', 'en', ',', 'Son@@', 'nen@@', 'ne', 'um', '40', 'Prozent', '.', '</s>']
2024-05-02 13:54:26,009 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 13:54:26,009 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 13:54:26,009 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dias gezeigt, dass die arktische Eis-Zahp, die die meisten drei Millionen Jahre lang der Größe von der letzten drei Millionen Jahre lang der Größe 48 Staten, Sonnenne um 40 Prozent.
2024-05-02 13:54:26,009 - INFO - joeynmt.training - Example #1
2024-05-02 13:54:26,009 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 13:54:26,009 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 13:54:26,009 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'such@@', 'ung', 'das', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'der', 'E@@', 'is', '.', '</s>']
2024-05-02 13:54:26,010 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 13:54:26,010 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 13:54:26,010 - INFO - joeynmt.training - 	Hypothesis: Aber diese Untersuchung das Ernsthaft dieses spezielle Problem, weil es nicht die Dickness der Eis.
2024-05-02 13:54:26,010 - INFO - joeynmt.training - Example #2
2024-05-02 13:54:26,010 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 13:54:26,010 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 13:54:26,010 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'z@@', 't@@', 'ische', 'E@@', 'is@@', 'z@@', '-@@', 'Z@@', 'ah@@', 'p', 'ist', ',', 'der', 'Bi@@', 'en@@', 'en@@', 'system', '.', '</s>']
2024-05-02 13:54:26,011 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 13:54:26,011 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 13:54:26,011 - INFO - joeynmt.training - 	Hypothesis: Der Arztische Eisz-Zahp ist, der Bienensystem.
2024-05-02 13:54:26,011 - INFO - joeynmt.training - Example #3
2024-05-02 13:54:26,011 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 13:54:26,011 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 13:54:26,011 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'weiter@@', 't', 'sich', 'in', 'Win@@', 'ter', 'und', 'Ver@@', 'fol@@', 'g@@', 's@@', 'amm@@', 'eln', 'in', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 13:54:26,012 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 13:54:26,012 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 13:54:26,012 - INFO - joeynmt.training - 	Hypothesis: Es erweitert sich in Winter und Verfolgsammeln in Sommer.
2024-05-02 13:54:26,012 - INFO - joeynmt.training - Example #4
2024-05-02 13:54:26,012 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 13:54:26,012 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 13:54:26,012 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'wird', 'ein', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', 'es', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'in', 'die', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 13:54:26,013 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 13:54:26,013 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 13:54:26,013 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, wird ein schnelles schnelles von dem, was in den letzten 25 Jahren in die letzten 25 Jahren passiert ist.
2024-05-02 13:55:06,823 - INFO - joeynmt.training - Epoch   9, Step:    35600, Batch Loss:     1.421028, Batch Acc: 0.592412, Tokens per Sec:     1783, Lr: 0.000300
2024-05-02 13:55:46,536 - INFO - joeynmt.training - Epoch   9, Step:    35700, Batch Loss:     1.588984, Batch Acc: 0.597909, Tokens per Sec:     1833, Lr: 0.000300
2024-05-02 13:56:26,021 - INFO - joeynmt.training - Epoch   9, Step:    35800, Batch Loss:     1.434726, Batch Acc: 0.595636, Tokens per Sec:     1901, Lr: 0.000300
2024-05-02 13:57:05,824 - INFO - joeynmt.training - Epoch   9, Step:    35900, Batch Loss:     1.193225, Batch Acc: 0.592221, Tokens per Sec:     1895, Lr: 0.000300
2024-05-02 13:57:45,537 - INFO - joeynmt.training - Epoch   9, Step:    36000, Batch Loss:     1.305189, Batch Acc: 0.597696, Tokens per Sec:     1865, Lr: 0.000300
2024-05-02 13:57:45,538 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 13:57:45,538 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 14:00:09,780 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.60, ppl:   4.93, acc:   0.55, generation: 144.1266[sec], evaluation: 0.0000[sec]
2024-05-02 14:00:10,003 - INFO - joeynmt.helpers - delete models/bpe_4k/35500.ckpt
2024-05-02 14:00:10,008 - INFO - joeynmt.helpers - delete /Users/Iris/test/mt-exercise-5/models/bpe_4k/35500.ckpt
2024-05-02 14:00:10,008 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /Users/Iris/test/mt-exercise-5/models/bpe_4k/35500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/Users/Iris/test/mt-exercise-5/models/bpe_4k/35500.ckpt')
2024-05-02 14:00:10,009 - INFO - joeynmt.training - Example #0
2024-05-02 14:00:10,009 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 14:00:10,009 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 14:00:10,009 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'as', 'ge@@', 'zeigt', ',', 'dass', 'die', 'Ar@@', 'kt@@', 'is', 'ge@@', 'zeigt', ',', 'dass', 'die', 'Ar@@', 'kt@@', 'is', ',', 'die', 'für', 'die', 'meisten', 'von', 'den', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'aten', ',', 'die', 'Son@@', 'nen@@', 'unter@@', 'e', '4@@', '8', 'Sta@@', 'aten', ',', 'die', 'Son@@', 'ne', 'um', '40', 'Prozent', '.', '</s>']
2024-05-02 14:00:10,010 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 14:00:10,010 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 14:00:10,010 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dias gezeigt, dass die Arktis gezeigt, dass die Arktis, die für die meisten von den letzten drei Millionen Jahre der Größe 48 Staaten, die Sonnenuntere 48 Staaten, die Sonne um 40 Prozent.
2024-05-02 14:00:10,010 - INFO - joeynmt.training - Example #1
2024-05-02 14:00:10,010 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 14:00:10,010 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 14:00:10,010 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'des', 'Er@@', 'n@@', 'n@@', 'ungs@@', 'ver@@', 'fahren', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'den', 'd@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is', 'nicht', 'zeigt', '.', '</s>']
2024-05-02 14:00:10,011 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 14:00:10,011 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 14:00:10,011 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung des Ernnungsverfahren dieses Problem, weil es nicht den dickness des Eis nicht zeigt.
2024-05-02 14:00:10,011 - INFO - joeynmt.training - Example #2
2024-05-02 14:00:10,011 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 14:00:10,011 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 14:00:10,011 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'tik@@', 'el', 'ist', 'in', 'einem', 'Sin@@', 'ne', ',', 'das', 'das', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wandel@@', 's', '.', '</s>']
2024-05-02 14:00:10,011 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 14:00:10,012 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 14:00:10,012 - INFO - joeynmt.training - 	Hypothesis: Das Artikel ist in einem Sinne, das das das Herz des globalen Klimawandels.
2024-05-02 14:00:10,012 - INFO - joeynmt.training - Example #3
2024-05-02 14:00:10,012 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 14:00:10,012 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 14:00:10,012 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'sich', 'in', 'Win@@', 'ter', 'und', 'Ver@@', 'halten', '.', '</s>']
2024-05-02 14:00:10,012 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 14:00:10,012 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 14:00:10,012 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und Verhalten.
2024-05-02 14:00:10,012 - INFO - joeynmt.training - Example #4
2024-05-02 14:00:10,013 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 14:00:10,013 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 14:00:10,013 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zeigen', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', 'ein', 'R@@', 'as@@', 'i@@', 'd', 'sein', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', '.', '</s>']
2024-05-02 14:00:10,013 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 14:00:10,013 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 14:00:10,013 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeigen, die ich Ihnen zeigen werde ein Rasid sein, was in den letzten 25 Jahren passiert.
2024-05-02 14:00:50,499 - INFO - joeynmt.training - Epoch   9, Step:    36100, Batch Loss:     1.269287, Batch Acc: 0.595702, Tokens per Sec:     1841, Lr: 0.000300
2024-05-02 14:01:32,187 - INFO - joeynmt.training - Epoch   9, Step:    36200, Batch Loss:     1.336950, Batch Acc: 0.592127, Tokens per Sec:     1807, Lr: 0.000300
2024-05-02 14:02:13,263 - INFO - joeynmt.training - Epoch   9, Step:    36300, Batch Loss:     1.577515, Batch Acc: 0.593755, Tokens per Sec:     1824, Lr: 0.000300
2024-05-02 14:02:54,803 - INFO - joeynmt.training - Epoch   9, Step:    36400, Batch Loss:     1.417652, Batch Acc: 0.600843, Tokens per Sec:     1834, Lr: 0.000300
2024-05-02 14:03:37,884 - INFO - joeynmt.training - Epoch   9, Step:    36500, Batch Loss:     1.315292, Batch Acc: 0.593740, Tokens per Sec:     1712, Lr: 0.000300
2024-05-02 14:03:37,885 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 14:03:37,885 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 14:12:36,579 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.59, ppl:   4.92, acc:   0.55, generation: 538.5886[sec], evaluation: 0.0000[sec]
2024-05-02 14:12:36,802 - INFO - joeynmt.helpers - delete models/bpe_4k/33500.ckpt
2024-05-02 14:12:36,809 - INFO - joeynmt.training - Example #0
2024-05-02 14:12:36,809 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 14:12:36,809 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 14:12:36,809 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'gen', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', 'sch@@', 'u@@', 'f', ',', 'die', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'ver@@', 'gan@@', 'gen@@', 'en', '4@@', '8', 'Sta@@', 'aten', ',', 'die', 'Son@@', 'ne', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 14:12:36,809 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 14:12:36,809 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 14:12:36,810 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Folgen gezeigt, dass die arktischen Eisschuf, die die meisten drei Millionen Jahre die Größe der vergangenen 48 Staaten, die Sonne von 40 Prozent.
2024-05-02 14:12:36,810 - INFO - joeynmt.training - Example #1
2024-05-02 14:12:36,810 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 14:12:36,810 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 14:12:36,810 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'des', 'Er@@', 'n@@', 'st', 'dieses', 'Problem', ',', 'weil', 'es', 'den', 'D@@', 'ick@@', 'n@@', 'ess', 'nicht', 'den', 'Sch@@', 'w@@', 'eine', 'nicht', 'zeigen', '.', '</s>']
2024-05-02 14:12:36,810 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 14:12:36,810 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 14:12:36,810 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung des Ernst dieses Problem, weil es den Dickness nicht den Schweine nicht zeigen.
2024-05-02 14:12:36,810 - INFO - joeynmt.training - Example #2
2024-05-02 14:12:36,811 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 14:12:36,811 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 14:12:36,811 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'den', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wandel@@', 's', '.', '</s>']
2024-05-02 14:12:36,811 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 14:12:36,811 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 14:12:36,811 - INFO - joeynmt.training - 	Hypothesis: Das arktische Eis ist, in einem Sinne, der den Herz des globalen Klimawandels.
2024-05-02 14:12:36,811 - INFO - joeynmt.training - Example #3
2024-05-02 14:12:36,811 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 14:12:36,811 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 14:12:36,812 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'in', 'Win@@', 'ter', 'und', 'Ver@@', 'fol@@', 'gen', 'im', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 14:12:36,812 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 14:12:36,812 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 14:12:36,812 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Verfolgen im Sommer.
2024-05-02 14:12:36,812 - INFO - joeynmt.training - Example #4
2024-05-02 14:12:36,812 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 14:12:36,812 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 14:12:36,812 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'wird', 'ein', 'schn@@', 'ell@@', 'es', 'F@@', 'ast@@', '-@@', 'F@@', 'el@@', 'd', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 14:12:36,813 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 14:12:36,813 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 14:12:36,813 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, wird ein schnelles Fast-Feld, was in den letzten 25 Jahren passiert ist.
2024-05-02 14:13:18,333 - INFO - joeynmt.training - Epoch   9, Step:    36600, Batch Loss:     1.370286, Batch Acc: 0.597843, Tokens per Sec:     1761, Lr: 0.000300
2024-05-02 14:13:58,305 - INFO - joeynmt.training - Epoch   9, Step:    36700, Batch Loss:     1.460310, Batch Acc: 0.596651, Tokens per Sec:     1871, Lr: 0.000300
2024-05-02 14:14:38,798 - INFO - joeynmt.training - Epoch   9, Step:    36800, Batch Loss:     1.296523, Batch Acc: 0.597790, Tokens per Sec:     1817, Lr: 0.000300
2024-05-02 14:15:19,884 - INFO - joeynmt.training - Epoch   9, Step:    36900, Batch Loss:     1.308845, Batch Acc: 0.594358, Tokens per Sec:     1831, Lr: 0.000300
2024-05-02 14:16:00,345 - INFO - joeynmt.training - Epoch   9, Step:    37000, Batch Loss:     1.613082, Batch Acc: 0.597569, Tokens per Sec:     1844, Lr: 0.000300
2024-05-02 14:16:00,346 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 14:16:00,346 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 14:18:09,008 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.59, ppl:   4.91, acc:   0.55, generation: 128.5508[sec], evaluation: 0.0000[sec]
2024-05-02 14:18:09,246 - INFO - joeynmt.helpers - delete models/bpe_4k/34000.ckpt
2024-05-02 14:18:09,253 - INFO - joeynmt.training - Example #0
2024-05-02 14:18:09,253 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 14:18:09,254 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 14:18:09,254 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Di@@', 'as', 'ge@@', 'm', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'K@@', 'a@@', 'p', ',', 'die', 'die', 'die', 'ar@@', 'kt@@', 'ischen', 'Z@@', 'ah@@', 'n@@', 'b@@', 'rech@@', 'nen', ',', 'die', 'die', 'größ@@', 'te', '4@@', '8', 'Stat@@', 'en', 'Son@@', 'nen@@', 'ne', '.', '</s>']
2024-05-02 14:18:09,254 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 14:18:09,254 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 14:18:09,254 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Dias gem gezeigt, dass die arktischen Kap, die die die arktischen Zahnbrechnen, die die größte 48 Staten Sonnenne.
2024-05-02 14:18:09,254 - INFO - joeynmt.training - Example #1
2024-05-02 14:18:09,255 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 14:18:09,255 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 14:18:09,255 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'des', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'spezi@@', 'ellen', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'd@@', 'ün@@', 'ne', 'die', 'Sch@@', 'i@@', 'ff@@', 'e', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 14:18:09,255 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 14:18:09,255 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 14:18:09,255 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung des Ernsthaft dieses speziellen Problem, weil es nicht die dünne die Schiffe des Eis.
2024-05-02 14:18:09,255 - INFO - joeynmt.training - Example #2
2024-05-02 14:18:09,256 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 14:18:09,256 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 14:18:09,256 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'z@@', 'ept', 'ist', ',', 'in', 'einem', 'Sin@@', 'n', ',', 'das', 'das', 'das', 'das', 'welt@@', 'wei@@', 'te', 'Kli@@', 'ma@@', 'wan@@', 'del', '.', '</s>']
2024-05-02 14:18:09,256 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 14:18:09,256 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 14:18:09,256 - INFO - joeynmt.training - 	Hypothesis: Der arktische Eiszept ist, in einem Sinn, das das das das weltweite Klimawandel.
2024-05-02 14:18:09,256 - INFO - joeynmt.training - Example #3
2024-05-02 14:18:09,256 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 14:18:09,257 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 14:18:09,257 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'weiter@@', 't', 'sich', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'im', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 14:18:09,257 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 14:18:09,257 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 14:18:09,257 - INFO - joeynmt.training - 	Hypothesis: Es erweitert sich in Winter und Kontrakten im Sommer.
2024-05-02 14:18:09,257 - INFO - joeynmt.training - Example #4
2024-05-02 14:18:09,257 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 14:18:09,257 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 14:18:09,258 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'dass', 'ein', 'R@@', 'oh@@', 'stoff@@', 'e', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', '.', '</s>']
2024-05-02 14:18:09,258 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 14:18:09,258 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 14:18:09,258 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, dass ein Rohstoffe von dem, was in den letzten 25 Jahren passiert.
2024-05-02 14:18:53,206 - INFO - joeynmt.training - Epoch   9, Step:    37100, Batch Loss:     1.344613, Batch Acc: 0.594763, Tokens per Sec:     1723, Lr: 0.000300
2024-05-02 14:19:37,412 - INFO - joeynmt.training - Epoch   9, Step:    37200, Batch Loss:     1.296057, Batch Acc: 0.593862, Tokens per Sec:     1691, Lr: 0.000300
2024-05-02 14:20:20,532 - INFO - joeynmt.training - Epoch   9, Step:    37300, Batch Loss:     1.253288, Batch Acc: 0.595164, Tokens per Sec:     1694, Lr: 0.000300
2024-05-02 14:21:04,570 - INFO - joeynmt.training - Epoch   9, Step:    37400, Batch Loss:     1.404858, Batch Acc: 0.594817, Tokens per Sec:     1656, Lr: 0.000300
2024-05-02 14:21:47,532 - INFO - joeynmt.training - Epoch   9, Step:    37500, Batch Loss:     1.586887, Batch Acc: 0.593487, Tokens per Sec:     1738, Lr: 0.000300
2024-05-02 14:21:47,532 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 14:21:47,532 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 14:24:07,949 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.59, ppl:   4.88, acc:   0.55, generation: 140.3018[sec], evaluation: 0.0000[sec]
2024-05-02 14:24:07,951 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 14:24:08,176 - INFO - joeynmt.helpers - delete models/bpe_4k/36000.ckpt
2024-05-02 14:24:08,181 - INFO - joeynmt.training - Example #0
2024-05-02 14:24:08,181 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 14:24:08,181 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 14:24:08,181 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'gen', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ischen', 'E@@', 'is', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'k@@', 'ti@@', 'ere', 'K@@', 'a@@', 'p', ',', 'die', 'die', 'größ@@', 'e', '4@@', '8', 'Sta@@', 'aten', 'der', 'Größ@@', 'e', 'der', 'ver@@', 'gan@@', 'gen@@', 'en', '4@@', '8', 'Jahren', 'Son@@', 'nen@@', 'ne', ',', 'hat', 'Son@@', 'nen@@', 'ne', '.', '</s>']
2024-05-02 14:24:08,182 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 14:24:08,182 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 14:24:08,182 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folgen gezeigt, dass die arktischen Eis gezeigt, dass die arktiere Kap, die die größe 48 Staaten der Größe der vergangenen 48 Jahren Sonnenne, hat Sonnenne.
2024-05-02 14:24:08,182 - INFO - joeynmt.training - Example #1
2024-05-02 14:24:08,182 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 14:24:08,182 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 14:24:08,182 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'der', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Sch@@', 'wer@@', 't@@', 'sch@@', 'ät@@', 'zen', 'der', 'E@@', 'is', '.', '</s>']
2024-05-02 14:24:08,183 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 14:24:08,183 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 14:24:08,183 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung der Ernsthaftigkeit dieses Problem, weil es nicht die Schwertschätzen der Eis.
2024-05-02 14:24:08,183 - INFO - joeynmt.training - Example #2
2024-05-02 14:24:08,183 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 14:24:08,183 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 14:24:08,183 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'kt@@', 'is', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'Bi@@', 'en@@', 'en@@', 'en@@', 'en@@', 'system', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', '.', '</s>']
2024-05-02 14:24:08,184 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 14:24:08,184 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 14:24:08,184 - INFO - joeynmt.training - 	Hypothesis: Das Arktis ist, in einem Sinne, der Bienenenensystem des globalen Klima.
2024-05-02 14:24:08,184 - INFO - joeynmt.training - Example #3
2024-05-02 14:24:08,184 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 14:24:08,184 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 14:24:08,184 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'weiter@@', 't', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ul@@', 'ti@@', 'eren', 'im', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 14:24:08,184 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 14:24:08,185 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 14:24:08,185 - INFO - joeynmt.training - 	Hypothesis: Es erweitert in Winter und Kontrakultieren im Sommer.
2024-05-02 14:24:08,185 - INFO - joeynmt.training - Example #4
2024-05-02 14:24:08,185 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 14:24:08,185 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 14:24:08,185 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', 'ein', 'R@@', 'a@@', 'pi@@', 'de', 'sein', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 14:24:08,185 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 14:24:08,185 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 14:24:08,186 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde ein Rapide sein, was in den letzten 25 Jahren passiert ist.
2024-05-02 14:24:50,667 - INFO - joeynmt.training - Epoch   9, Step:    37600, Batch Loss:     1.494494, Batch Acc: 0.595424, Tokens per Sec:     1782, Lr: 0.000300
2024-05-02 14:25:33,960 - INFO - joeynmt.training - Epoch   9, Step:    37700, Batch Loss:     1.520620, Batch Acc: 0.593464, Tokens per Sec:     1700, Lr: 0.000300
2024-05-02 14:26:19,279 - INFO - joeynmt.training - Epoch   9, Step:    37800, Batch Loss:     1.535382, Batch Acc: 0.589627, Tokens per Sec:     1658, Lr: 0.000300
2024-05-02 14:27:05,773 - INFO - joeynmt.training - Epoch   9, Step:    37900, Batch Loss:     1.378157, Batch Acc: 0.595293, Tokens per Sec:     1581, Lr: 0.000300
2024-05-02 14:27:49,997 - INFO - joeynmt.training - Epoch   9, Step:    38000, Batch Loss:     1.468843, Batch Acc: 0.588015, Tokens per Sec:     1730, Lr: 0.000300
2024-05-02 14:27:49,997 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 14:27:49,998 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 14:30:14,545 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.87, acc:   0.56, generation: 144.4267[sec], evaluation: 0.0000[sec]
2024-05-02 14:30:14,547 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 14:30:14,781 - INFO - joeynmt.helpers - delete models/bpe_4k/36500.ckpt
2024-05-02 14:30:14,786 - INFO - joeynmt.training - Example #0
2024-05-02 14:30:14,787 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 14:30:14,787 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 14:30:14,787 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'gen', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is', 'ge@@', 'zeigt', ',', 'dass', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', '4@@', '8', 'Zu@@', 'stand', 'der', 'unter@@', 'en', '4@@', '8', 'Stat@@', 'en', ',', 'Son@@', 'nen@@', 'nen@@', 'ne', '.', '</s>']
2024-05-02 14:30:14,787 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 14:30:14,788 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 14:30:14,788 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folgen gezeigt, dass die arktischen Eis gezeigt, dass die meisten drei Millionen Jahre die Größe der letzten drei Millionen Jahre der Größe 48 Zustand der unteren 48 Staten, Sonnennenne.
2024-05-02 14:30:14,788 - INFO - joeynmt.training - Example #1
2024-05-02 14:30:14,788 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 14:30:14,788 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 14:30:14,788 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'des', 'spezi@@', 'ellen', 'Problem@@', 's', ',', 'denn', 'es', 'zeigt', 'das', 'E@@', 'is', 'nicht', 'die', 'Sch@@', 'w@@', 'ein@@', 's', '.', '</s>']
2024-05-02 14:30:14,788 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 14:30:14,789 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 14:30:14,789 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung des speziellen Problems, denn es zeigt das Eis nicht die Schweins.
2024-05-02 14:30:14,789 - INFO - joeynmt.training - Example #2
2024-05-02 14:30:14,789 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 14:30:14,789 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 14:30:14,789 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'tik@@', 'et@@', 'ische', 'E@@', 'is', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'der', 'Bi@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', '-', 'des', 'Kli@@', 'ma@@', 'wandel@@', 's', '.', '</s>']
2024-05-02 14:30:14,789 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 14:30:14,790 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 14:30:14,790 - INFO - joeynmt.training - 	Hypothesis: Das Artiketische Eis ist, in einem Sinne, der Bienenenenen- des Klimawandels.
2024-05-02 14:30:14,790 - INFO - joeynmt.training - Example #3
2024-05-02 14:30:14,790 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 14:30:14,790 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 14:30:14,790 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'sich', 'in', 'den', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', '.', '</s>']
2024-05-02 14:30:14,790 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 14:30:14,790 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 14:30:14,790 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in den Sommer und Kontrakten.
2024-05-02 14:30:14,791 - INFO - joeynmt.training - Example #4
2024-05-02 14:30:14,791 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 14:30:14,791 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 14:30:14,791 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', ',', 'ein', 'R@@', 'a@@', 'di@@', 'er@@', 'ung', 'sein', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', '.', '</s>']
2024-05-02 14:30:14,791 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 14:30:14,791 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 14:30:14,791 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde, ein Radierung sein, was in den letzten 25 Jahren passiert.
2024-05-02 14:30:59,591 - INFO - joeynmt.training - Epoch   9, Step:    38100, Batch Loss:     1.440189, Batch Acc: 0.593308, Tokens per Sec:     1693, Lr: 0.000300
2024-05-02 14:31:43,653 - INFO - joeynmt.training - Epoch   9, Step:    38200, Batch Loss:     1.288407, Batch Acc: 0.592537, Tokens per Sec:     1684, Lr: 0.000300
2024-05-02 14:32:28,206 - INFO - joeynmt.training - Epoch   9, Step:    38300, Batch Loss:     1.344400, Batch Acc: 0.591786, Tokens per Sec:     1654, Lr: 0.000300
2024-05-02 14:33:12,511 - INFO - joeynmt.training - Epoch   9, Step:    38400, Batch Loss:     1.300673, Batch Acc: 0.595797, Tokens per Sec:     1636, Lr: 0.000300
2024-05-02 14:33:57,209 - INFO - joeynmt.training - Epoch   9, Step:    38500, Batch Loss:     1.445125, Batch Acc: 0.589698, Tokens per Sec:     1685, Lr: 0.000300
2024-05-02 14:33:57,210 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 14:33:57,210 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 14:36:02,813 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.85, acc:   0.56, generation: 125.4803[sec], evaluation: 0.0000[sec]
2024-05-02 14:36:02,815 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 14:36:03,050 - INFO - joeynmt.helpers - delete models/bpe_4k/35000.ckpt
2024-05-02 14:36:03,053 - INFO - joeynmt.training - Example #0
2024-05-02 14:36:03,053 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 14:36:03,053 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 14:36:03,053 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'as', 'ge@@', 'zeigt', ',', 'dass', 'die', 'Ar@@', 'kt@@', 'is', 'in', 'den', 'meisten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', '4@@', '8', 'Stat@@', 'en', ',', 'die', 'Son@@', 'de', ',', 'die', 'Son@@', 'ne', '4@@', '8', 'Prozent', '.', '</s>']
2024-05-02 14:36:03,054 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 14:36:03,054 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 14:36:03,054 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dias gezeigt, dass die Arktis in den meisten drei Millionen Jahre lang der Größe der letzten drei Millionen Jahre die Größe 48 Staten, die Sonde, die Sonne 48 Prozent.
2024-05-02 14:36:03,054 - INFO - joeynmt.training - Example #1
2024-05-02 14:36:03,054 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 14:36:03,054 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 14:36:03,055 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'h@@', 'än@@', 'ge', 'das', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'Problem', ',', 'weil', 'es', 'das', 'E@@', 'is', 'nicht', 'zeigt', '.', '</s>']
2024-05-02 14:36:03,055 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 14:36:03,055 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 14:36:03,055 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterhänge das Ernsthaftigkeit dieses Problem, weil es das Eis nicht zeigt.
2024-05-02 14:36:03,055 - INFO - joeynmt.training - Example #2
2024-05-02 14:36:03,055 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 14:36:03,055 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 14:36:03,056 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'is', 'ist', 'in', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', ',', 'der', 'Bi@@', 'en@@', 'en@@', 'system', '.', '</s>']
2024-05-02 14:36:03,056 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 14:36:03,056 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 14:36:03,056 - INFO - joeynmt.training - 	Hypothesis: Der Arktis ist in gewissem Sinne, der Bienensystem.
2024-05-02 14:36:03,056 - INFO - joeynmt.training - Example #3
2024-05-02 14:36:03,056 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 14:36:03,056 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 14:36:03,056 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 14:36:03,057 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 14:36:03,057 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 14:36:03,057 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontrakte im Sommer.
2024-05-02 14:36:03,057 - INFO - joeynmt.training - Example #4
2024-05-02 14:36:03,057 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 14:36:03,057 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 14:36:03,057 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', ',', 'dass', 'eine', 'R@@', 'a@@', 'di@@', 'er@@', '-@@', 'schn@@', 'ell@@', '-@@', 'Vor@@', 'aus@@', 'setz@@', 'ung', 'von', 'dem', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahre', 'gesch@@', 'a@@', 'h', 'ist', '.', '</s>']
2024-05-02 14:36:03,058 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 14:36:03,058 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 14:36:03,058 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde, dass eine Radier-schnell-Voraussetzung von dem, was über die letzten 25 Jahre geschah ist.
2024-05-02 14:36:47,548 - INFO - joeynmt.training - Epoch   9, Step:    38600, Batch Loss:     1.426121, Batch Acc: 0.593923, Tokens per Sec:     1640, Lr: 0.000300
2024-05-02 14:37:30,387 - INFO - joeynmt.training - Epoch   9, Step:    38700, Batch Loss:     1.458102, Batch Acc: 0.588095, Tokens per Sec:     1690, Lr: 0.000300
2024-05-02 14:38:13,426 - INFO - joeynmt.training - Epoch   9, Step:    38800, Batch Loss:     1.465510, Batch Acc: 0.588815, Tokens per Sec:     1749, Lr: 0.000300
2024-05-02 14:38:39,644 - INFO - joeynmt.training - Epoch   9: total training loss 6143.10
2024-05-02 14:38:39,644 - INFO - joeynmt.training - EPOCH 10
2024-05-02 14:38:56,679 - INFO - joeynmt.training - Epoch  10, Step:    38900, Batch Loss:     1.295573, Batch Acc: 0.614855, Tokens per Sec:     1603, Lr: 0.000300
2024-05-02 14:39:38,671 - INFO - joeynmt.training - Epoch  10, Step:    39000, Batch Loss:     1.167047, Batch Acc: 0.615999, Tokens per Sec:     1729, Lr: 0.000300
2024-05-02 14:39:38,671 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 14:39:38,671 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 14:41:48,419 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.86, acc:   0.56, generation: 129.6205[sec], evaluation: 0.0000[sec]
2024-05-02 14:41:48,662 - INFO - joeynmt.helpers - delete models/bpe_4k/37000.ckpt
2024-05-02 14:41:48,675 - INFO - joeynmt.training - Example #0
2024-05-02 14:41:48,675 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 14:41:48,675 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 14:41:48,675 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'des', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', 'k@@', 'ti@@', 'eren', ',', 'die', 'die', 'für', 'die', 'meisten', '4@@', '8', 'Sta@@', 'aten', ',', 'die', 'Größ@@', 'e', 'der', 'unter@@', 'en', '4@@', '8', 'Stat@@', 'en', ',', 'Son@@', 'nen@@', 'nen@@', 'ne', ',', 'hat', 'Son@@', 'nen@@', 'ne', ',', 'hat', 'Son@@', 'nen@@', 'ne', ',', 'hat', 'Son@@', 'nen@@', 'ne', ',', 'hat', 'Son@@', 'nen@@', 'ne', ',', 'hat', 'Son@@', 'nen@@', 'ne', ',', 'die', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 14:41:48,676 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 14:41:48,676 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 14:41:48,676 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Foldes gezeigt, dass die arktischen Eisktieren, die die für die meisten 48 Staaten, die Größe der unteren 48 Staten, Sonnennenne, hat Sonnenne, hat Sonnenne, hat Sonnenne, hat Sonnenne, hat Sonnenne, die von 40 Prozent.
2024-05-02 14:41:48,676 - INFO - joeynmt.training - Example #1
2024-05-02 14:41:48,676 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 14:41:48,676 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 14:41:48,676 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'des', 'S@@', 'eri@@', 'ums', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Sch@@', 'wer@@', 't', 'nicht', 'die', 'd@@', 'ün@@', 'ne', 'der', 'E@@', 'is', '.', '</s>']
2024-05-02 14:41:48,677 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 14:41:48,677 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 14:41:48,677 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung des Seriums dieses spezielle Problem, weil es nicht die Schwert nicht die dünne der Eis.
2024-05-02 14:41:48,677 - INFO - joeynmt.training - Example #2
2024-05-02 14:41:48,677 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 14:41:48,677 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 14:41:48,677 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'is', 'ist', ',', 'im', 'Sin@@', 'ne', ',', 'im', 'Sin@@', 'ne', ',', 'der', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wandel@@', 's', '.', '</s>']
2024-05-02 14:41:48,678 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 14:41:48,678 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 14:41:48,678 - INFO - joeynmt.training - 	Hypothesis: Der Arktis ist, im Sinne, im Sinne, der das Herz des globalen Klimawandels.
2024-05-02 14:41:48,678 - INFO - joeynmt.training - Example #3
2024-05-02 14:41:48,678 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 14:41:48,678 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 14:41:48,678 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'weiter@@', 'n', 'sich', 'in', 'Win@@', 'ter', 'und', 'kon@@', 'tra@@', 'k@@', 'ti@@', 'ert', '.', '</s>']
2024-05-02 14:41:48,679 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 14:41:48,679 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 14:41:48,679 - INFO - joeynmt.training - 	Hypothesis: Es erweitern sich in Winter und kontraktiert.
2024-05-02 14:41:48,679 - INFO - joeynmt.training - Example #4
2024-05-02 14:41:48,679 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 14:41:48,679 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 14:41:48,679 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', ',', 'dass', 'ein', 'R@@', 'a@@', 'di@@', 'o@@', 't@@', 't@@', 'eil', 'davon', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 14:41:48,680 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 14:41:48,680 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 14:41:48,680 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde, dass ein Radiotteil davon, was über die letzten 25 Jahren passiert ist.
2024-05-02 14:42:33,600 - INFO - joeynmt.training - Epoch  10, Step:    39100, Batch Loss:     1.331425, Batch Acc: 0.611712, Tokens per Sec:     1685, Lr: 0.000300
2024-05-02 14:43:16,539 - INFO - joeynmt.training - Epoch  10, Step:    39200, Batch Loss:     1.379188, Batch Acc: 0.615870, Tokens per Sec:     1738, Lr: 0.000300
2024-05-02 14:44:00,118 - INFO - joeynmt.training - Epoch  10, Step:    39300, Batch Loss:     1.341436, Batch Acc: 0.606264, Tokens per Sec:     1692, Lr: 0.000300
2024-05-02 14:44:43,160 - INFO - joeynmt.training - Epoch  10, Step:    39400, Batch Loss:     1.386660, Batch Acc: 0.611876, Tokens per Sec:     1700, Lr: 0.000300
2024-05-02 14:45:27,465 - INFO - joeynmt.training - Epoch  10, Step:    39500, Batch Loss:     1.231004, Batch Acc: 0.611653, Tokens per Sec:     1681, Lr: 0.000300
2024-05-02 14:45:27,465 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 14:45:27,466 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 14:47:55,163 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.59, ppl:   4.90, acc:   0.56, generation: 147.5692[sec], evaluation: 0.0000[sec]
2024-05-02 14:47:55,165 - INFO - joeynmt.training - Example #0
2024-05-02 14:47:55,165 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 14:47:55,166 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 14:47:55,166 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'S@@', 'a@@', 'den', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', '-@@', 'C@@', 'a@@', 'p', ',', 'die', 'die', 'größ@@', 'te', '4@@', '8', 'Stat@@', 'en', 'der', 'großen', 'B@@', 'ör@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Stat@@', 'en', 'der', 'unter@@', 'en', '4@@', '8', 'Stat@@', 'en', 'Son@@', 'nen@@', 'be@@', 'l', ',', 'hat', 'Son@@', 'nen@@', 'k', 'von', '40', '%', '.', '</s>']
2024-05-02 14:47:55,166 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 14:47:55,166 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 14:47:55,166 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Saden gezeigt, dass die arktischen Eis-Cap, die die größte 48 Staten der großen Börse der unteren 48 Staten der unteren 48 Staten Sonnenbel, hat Sonnenk von 40%.
2024-05-02 14:47:55,167 - INFO - joeynmt.training - Example #1
2024-05-02 14:47:55,167 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 14:47:55,167 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 14:47:55,167 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'h@@', 'än@@', 'd@@', 'ungs@@', 'ver@@', 'd@@', 'amm@@', 't', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Sch@@', 'w@@', 'ein@@', 'ung', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 14:47:55,167 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 14:47:55,167 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 14:47:55,167 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterhändungsverdammt dieses spezielle Problem, weil es nicht die Schweinung des Eis.
2024-05-02 14:47:55,168 - INFO - joeynmt.training - Example #2
2024-05-02 14:47:55,168 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 14:47:55,168 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 14:47:55,168 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'is', 'ist', ',', 'im', 'Sin@@', 'ne', ',', 'der', 'das', 'das', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', '.', '</s>']
2024-05-02 14:47:55,168 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 14:47:55,168 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 14:47:55,168 - INFO - joeynmt.training - 	Hypothesis: Der Arktis ist, im Sinne, der das das das Herz des globalen Klima des globalen Klima.
2024-05-02 14:47:55,168 - INFO - joeynmt.training - Example #3
2024-05-02 14:47:55,169 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 14:47:55,169 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 14:47:55,169 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 14:47:55,169 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 14:47:55,169 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 14:47:55,169 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Kontrakte im Sommer und Kontrakte im Sommer.
2024-05-02 14:47:55,169 - INFO - joeynmt.training - Example #4
2024-05-02 14:47:55,170 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 14:47:55,170 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 14:47:55,170 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'zei@@', 'ge', ',', 'dass', 'man', 'ein', 'Re@@', 'p@@', 'di@@', 'en@@', 'st@@', '-@@', 'Vor@@', 'wär@@', 'ts@@', 'sch@@', 'la@@', 'g', 'sein', 'wird', '.', '</s>']
2024-05-02 14:47:55,170 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 14:47:55,170 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 14:47:55,170 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich zeige, dass man ein Repdienst-Vorwärtsschlag sein wird.
2024-05-02 14:48:38,718 - INFO - joeynmt.training - Epoch  10, Step:    39600, Batch Loss:     1.550750, Batch Acc: 0.609509, Tokens per Sec:     1718, Lr: 0.000300
2024-05-02 14:49:22,612 - INFO - joeynmt.training - Epoch  10, Step:    39700, Batch Loss:     1.373538, Batch Acc: 0.609960, Tokens per Sec:     1758, Lr: 0.000300
2024-05-02 14:50:04,982 - INFO - joeynmt.training - Epoch  10, Step:    39800, Batch Loss:     1.263045, Batch Acc: 0.605340, Tokens per Sec:     1769, Lr: 0.000300
2024-05-02 14:50:46,455 - INFO - joeynmt.training - Epoch  10, Step:    39900, Batch Loss:     1.711737, Batch Acc: 0.601404, Tokens per Sec:     1804, Lr: 0.000300
2024-05-02 14:51:30,136 - INFO - joeynmt.training - Epoch  10, Step:    40000, Batch Loss:     1.520844, Batch Acc: 0.598470, Tokens per Sec:     1715, Lr: 0.000300
2024-05-02 14:51:30,137 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 14:51:30,137 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 14:53:40,601 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.86, acc:   0.56, generation: 130.3454[sec], evaluation: 0.0000[sec]
2024-05-02 14:53:40,847 - INFO - joeynmt.helpers - delete models/bpe_4k/34500.ckpt
2024-05-02 14:53:40,858 - INFO - joeynmt.training - Example #0
2024-05-02 14:53:40,858 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 14:53:40,858 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 14:53:40,859 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'de', 'ge@@', 'zeig@@', 'te', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ge@@', 'zeigt', ',', 'dass', 'die', 'größ@@', 'te', 'K@@', 'ü@@', 'hl@@', 'sch@@', 'r@@', 'än@@', 'k@@', 'te', ',', 'die', 'die', 'größ@@', 'te', '4@@', '8', 'Stat@@', 'en', ',', 'der', 'Son@@', 'ne', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 14:53:40,859 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 14:53:40,859 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 14:53:40,859 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folde gezeigte, dass die arktische Eis gezeigt, dass die größte Kühlschränkte, die die größte 48 Staten, der Sonne von 40 Prozent.
2024-05-02 14:53:40,859 - INFO - joeynmt.training - Example #1
2024-05-02 14:53:40,860 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 14:53:40,860 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 14:53:40,860 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'h@@', 'än@@', 'ger', 'das', 'Er@@', 'n@@', 'st', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'das', 'D@@', 'ick@@', 'n@@', 'ess@@', 'er', 'zeigt', '.', '</s>']
2024-05-02 14:53:40,860 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 14:53:40,860 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 14:53:40,861 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterhänger das Ernst dieses Problem, weil es nicht das Dicknesser zeigt.
2024-05-02 14:53:40,861 - INFO - joeynmt.training - Example #2
2024-05-02 14:53:40,861 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 14:53:40,861 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 14:53:40,861 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'is', 'ist', ',', 'im', 'Sin@@', 'ne', ',', 'der', 'Bi@@', 'en@@', 'en@@', 'en@@', 'st@@', 'off', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wandel@@', 's', '.', '</s>']
2024-05-02 14:53:40,861 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 14:53:40,862 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 14:53:40,862 - INFO - joeynmt.training - 	Hypothesis: Der Arktis ist, im Sinne, der Bienenenstoff des globalen Klimawandels.
2024-05-02 14:53:40,862 - INFO - joeynmt.training - Example #3
2024-05-02 14:53:40,862 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 14:53:40,862 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 14:53:40,862 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 14:53:40,862 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 14:53:40,863 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 14:53:40,863 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Kontrakte im Sommer.
2024-05-02 14:53:40,863 - INFO - joeynmt.training - Example #4
2024-05-02 14:53:40,863 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 14:53:40,863 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 14:53:40,863 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'dass', 'ein', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'vor@@', 'ne', 'davon', ',', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 14:53:40,863 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 14:53:40,863 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 14:53:40,864 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, dass ein Fast-Fast-vorne davon, was über die letzten 25 Jahren passiert ist.
2024-05-02 14:54:22,706 - INFO - joeynmt.training - Epoch  10, Step:    40100, Batch Loss:     1.363474, Batch Acc: 0.604903, Tokens per Sec:     1763, Lr: 0.000300
2024-05-02 14:55:05,222 - INFO - joeynmt.training - Epoch  10, Step:    40200, Batch Loss:     1.407489, Batch Acc: 0.605627, Tokens per Sec:     1724, Lr: 0.000300
2024-05-02 14:55:48,512 - INFO - joeynmt.training - Epoch  10, Step:    40300, Batch Loss:     1.194404, Batch Acc: 0.607343, Tokens per Sec:     1736, Lr: 0.000300
2024-05-02 14:56:31,108 - INFO - joeynmt.training - Epoch  10, Step:    40400, Batch Loss:     1.350199, Batch Acc: 0.601874, Tokens per Sec:     1759, Lr: 0.000300
2024-05-02 14:57:13,694 - INFO - joeynmt.training - Epoch  10, Step:    40500, Batch Loss:     1.412761, Batch Acc: 0.599691, Tokens per Sec:     1749, Lr: 0.000300
2024-05-02 14:57:13,695 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 14:57:13,695 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 14:59:16,903 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.85, acc:   0.56, generation: 123.0887[sec], evaluation: 0.0000[sec]
2024-05-02 14:59:16,905 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 14:59:17,138 - INFO - joeynmt.helpers - delete models/bpe_4k/37500.ckpt
2024-05-02 14:59:17,147 - INFO - joeynmt.training - Example #0
2024-05-02 14:59:17,147 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 14:59:17,148 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 14:59:17,148 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'as', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', '-@@', 'Z@@', 'ah@@', 'n', 'der', 'meisten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'der', 'unter@@', 'e', '4@@', '8', 'Stat@@', 'en', ',', 'hat', 'Son@@', 'nen@@', 'k', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 14:59:17,148 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 14:59:17,148 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 14:59:17,148 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dias gezeigt, dass die arktische Eis-Zahn der meisten drei Millionen Jahre lang der Größe der letzten drei Millionen Jahre der Größe der untere 48 Staten, hat Sonnenk von 40 Prozent.
2024-05-02 14:59:17,148 - INFO - joeynmt.training - Example #1
2024-05-02 14:59:17,149 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 14:59:17,149 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 14:59:17,149 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'des', 'Er@@', 'n@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'das', 'E@@', 'is', '.', '</s>']
2024-05-02 14:59:17,149 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 14:59:17,149 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 14:59:17,149 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung des Ernnsthaftigkeit dieses spezielle Problem, weil es nicht das Eis.
2024-05-02 14:59:17,149 - INFO - joeynmt.training - Example #2
2024-05-02 14:59:17,150 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 14:59:17,150 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 14:59:17,150 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'tik@@', 'et@@', 'ische', 'E@@', 'is@@', '-@@', 'C@@', 'a@@', 'p', 'ist', 'in', 'ge@@', 'wis@@', 'ser', 'Weise', ',', 'der', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', '.', '</s>']
2024-05-02 14:59:17,150 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 14:59:17,150 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 14:59:17,150 - INFO - joeynmt.training - 	Hypothesis: Der Artiketische Eis-Cap ist in gewisser Weise, der das Herz des globalen Klima.
2024-05-02 14:59:17,150 - INFO - joeynmt.training - Example #3
2024-05-02 14:59:17,151 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 14:59:17,151 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 14:59:17,151 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'sich', 'in', 'Win@@', 'ter', 'und', 'Ver@@', 'trä@@', 'ge', 'im', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 14:59:17,151 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 14:59:17,151 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 14:59:17,151 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und Verträge im Sommer.
2024-05-02 14:59:17,151 - INFO - joeynmt.training - Example #4
2024-05-02 14:59:17,151 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 14:59:17,152 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 14:59:17,152 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'zei@@', 'ge', ',', 'dass', 'es', 'ein', 'F@@', 'ast@@', '-@@', 'Vor@@', 'wär@@', 'ts@@', 'stellung', 'davon', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'gesch@@', 'ehen', 'ist', '.', '</s>']
2024-05-02 14:59:17,152 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 14:59:17,152 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 14:59:17,152 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich zeige, dass es ein Fast-Vorwärtsstellung davon, was in den letzten 25 Jahren geschehen ist.
2024-05-02 14:59:58,574 - INFO - joeynmt.training - Epoch  10, Step:    40600, Batch Loss:     1.619657, Batch Acc: 0.603734, Tokens per Sec:     1791, Lr: 0.000300
2024-05-02 15:00:39,638 - INFO - joeynmt.training - Epoch  10, Step:    40700, Batch Loss:     1.445768, Batch Acc: 0.603233, Tokens per Sec:     1755, Lr: 0.000300
2024-05-02 15:01:21,364 - INFO - joeynmt.training - Epoch  10, Step:    40800, Batch Loss:     1.371344, Batch Acc: 0.599475, Tokens per Sec:     1827, Lr: 0.000300
2024-05-02 15:02:02,222 - INFO - joeynmt.training - Epoch  10, Step:    40900, Batch Loss:     1.377077, Batch Acc: 0.607171, Tokens per Sec:     1839, Lr: 0.000300
2024-05-02 15:02:44,659 - INFO - joeynmt.training - Epoch  10, Step:    41000, Batch Loss:     1.256757, Batch Acc: 0.599997, Tokens per Sec:     1740, Lr: 0.000300
2024-05-02 15:02:44,659 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 15:02:44,659 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 15:04:58,474 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.87, acc:   0.56, generation: 133.6935[sec], evaluation: 0.0000[sec]
2024-05-02 15:04:58,476 - INFO - joeynmt.training - Example #0
2024-05-02 15:04:58,477 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 15:04:58,477 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 15:04:58,477 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', 'sch@@', 'rit@@', 't@@', 'stellen', ',', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'gro@@', 'ß', 'der', 'unter@@', 'en', '4@@', '8', 'Stat@@', 'en', ',', 'hat', 'Son@@', 'nen@@', 'st@@', 'ein@@', 'l@@', 'äu@@', 'ft', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 15:04:58,477 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 15:04:58,478 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 15:04:58,478 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folien gezeigt, dass die arktischen Eisschrittstellen, die meisten drei Millionen Jahre die Größe der letzten drei Millionen Jahre der groß der unteren 48 Staten, hat Sonnensteinläuft von 40 Prozent.
2024-05-02 15:04:58,478 - INFO - joeynmt.training - Example #1
2024-05-02 15:04:58,478 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 15:04:58,478 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 15:04:58,478 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'des', 'Er@@', 'n@@', 'st', 'dieses', 'spezi@@', 'elle', 'Problem', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'denn', 'es', 'zeigt', 'nicht', 'das', 'E@@', 'is', 'des', 'E@@', 'is@@', 'en@@', 'des', 'E@@', 'is@@', 'en@@', 'ver@@', 'mö@@', 'gen', '.', '</s>']
2024-05-02 15:04:58,479 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 15:04:58,479 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 15:04:58,479 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung des Ernst dieses spezielle Problem dieses spezielle Problem, denn es zeigt nicht das Eis des Eisendes Eisenvermögen.
2024-05-02 15:04:58,479 - INFO - joeynmt.training - Example #2
2024-05-02 15:04:58,479 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 15:04:58,479 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 15:04:58,479 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'sch@@', 'rit@@', 't@@', 'weise', 'ist', ',', 'im', 'Sin@@', 'ne', ',', 'der', 'Bi@@', 'en@@', 'en@@', 'en@@', 'system', 'des', 'Kli@@', 'ma@@', 'wandel@@', 's', '.', '</s>']
2024-05-02 15:04:58,479 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 15:04:58,480 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 15:04:58,480 - INFO - joeynmt.training - 	Hypothesis: Der arktische Eisschrittweise ist, im Sinne, der Bienenensystem des Klimawandels.
2024-05-02 15:04:58,480 - INFO - joeynmt.training - Example #3
2024-05-02 15:04:58,480 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 15:04:58,480 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 15:04:58,480 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'sich', 'in', 'Win@@', 'ter', 'und', 'Ver@@', 'trä@@', 'ge', 'in', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'tra@@', 'g', '.', '</s>']
2024-05-02 15:04:58,480 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 15:04:58,481 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 15:04:58,481 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und Verträge in Sommer und Vertrag.
2024-05-02 15:04:58,481 - INFO - joeynmt.training - Example #4
2024-05-02 15:04:58,481 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 15:04:58,481 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 15:04:58,481 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', 'ein', 'R@@', 'as@@', 'se', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passier@@', 'te', '.', '</s>']
2024-05-02 15:04:58,481 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 15:04:58,482 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 15:04:58,482 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige ein Rasse von dem, was in den letzten 25 Jahren passierte.
2024-05-02 15:05:40,816 - INFO - joeynmt.training - Epoch  10, Step:    41100, Batch Loss:     1.409335, Batch Acc: 0.606427, Tokens per Sec:     1736, Lr: 0.000300
2024-05-02 15:06:23,249 - INFO - joeynmt.training - Epoch  10, Step:    41200, Batch Loss:     1.487230, Batch Acc: 0.609373, Tokens per Sec:     1792, Lr: 0.000300
2024-05-02 15:07:07,444 - INFO - joeynmt.training - Epoch  10, Step:    41300, Batch Loss:     1.448498, Batch Acc: 0.605133, Tokens per Sec:     1694, Lr: 0.000300
2024-05-02 15:07:51,654 - INFO - joeynmt.training - Epoch  10, Step:    41400, Batch Loss:     1.435409, Batch Acc: 0.602567, Tokens per Sec:     1670, Lr: 0.000300
2024-05-02 15:08:34,363 - INFO - joeynmt.training - Epoch  10, Step:    41500, Batch Loss:     1.434048, Batch Acc: 0.598890, Tokens per Sec:     1755, Lr: 0.000300
2024-05-02 15:08:34,363 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 15:08:34,364 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 15:10:48,698 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.86, acc:   0.56, generation: 134.2188[sec], evaluation: 0.0000[sec]
2024-05-02 15:10:48,944 - INFO - joeynmt.helpers - delete models/bpe_4k/38000.ckpt
2024-05-02 15:10:48,953 - INFO - joeynmt.training - Example #0
2024-05-02 15:10:48,954 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 15:10:48,954 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 15:10:48,954 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'beiden', 'Fol@@', 'des', 'zeig@@', 'te', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', '-@@', 'Z@@', 'ah@@', 'n@@', 'b@@', 'rech@@', 'enden', ',', 'die', 'die', 'größ@@', 'te', '4@@', '8', 'Sta@@', 'aten', ',', 'die', 'Größ@@', 'e', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', ',', 'die', 'Son@@', 'nen@@', 'k', 'von', '40', 'Prozent', '.', '</s>']
2024-05-02 15:10:48,954 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 15:10:48,954 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 15:10:48,954 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese beiden Foldes zeigte, dass die arktischen Eis-Zahnbrechenden, die die größte 48 Staaten, die Größe der unteren 48 Staaten, die Sonnenk von 40 Prozent.
2024-05-02 15:10:48,955 - INFO - joeynmt.training - Example #1
2024-05-02 15:10:48,955 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 15:10:48,955 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 15:10:48,955 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ütz@@', 'ung', 'des', 'Er@@', 'n@@', 'st', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'das', 'Sch@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is@@', 'es', 'nicht', '.', '</s>']
2024-05-02 15:10:48,955 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 15:10:48,955 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 15:10:48,955 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung des Ernst dieses spezielle Problem, weil es nicht das Schickness des Eises nicht.
2024-05-02 15:10:48,956 - INFO - joeynmt.training - Example #2
2024-05-02 15:10:48,956 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 15:10:48,956 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 15:10:48,956 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'sch@@', 'o@@', 'berfläch@@', 'e', 'ist', 'in', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wandel@@', 's', '.', '</s>']
2024-05-02 15:10:48,956 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 15:10:48,956 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 15:10:48,956 - INFO - joeynmt.training - 	Hypothesis: Der Arktische Eisschoberfläche ist in gewissem Sinne des globalen Klimawandels.
2024-05-02 15:10:48,956 - INFO - joeynmt.training - Example #3
2024-05-02 15:10:48,957 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 15:10:48,957 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 15:10:48,957 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'sich', 'in', 'den', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 15:10:48,957 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 15:10:48,957 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 15:10:48,957 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in den Winter und Kontrakte im Sommer.
2024-05-02 15:10:48,957 - INFO - joeynmt.training - Example #4
2024-05-02 15:10:48,957 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 15:10:48,958 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 15:10:48,958 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', 'ein', 'schn@@', 'eller', 'Vor@@', 'wär@@', 'ts', 'dessen', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 15:10:48,958 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 15:10:48,958 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 15:10:48,958 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige ein schneller Vorwärts dessen, was in den letzten 25 Jahren passiert ist.
2024-05-02 15:11:31,025 - INFO - joeynmt.training - Epoch  10, Step:    41600, Batch Loss:     1.169807, Batch Acc: 0.598896, Tokens per Sec:     1713, Lr: 0.000300
2024-05-02 15:12:16,385 - INFO - joeynmt.training - Epoch  10, Step:    41700, Batch Loss:     1.388481, Batch Acc: 0.597357, Tokens per Sec:     1650, Lr: 0.000300
2024-05-02 15:12:59,971 - INFO - joeynmt.training - Epoch  10, Step:    41800, Batch Loss:     1.427827, Batch Acc: 0.606169, Tokens per Sec:     1759, Lr: 0.000300
2024-05-02 15:13:43,795 - INFO - joeynmt.training - Epoch  10, Step:    41900, Batch Loss:     1.344130, Batch Acc: 0.599363, Tokens per Sec:     1684, Lr: 0.000300
2024-05-02 15:14:26,636 - INFO - joeynmt.training - Epoch  10, Step:    42000, Batch Loss:     1.500841, Batch Acc: 0.599283, Tokens per Sec:     1719, Lr: 0.000300
2024-05-02 15:14:26,637 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 15:14:26,637 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 15:16:48,834 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.57, ppl:   4.82, acc:   0.56, generation: 142.0767[sec], evaluation: 0.0000[sec]
2024-05-02 15:16:48,835 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 15:16:49,078 - INFO - joeynmt.helpers - delete models/bpe_4k/41500.ckpt
2024-05-02 15:16:49,091 - INFO - joeynmt.training - Example #0
2024-05-02 15:16:49,091 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 15:16:49,091 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 15:16:49,092 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'ie', 'ge@@', 'zeigt', ',', 'dass', 'die', 'Ar@@', 'kt@@', 'is', 'in', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'ge@@', 'zeigt', ',', 'dass', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Stat@@', 'en', '4@@', '8', 'Prozent', '.', '</s>']
2024-05-02 15:16:49,092 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 15:16:49,092 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 15:16:49,093 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folie gezeigt, dass die Arktis in der letzten drei Millionen Jahre gezeigt, dass die meisten drei Millionen Jahre der Größe der letzten drei Millionen Jahre der Größe der letzten drei Staten 48 Prozent.
2024-05-02 15:16:49,093 - INFO - joeynmt.training - Example #1
2024-05-02 15:16:49,093 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 15:16:49,093 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 15:16:49,093 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'sta@@', 'aten', 'das', 'Er@@', 'n@@', 'st', 'des', 'S@@', 'eri@@', 'ums', 'dieses', 'spezi@@', 'elle', 'Problem', ',', 'weil', 'es', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 15:16:49,094 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 15:16:49,094 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 15:16:49,094 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstaaten das Ernst des Seriums dieses spezielle Problem, weil es die Dickness nicht die Dickness des Eis.
2024-05-02 15:16:49,094 - INFO - joeynmt.training - Example #2
2024-05-02 15:16:49,094 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 15:16:49,094 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 15:16:49,094 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', ',', 'in', 'einem', 'Sin@@', 'ne', ',', 'das', 'das', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', 'wandel@@', 's', '.', '</s>']
2024-05-02 15:16:49,095 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 15:16:49,095 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 15:16:49,095 - INFO - joeynmt.training - 	Hypothesis: Der Arktische Eis ist, in einem Sinne, das das das Herz des globalen Klimawandels.
2024-05-02 15:16:49,095 - INFO - joeynmt.training - Example #3
2024-05-02 15:16:49,095 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 15:16:49,095 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 15:16:49,096 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'sich', 'in', 'den', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', '.', '</s>']
2024-05-02 15:16:49,096 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 15:16:49,096 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 15:16:49,096 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in den Sommer und Kontrakte.
2024-05-02 15:16:49,096 - INFO - joeynmt.training - Example #4
2024-05-02 15:16:49,096 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 15:16:49,097 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 15:16:49,097 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', ',', 'dass', 'man', 'ein', 'R@@', 'a@@', 'di@@', 'en@@', 'st@@', '-@@', 'Vor@@', 'wär@@', 'ts@@', 'stellung', 'davon', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 15:16:49,097 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 15:16:49,097 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 15:16:49,097 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde, dass man ein Radienst-Vorwärtsstellung davon, was in den letzten 25 Jahren passiert ist.
2024-05-02 15:17:31,804 - INFO - joeynmt.training - Epoch  10, Step:    42100, Batch Loss:     1.398925, Batch Acc: 0.603039, Tokens per Sec:     1711, Lr: 0.000300
2024-05-02 15:18:14,900 - INFO - joeynmt.training - Epoch  10, Step:    42200, Batch Loss:     1.465635, Batch Acc: 0.603174, Tokens per Sec:     1686, Lr: 0.000300
2024-05-02 15:18:59,374 - INFO - joeynmt.training - Epoch  10, Step:    42300, Batch Loss:     1.299292, Batch Acc: 0.600507, Tokens per Sec:     1677, Lr: 0.000300
2024-05-02 15:19:41,797 - INFO - joeynmt.training - Epoch  10, Step:    42400, Batch Loss:     1.450804, Batch Acc: 0.601873, Tokens per Sec:     1772, Lr: 0.000300
2024-05-02 15:20:25,687 - INFO - joeynmt.training - Epoch  10, Step:    42500, Batch Loss:     1.348681, Batch Acc: 0.598683, Tokens per Sec:     1734, Lr: 0.000300
2024-05-02 15:20:25,687 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 15:20:25,688 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 15:22:47,777 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.57, ppl:   4.80, acc:   0.56, generation: 141.9683[sec], evaluation: 0.0000[sec]
2024-05-02 15:22:47,778 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 15:22:48,031 - INFO - joeynmt.helpers - delete models/bpe_4k/40000.ckpt
2024-05-02 15:22:48,042 - INFO - joeynmt.training - Example #0
2024-05-02 15:22:48,042 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 15:22:48,042 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 15:22:48,042 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'a@@', 'den', 'ge@@', 'zeigt', ',', 'dass', 'die', 'Ar@@', 'kt@@', 'is', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', '-@@', 'K@@', 'a@@', 'p', ',', 'die', 'die', 'meisten', '4@@', '8', 'Stat@@', 'en', ',', 'die', 'die', 'größ@@', 'e', '4@@', '8', 'Stat@@', 'en', ',', 'Son@@', 'nen@@', 'nen@@', 'ne', ',', 'um', '40', '%', '.', '</s>']
2024-05-02 15:22:48,043 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 15:22:48,043 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 15:22:48,043 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Diaden gezeigt, dass die Arktis, dass die arktischen Eis-Kap, die die meisten 48 Staten, die die größe 48 Staten, Sonnennenne, um 40%.
2024-05-02 15:22:48,043 - INFO - joeynmt.training - Example #1
2024-05-02 15:22:48,043 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 15:22:48,043 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 15:22:48,044 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'h@@', 'än@@', 'gen', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'ess', 'des', 'E@@', 'is', '.', '</s>']
2024-05-02 15:22:48,044 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 15:22:48,044 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 15:22:48,044 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterhängen die Ernsthaftigkeit dieses Problem, weil es nicht die Dickness des Eis.
2024-05-02 15:22:48,044 - INFO - joeynmt.training - Example #2
2024-05-02 15:22:48,044 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 15:22:48,045 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 15:22:48,045 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'et@@', 'ische', 'E@@', 'is@@', 'k@@', 'et@@', 'te', 'ist', ',', 'im', 'Sin@@', 'ne', ',', 'der', 'Bi@@', 'en@@', 'en@@', 'en@@', 'system', '.', '</s>']
2024-05-02 15:22:48,045 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 15:22:48,045 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 15:22:48,045 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eisketische Eiskette ist, im Sinne, der Bienenensystem.
2024-05-02 15:22:48,045 - INFO - joeynmt.training - Example #3
2024-05-02 15:22:48,045 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 15:22:48,045 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 15:22:48,045 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'weiter@@', 't', 'in', 'den', 'Win@@', 'ter', 'und', 'kon@@', 'tra@@', 'k@@', 'ti@@', 'ert', 'im', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 15:22:48,046 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 15:22:48,046 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 15:22:48,046 - INFO - joeynmt.training - 	Hypothesis: Es erweitert in den Winter und kontraktiert im Sommer.
2024-05-02 15:22:48,046 - INFO - joeynmt.training - Example #4
2024-05-02 15:22:48,046 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 15:22:48,046 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 15:22:48,046 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'dass', 'es', 'schn@@', 'elle', 'F@@', 'ast@@', '-@@', 'F@@', 'el@@', 'd', 'der', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2024-05-02 15:22:48,047 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 15:22:48,047 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 15:22:48,047 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeige, dass es schnelle Fast-Feld der letzten 25 Jahren passiert ist.
2024-05-02 15:23:31,520 - INFO - joeynmt.training - Epoch  10, Step:    42600, Batch Loss:     1.582202, Batch Acc: 0.602727, Tokens per Sec:     1683, Lr: 0.000300
2024-05-02 15:24:16,088 - INFO - joeynmt.training - Epoch  10, Step:    42700, Batch Loss:     1.605419, Batch Acc: 0.594945, Tokens per Sec:     1676, Lr: 0.000300
2024-05-02 15:24:59,936 - INFO - joeynmt.training - Epoch  10, Step:    42800, Batch Loss:     1.436837, Batch Acc: 0.599475, Tokens per Sec:     1730, Lr: 0.000300
2024-05-02 15:25:44,119 - INFO - joeynmt.training - Epoch  10, Step:    42900, Batch Loss:     1.445632, Batch Acc: 0.597114, Tokens per Sec:     1702, Lr: 0.000300
2024-05-02 15:26:28,376 - INFO - joeynmt.training - Epoch  10, Step:    43000, Batch Loss:     1.447693, Batch Acc: 0.600492, Tokens per Sec:     1699, Lr: 0.000300
2024-05-02 15:26:28,376 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 15:26:28,376 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 15:28:32,450 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.57, ppl:   4.78, acc:   0.56, generation: 123.9551[sec], evaluation: 0.0000[sec]
2024-05-02 15:28:32,452 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-02 15:28:32,701 - INFO - joeynmt.helpers - delete models/bpe_4k/39000.ckpt
2024-05-02 15:28:32,710 - INFO - joeynmt.training - Example #0
2024-05-02 15:28:32,710 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es', ',', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'percent', '.']
2024-05-02 15:28:32,710 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lichen', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist', '.']
2024-05-02 15:28:32,710 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'as', 'ge@@', 'zeigt', ',', 'dass', 'die', 'Ar@@', 'kt@@', 'is', ',', 'die', 'die', 'Ar@@', 'kt@@', 'is', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', ',', 'Son@@', 'nen@@', 'st@@', 'ad@@', 't', ',', 'hat', 'Son@@', 'nen@@', 'st@@', 'ad@@', 't', 'von', '40', '%', '.', '</s>']
2024-05-02 15:28:32,711 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-02 15:28:32,711 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-02 15:28:32,711 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dias gezeigt, dass die Arktis, die die Arktis der letzten drei Millionen Jahre die Größe der letzten drei Millionen Jahre die Größe der unteren 48 Staaten, Sonnenstadt, hat Sonnenstadt von 40%.
2024-05-02 15:28:32,711 - INFO - joeynmt.training - Example #1
2024-05-02 15:28:32,711 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'particular', 'problem', 'because', 'it', 'does@@', 'n', '&@@', 'a@@', 'pos@@', ';@@', 't', 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ice', '.']
2024-05-02 15:28:32,711 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeigt', '.']
2024-05-02 15:28:32,711 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', 'st@@', 'ütz@@', 't', 'das', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Sch@@', 'wer@@', 'el@@', 'heit', 'des', 'E@@', 'is@@', 'es', '.', '</s>']
2024-05-02 15:28:32,712 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-02 15:28:32,712 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-02 15:28:32,712 - INFO - joeynmt.training - 	Hypothesis: Aber das unterstützt das Ernsthaftigkeit dieses Problem, weil es nicht die Schwerelheit des Eises.
2024-05-02 15:28:32,712 - INFO - joeynmt.training - Example #2
2024-05-02 15:28:32,712 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '.']
2024-05-02 15:28:32,712 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's', '.']
2024-05-02 15:28:32,712 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'is', 'ist', 'in', 'ge@@', 'wis@@', 'ser', 'Weise', ',', 'im', 'B@@', 'au', 'des', 'glob@@', 'alen', 'Kli@@', 'ma@@', '-@@', 'K@@', 'lim@@', 'a', '.', '</s>']
2024-05-02 15:28:32,713 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-02 15:28:32,713 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-02 15:28:32,713 - INFO - joeynmt.training - 	Hypothesis: Die Arktis ist in gewisser Weise, im Bau des globalen Klima-Klima.
2024-05-02 15:28:32,713 - INFO - joeynmt.training - Example #3
2024-05-02 15:28:32,713 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer', '.']
2024-05-02 15:28:32,713 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er', '.']
2024-05-02 15:28:32,713 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'im', 'S@@', 'omm@@', 'er', '.', '</s>']
2024-05-02 15:28:32,714 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-02 15:28:32,714 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-02 15:28:32,714 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontrakten im Sommer.
2024-05-02 15:28:32,714 - INFO - joeynmt.training - Example #4
2024-05-02 15:28:32,714 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'ward', 'of', 'what', '&@@', 'a@@', 'pos@@', ';@@', 's', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-02 15:28:32,714 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge', ',', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2024-05-02 15:28:32,714 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', ',', 'die', 'ich', 'Ihnen', 'Ihnen', 'Ihnen', 'zeigen', 'werde', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', '.', '</s>']
2024-05-02 15:28:32,715 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-02 15:28:32,715 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-02 15:28:32,715 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen Ihnen Ihnen zeigen werde, was in den letzten 25 Jahren passiert.
2024-05-02 15:29:16,037 - INFO - joeynmt.training - Epoch  10, Step:    43100, Batch Loss:     1.488643, Batch Acc: 0.597012, Tokens per Sec:     1719, Lr: 0.000300
2024-05-02 15:29:51,462 - INFO - joeynmt.training - Epoch  10: total training loss 6022.87
2024-05-02 15:29:51,462 - INFO - joeynmt.training - Training ended after  10 epochs.
2024-05-02 15:29:51,462 - INFO - joeynmt.training - Best validation result (greedy) at step    43000:   4.78 ppl.
2024-05-02 15:29:51,487 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-02 15:29:51,558 - INFO - joeynmt.model - Enc-dec model built.
2024-05-02 15:29:51,603 - INFO - joeynmt.helpers - Load model from /Users/Iris/test/mt-exercise-5/models/bpe_4k/43000.ckpt.
2024-05-02 15:29:51,610 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=4000),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=4000),
	loss_function=None)
2024-05-02 15:29:51,611 - INFO - joeynmt.prediction - Decoding on dev set...
2024-05-02 15:29:51,611 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 15:29:51,611 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 15:32:57,896 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 186.1678[sec], evaluation: 0.0000[sec]
2024-05-02 15:32:57,899 - INFO - joeynmt.prediction - Translations saved to: /Users/Iris/test/mt-exercise-5/models/bpe_4k/00043000.hyps.dev.
2024-05-02 15:32:57,899 - INFO - joeynmt.prediction - Decoding on test set...
2024-05-02 15:32:57,899 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-02 15:32:57,899 - INFO - joeynmt.prediction - Predicting 1568 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-02 15:36:59,358 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 241.2831[sec], evaluation: 0.0000[sec]
2024-05-02 15:36:59,363 - INFO - joeynmt.prediction - Translations saved to: /Users/Iris/test/mt-exercise-5/models/bpe_4k/00043000.hyps.test.
